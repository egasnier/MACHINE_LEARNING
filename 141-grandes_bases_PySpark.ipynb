{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78d6ccd4-8055-4633-a483-6e5c66e40097",
   "metadata": {},
   "source": [
    "# A - Introduction à PySpark \n",
    "## Introduction\n",
    "**Apache Spark** est un framework (infrastructure logicielle) open-source de calcul distribué qui permet de traiter des bases de données de taille massive. Ceci est permis grâce à une distribution de données intelligente où les calculs sont effectués en parallèle sur plusieurs clusters, travaillant chacun sur une partie des données.\n",
    "\n",
    "Si vous avez des bases de données trop volumineuses entraînant des temps de calcul trop lents, il est judicieux d'utiliser Apache Spark.\n",
    "\n",
    "Spark est un framework optimisé pour le traitement de données et l'application d'algorithmes de Machine Learning. Cela est notamment possible grâce à des structures de données présentées dans ce module.\n",
    "\n",
    "Apache Spark a été développé par l'AMPLab de UC Berkeley. Il a été développé en Scala et est au meilleur de ses capacités dans son langage natif. La librairie PySpark propose d'utiliser Spark avec le langage Python où dans de nombreux cas d'usages les performances sont similaires à des implémentations avec **Scala**.\n",
    "\n",
    "Nous allons voir au cours de ce module qu'il est possible de traiter et d'analyser des bases de données grâce au framework Spark et en un temps très court, dès lors que nous avons des clusters à disposition.\n",
    "\n",
    "Au cours de cette formation, nous allons :\n",
    "\n",
    "- Présenter les différents éléments centraux de Spark, en commençant par les RDD, la structure la plus élémentaire de Spark qui permet d'effectuer des tâches très variées.\n",
    "- Comprendre le fonctionnement sous-jacent de Spark.\n",
    "- Présenter le type DataFrame, une structure plus riche que les RDD, optimisée pour le Machine Learning. La structure DataFrame de Spark est très proche dans sa forme à celle du package pandas en Python. Les DataFrame de Spark offrent néanmoins une performance sans égale lorsqu'ils sont mis en place sur plusieurs clusters.\n",
    "- Nous terminerons cette formation avec la construction d'un système de recommandation avec Spark qui sera une bonne base pour une mise en production.  \n",
    "\n",
    "Dans ce chapitre nous allons nous intéresser aux **Resilient Distributed Datasets (RDD)**.\n",
    "\n",
    "Ce premier exercice a pour but de se familiariser avec les concepts de base de Spark et plus généralement de la distribution de données. La structure RDD est la structure élémentaire de Spark, elle est souple et optimale en performance pour toute opération linéaire. Cette structure a cependant une performance limitée lorsqu'il s'agit d'opérations non linéaires, c'est pour cela que nous introduirons la structure DataFrame dans le chapitre suivant.\n",
    "\n",
    "Tout au long de cette formation, nous travaillerons sur une base de données proposée par IBM que nous avons mise en forme (version originale disponible ici) contenant tous les vols effectués par Alaska Airlines sur l'année 2008 ainsi que les informations relatives aux annulations et retards des différents vols.\n",
    "\n",
    "## 1. Introduction au SparkContext\n",
    "Avant de commencer à coder, il est important de comprendre les bases de fonctionnement de Spark. Toute fonction Spark est appelée à partir d'un SparkContext.\n",
    "\n",
    "SparkContext est l'objet qui gère les connexions avec le cluster Spark et coordonne les différents calculs sur les différents clusters. Un des très gros avantages de Spark réside dans le fait que nous ne pourrions définir le SparkContext qu'une seule fois, et que le code est totalement indépendant du SparkContext. Si nous travaillons sur un cluster de 8 machines par exemple, la distribution des calculs se fait par le biais du SparkContext, mais le reste du code reste identique. Il est donc possible de développer son code en local ou sur une machine quelconque. Pour envoyer le programme en production sur un **cluster**, il suffira de changer la définition du SparkContext. Par convention, sc désignera le SparkContext tout au long de la formation."
   ]
  },
  {
   "attachments": {
    "55bba2c2-26bc-43a6-9026-ee4b382c1b50.jpg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "id": "89be8655-5504-425f-9931-bb4c1a9c1153",
   "metadata": {},
   "source": [
    "![pyspark_1.jpg](attachment:55bba2c2-26bc-43a6-9026-ee4b382c1b50.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd937e12-b581-4a5a-a396-e99318d0391b",
   "metadata": {},
   "source": [
    "- (a) Exécuter la cellule ci-dessous pour importer SparkContext et définir un SparkContext local.   \n",
    "   L'exécution deux fois successives de la cellule ci-dessous génère une erreur. En général, il est conseillé d'utiliser SparkContext.getOrCreate() qui récupère un contexte Spark s'il y en a un, ou le crée sinon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c4fa13-8290-43eb-abe0-faefc0578587",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "probleme de version spark et pyspark pour la version python...trouver une solution\n",
    "Cannot run program \"python3\": CreateProcess error=2, Le fichier spécifié est introuvable\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42e944a-383a-4171-b7d7-76246a735be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark  # ?? leur version = 2.4.3, ici 3.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f6de49-99d8-4c41-9f0a-3b274a804ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version  # ok verion python ici = 3.9.12 ( et 3.9.13 sur spyder-env )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcf7b4c-a2d6-44af-815f-f3f67d4b4175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de SparkContext du module pyspark\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Définiton d'un SparkContext en local\n",
    "sc = SparkContext('local')\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b212982",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f046031c-f190-475c-8f62-ee023f4cc0a3",
   "metadata": {},
   "source": [
    "Un RDD est la représentation Spark d'un tableau de données, c'est une collection d'éléments que l'on peut utiliser pour contenir des tuples, des dictionnaires, des listes, etc. La structure RDD ressemble à une liste Python mais elle est distribuée sur plusieurs clusters et sa vitesse d'exécution est optimisée pour l'ensemble des opérations linéaires.\n",
    "\n",
    "Nous insistons sur l'avantage des programmes Spark à être identique pour travailler en local ou sur plusieurs clusters. Localement, Spark simule une distribution des calculs en divisant la mémoire de l'ordinateur sans qu'aucune modification du code soit nécessaire : il est possible de développer sur sa machine puis de mettre en production ce code sur un cluster en ne modifiant que le SparkContext.\n",
    "\n",
    "La méthode textFile d'un SparkContext permet de charger un fichier CSV (ou un fichier compressé) dans un RDD. Cette fonction s'utilise ainsi :\n",
    "\n",
    "sc.textFile(\"path_to_file.ext\")   \n",
    "\n",
    "- (b) Importer la base de données \"2008_raw.csv\" dans un RDD appelé raw_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b520cd-f435-4457-922e-6169ac5c0fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer la bibliothèque time et calcul du temps au début de l'exécution (t0)\n",
    "from time import time\n",
    "t0 = time()\n",
    "\n",
    "#Lecture du fichier \"2008_raw.csv\"\n",
    "raw_rdd = sc.textFile(\"raw_2008.csv\")\n",
    "# lien de la data\n",
    "# https://raw.githubusercontent.com/nraychaudhuri/ibm-spark-examples/master/data/airline-flights/alaska-airlines/2008.csv\n",
    "\n",
    "### Ne pas modifier le code ci-dessous\n",
    "# Calcul du temps de la lecture du fichier\n",
    "t1 = time() - t0\n",
    "print(\"Réalisé en {} secondes\".format(round(t1,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca51746-21ad-4bf6-acfc-59e329c5db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"raw_2008.csv\")\n",
    "df.info()    # PB eux 18 colonnes seulement, moi 29 colonnes !!! meme nombre de lignes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1772f80-5afd-466b-8bf8-459a5956c414",
   "metadata": {},
   "source": [
    "    - Une caractéristique importante des RDD est leur immutabilité. Une fois créés, ils ne peuvent plus être modifiés. Il faudra donc créer un nouveau RDD chaque fois que l'on voudra le modifier: une opération à éviter au maximum pour préserver la mémoire de l'ordinateur ou du serveur. Ce caractère immutable est essentiel pour le fonctionnement du calcul distribué : il garantit qu'une fois le RDD distribué sur un cluster, aucune modification n'aura lieu sur une partie du RDD qui pourrait fausser les résultats finaux.\n",
    "## 2. Évaluation Paresseuse\n",
    "L'appel à la fonction a été exécuté en un temps bref. Cela est lié au second avantage de cette structure qui est leur capacité à évaluer le code de façon paresseuse (lazy evaluation), c'est à dire reporter le lancement des calculs jusqu'à ce que ce soit absolument nécessaire.\n",
    "\n",
    "Par exemple, lorsque la méthode sc.textFile a été appelée, seul un pointeur vers le fichier a été créé. Ce n'est qu'au dernier moment, lorsque l'on recherche à afficher ou utiliser un résultat, que le calcul est effectué.\n",
    "\n",
    "Pour apercevoir le contenu d'un RDD, on doit utiliser des méthodes comme take(n) permet de retourner les n premiers éléments d'un RDD. Toutes les méthodes PySpark sont ici très puissantes. En effet, PySpark est une bibliothèque qui interface avec Python en cachant le langage Scala. Par exemple, la méthode take renvoie une liste Python d'éléments mais est allée chercher cette information au sein d'un cluster Spark codé en Scala.\n",
    "\n",
    "La force de PySpark réside dans l'exploitation des performances de l'architecture de Spark sans coder une seule ligne de Scala.\n",
    "\n",
    "- (a) Afficher les 5 premiers éléments de raw_rdd à l'aide de la méthode take.\n",
    "- (b) Mesurer le temps nécessaire à l'affichage de ces 5 éléments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe920119-730e-4270-ae69-141b2c1012bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du temps au début de l'exécution (t0)\n",
    "t0 = time()\n",
    "\n",
    "# Affichage de 5 éléments du rdd\n",
    "print(raw_rdd.take(5))\n",
    "\n",
    "# Calcul du temps de l'affichage des 5 éléments\n",
    "t1 = time() - t0\n",
    "print(\"Réalisé en {} secondes\".format(round(t1,3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df8ce3-abcf-456f-9c4f-fe9f8640e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
    ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (HappyNous executor driver): \n",
    "java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, Le fichier spécifié est introuvable\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20ccbad-f768-42d7-8737-1054221b20db",
   "metadata": {},
   "source": [
    "    - Le temps de calcul pour charger 5 lignes est donc plus grand que celui de la méthode textFile. En effet, textFile construit simplement un pointeur vers la donnée alors que take(5) calcule les résultats pour les 5 premières lignes. Vous êtes invité à faire varier le nombre de lignes pour tester le temps de calcul de votre ordinateur : testez avec 10, 100, 1000, etc.  \n",
    "    \n",
    "    - Vous pouvez remarquer que les colonnes d'un RDD ne sont pas \"nommées\". Les RDD sont extrêmement rapides pour le traitement des données en ligne mais ne possèdent pas de structure inhérente permettant de faire des traitements en colonne.  \n",
    "    \n",
    "La méthode count qui permet de compter le nombre de lignes du RDD demande d'aller chercher les informations au sein du fichier CSV et donc demande un léger temps de calcul.\n",
    "\n",
    "- (c) Utiliser la méthode count pour enregistrer dans une variable nommée count le nombre de lignes du RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc2ef8f-fd60-445e-b936-a37197bb9ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du temps au début de l'exécution (t0)\n",
    "t0 = time()\n",
    "\n",
    "# Insérez votre code ici\n",
    "\n",
    "\n",
    "\n",
    "### Ne pas modifier le code ci-dessous\n",
    "# Calcul du temps d'affichage du nombre de lignes du RDD\n",
    "t1 = time() - t0\n",
    "print(\"Nombre de lignes :\", count)\n",
    "print(\"Réalisé en {} secondes\".format(round(t1,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70174d78-a544-4532-9eb2-301190a6b2ed",
   "metadata": {},
   "source": [
    "## 3. Map & Reduce\n",
    "Un RDD lit un fichier ligne par ligne. Dans son format actuel, il permet d'afficher certaines lignes ou de compter le nombre de lignes. Chaque ligne est un simple string contenant l'ensemble des données séparées par des virgules. Il faut donc séparer chaque ligne en une liste d'éléments pour pouvoir les traiter séparément par la suite.\n",
    "\n",
    "Pour effectuer des traitements sur les lignes d'un RDD, il est possible d'utiliser la méthode map qui prend en argument une fonction à appliquer sur chaque ligne. N'importe quelle fonction peut être mise en argument dans map, à partir du moment où elle prend une ligne en entrée et renvoie en sortie une ligne à stocker dans un nouveau RDD.\n",
    "\n",
    "\n",
    "\n",
    "Exemple : Renvoyer le premier élément de chaque ligne\n",
    "-     raw_rdd.map(lambda line: line[0])   \n",
    "\n",
    "\n",
    "Fonctions anonymes\n",
    "\n",
    "Le mot clé lambda permet de créer une fonction en une seule ligne. Par exemple, il existe deux façons de créer la fonction fun : \n",
    "\n",
    "• def fun(x):   \n",
    "    return x[0]   \n",
    "• fun = lambda x: x[0]   \n",
    "\n",
    "Cette écriture a l'avantage de pouvoir être écrite directement au sein du code, sans avoir besoin de créer la fonction fun pour l'utiliser. Le même résultat est obtenu en créant une fonction fun puis en l'appelant dans map : raw_rdd.map(fun)\n",
    "\n",
    "- (a) Créer un RDD nommé airplane_rdd dont chaque ligne est la liste des éléments de la ligne correspondante de raw_rdd.\n",
    "- (b) Afficher la première ligne de ce nouveau RDD.   \n",
    "    -  La méthode split permet de transformer une chaîne de caractères en une liste. Par exemple, la fonction qui a une chaîne de caractère line associe la liste line.split(\",\") isole tous les éléments de la ligne originellement séparés par des virgules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5123b3a4-9b99-457d-9aff-d06f6086384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un RDD dont les lignes sont la liste des éléments de raw_rdd \n",
    "airplane_rdd = raw_rdd.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Affichage de la première ligne du rdd\n",
    "airplane_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4352f33-382c-4b41-89b9-e77099431f45",
   "metadata": {},
   "source": [
    "Les RDD fonctionnent très bien pour des opérations \"ligne par ligne\", c'est pour cela que les méthodes map et reduce sont extrêmement efficaces pour ces calculs. Appliquer successivement la méthode map puis la méthode reduceByKey permet de résumer intelligemment nos données.\n",
    "\n",
    "Par exemple, pour construire un diagramme à barres du nombre de vols effectués par aéroport de départ (l'aéroport de départ est la 8ème variable), il faut procéder en plusieurs étapes :\n",
    "\n",
    "- (c) Utiliser la méthode map pour regrouper les données dans un couple grâce à la fonction qui à line associe le couple (line[7], 1). Cela nous permet d'avoir pour chaque ligne :\n",
    "    - Une clé correspondant à l'aéroport de départ.\n",
    "    - Une valeur égale à 1 (à additionner ultérieurement).    \n",
    "    \n",
    "reduceByKey(f) combine des couples ayant la même clé en leur appliquant une fonction f. Il est donc possible ici d'appliquer la fonction f qui à un couple (x,y) associe x+y de façon à additionner successivement toutes les valeurs et donc obtenir le nombre d'apparition de chaque clé.    \n",
    "   \n",
    "- (d) Créer un RDD nommé hist_rdd, en appliquant à airplane_rdd la méthode map puis la méthode reduceByKey comme expliqué ci-dessus.\n",
    "- (e) Afficher les 5 premières lignes de hist_rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c006e36e-7f3f-45dd-8ba1-699b09480ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un nouveau RDD en résumant les lignes par aéroport de départ\n",
    "hist_rdd = airplane_rdd.map(lambda x: (x[7], 1)).reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "# Affichage d'un 5 premières lignes \n",
    "hist_rdd.take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88122371-9d5d-4a5c-bdf0-9353ccd7e980",
   "metadata": {},
   "source": [
    "     - De façon plus générale, les techniques de map et reduce permettent de résumer les données et s'effectuent généralement de la façon suivante :   \n",
    "     \n",
    "• Créer un couple (key, value) sur chaque ligne grâce à map.   \n",
    "• Regrouper les clés grâce à reduceByKey en effectuant l'opération de notre choix sur les valeurs.   \n",
    "\n",
    "## 4. La méthode collect pour forcer l'évaluation \n",
    "Les évaluations précédentes ont prouvé que les RDD sont évalués paresseusement. La méthode collect offre la possibilité de forcer son calcul. Cette méthode est très pratique pour afficher des RDD de petite taille, notamment ceux issus d'un map ou reduce.\n",
    "\n",
    "     - La méthode collect prend en entrée un RDD et renvoie une liste correspondante au contenu du RDD. Le résultat de cette fonction se traite donc comme une liste et ne possède plus les attributs d'un RDD. Il est notamment non distribué et ne bénéficie plus de l'architecture de Spark.\n",
    "- (a) Créer puis afficher la liste hist à partir du RDD hist_rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3076ae-54dc-45d1-b57c-641c406c83fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une liste à partir d'un RDD\n",
    "hist = hist_rdd.collect()\n",
    "\n",
    "# Affichage de la liste \n",
    "print(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7b6be7-5086-421d-b3e2-9663cc309a0e",
   "metadata": {},
   "source": [
    "## 5. Trier et filtrer un RDD \n",
    "Beaucoup d'aéroports sont présents dans la liste ce qui la rend peu lisible au premier coup d'œil. Il est donc intéressant de trier cette liste par rapport aux valeurs de chaque clé (i.e.le nombre de vol pour chaque aéroport).\n",
    "\n",
    "Pour cela, il est possible d'utiliser la fonction sorted qui prend en paramètre :\n",
    "\n",
    "- list : la liste à trier.\n",
    "- key : la liste des éléments à utiliser par référence de tri.\n",
    "- reverse : un entier indiquant l'ordre du tri : croissant (reverse = 0) ou décroissant (reverse = 1).  \n",
    "\n",
    "Exemple : Tri par ordre alphabétique sur le premier élément   \n",
    "sorted(hist_rdd.collect(), key=lambda x: x[0])   \n",
    "\n",
    "- (a) Afficher hist_rdd ordonné de façon décroissante par rapport au nombre de vol par aéroport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34bba73-0835-4fd3-9cbc-8a106ce30f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tri des données par ordre décroissant\n",
    "sorted(hist, key= lambda x: x[1], reverse= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf9943a-82b5-493a-ac06-e65dfb40d002",
   "metadata": {},
   "source": [
    "D'après le tri, l'aéroport de Seattle est de loin l'aéroport le plus desservi.\n",
    "\n",
    "Il est souvent intéressant de pouvoir filtrer nos données. En effet, les fichiers CSV possèdent un header contenant le nom de la variable qui va être interprétée comme une modalité. Il existe pour cela la méthode filter qui permet filtrer certaines lignes.\n",
    "\n",
    "Exemple : Modalité de la première colonne\n",
    "airport_rdd.filter(lambda x: x[0] == \"year\")   \n",
    "\n",
    "- (b) Afficher le nombre de vols annulés par ville d'origine. Pour cela :\n",
    "    - Filtrer les vols annulés ( line[10] ) à l'aide de filter.\n",
    "    - Effectuer un map et reduce pour compter le nombre d'occurrence par ville d'origine (line[8]).\n",
    "    - Afficher les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb730d-03f4-4ebb-b63f-8e7a0fdfbfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul et affichage du nombre de vols annulés par ville d'origine\n",
    "airplane_rdd \\\n",
    "    .filter(lambda x: x[10] == \"1\") \\\n",
    "    .map(lambda x: (x[8], 1)) \\\n",
    "    .reduceByKey(lambda x,y: x+y) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0786777-96f4-4891-8a98-05d860b0ed8d",
   "metadata": {},
   "source": [
    "- Il est possible d'écrire le code en une seule ligne. L'utilisation du \"\\\" permet de passer à la ligne entre chaque méthode. En effet, comme chaque méthode PySpark utilisée ici crée un nouvel objet, il est alors optimal d'effectuer toutes les opérations successivement. L'utilisation de \"\\\" permet donc un code plus lisible. Par exemple : \n",
    "     -  rdd \\\n",
    "        -  .filter(fun1) \\\n",
    "        - .map(fun2) \\\n",
    "        - .reduceByKey(fun3) \\\n",
    "        - .collect()    \n",
    "        \n",
    "Une fois que toutes les opérations sont effectuées et terminées, il faut avoir le bon réflexe de fermer le SparkContext ouvert en début d'exercice. En effet, on ne peut pas avoir plus d'un contexte sur une machine et pour éviter les conflits à l'avenir, fermer son SparkContext permet de s'assurer que plus aucun programme ne tourne sur la machine. Pour cela il existe tout simplement la méthode stop sans paramètre.\n",
    "\n",
    "- (c) Fermer sc, le SparkContext de l'exercice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "66cc5269-ab18-4f4a-8c62-330edcab2c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fermeture du SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7769fbc9-771d-42e2-8e0a-b6d801f8220f",
   "metadata": {},
   "source": [
    "# B - Data Processing \n",
    "Introduction\n",
    "La structure RDD que nous avons étudiée est importante. Cependant, lorsqu'il s'agit de gérer des bases de données structurées (organisées en colonnes), nous verrons qu'il existe une structure plus optimale. Les RDDs sont néanmoins extrêmement performants devant un problème non structuré tel qu'un texte.\n",
    "\n",
    "Dans cet exercice, nous allons utiliser la structure RDD pour une base de données non structurée : l'intégrale des Misérables de Victor Hugo. L'objectif est de construire un simple WordCount comptant le nombre d'occurrences de chaque mot, pour approfondir et mettre en pratique vos compétences en RDD et map reduce.\n",
    "\n",
    "Pour cet exercice, nous considérons un fichier texte contenant l'intégralité des Misérables de Victor Hugo.\n",
    "\n",
    "## 1. Création d'un SparkContext\n",
    "(a) Exécuter la cellule ci-dessous pour importer SparkContext et définir un SparkContext local nommé sc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adfd9a8-0fb9-4e04-86a8-2742e5e2f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de SparkContext du module pyspark\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Défintion d'un SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8fd068-0b90-4ea6-93a7-ac8d5e1bf143",
   "metadata": {},
   "source": [
    "## 2. Importation de la base de données\n",
    "\n",
    "- (a) Importer le fichier miserables_full.txt dans un RDD nommé miserables.\n",
    "- (b) Afficher les 10 premières lignes du RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b1cd3c-77fb-4d17-b59b-20e9dd5676f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du fichier \"miserables_full.txt\" et affichage des 10 premières lignes\n",
    "miserables = sc.textFile(\"miserables_full.txt\")\n",
    "miserables.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391ed5ae-b901-4011-b6c6-48096a10e6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ->\n",
    "['Les Misérables Tome 1 - Fantine',\n",
    " 'Livre Premier - Un Juste',\n",
    " 'Chapitre I - Monsieur Myriel',\n",
    " 'En 1815, M. Charles-François-Bienvenu Myriel était évêque de Digne. C’était un vieillard d’environ soixante-quinze ans ; il occupait le siège de Digne depuis 1806.',\n",
    " 'Quoique ce détail ne touche en aucune manière au fond même de ce que nous avons à raconter, il n’est peut-être pas inutile, ne fût-ce que pour être exact en tout, d’indiquer ici les bruits et les propos qui avaient couru sur son compte au moment où il était arrivé dans le diocèse. Vrai ou faux, ce qu’on dit des hommes tient souvent autant de place dans leur vie et surtout dans leur destinée que ce qu’ils font. M. Myriel était fils d’un conseiller au parlement d’Aix ; noblesse de robe. On contait de lui que son père, le réservant pour hériter de sa charge, l’avait marié de fort bonne heure, à dix-huit ou vingt ans, suivant un usage assez répandu dans les familles parlementaires. Charles Myriel, nonobstant ce mariage, avait, disait-on, beaucoup fait parler de lui. Il était bien fait de sa personne, quoique d’assez petite taille, élégant, gracieux, spirituel ; toute la première partie de sa vie avait été donnée au monde et aux galanteries. La révolution survint, les événements se précipitèrent, les familles parlementaires décimées, chassées, traquées, se dispersèrent. M. Charles Myriel, dès les premiers jours de la révolution, émigra en Italie. Sa femme y mourut d’une maladie de poitrine dont elle était atteinte depuis longtemps. Ils n’avaient point d’enfants. Que se passa-t-il ensuite dans la destinée de M. Myriel ? L’écroulement de l’ancienne société française, la chute de sa propre famille, les tragiques spectacles de 93, plus effrayants encore peut-être pour les émigrés qui les voyaient de loin avec le grossissement de l’épouvante, firent-ils germer en lui des idées de renoncement et de solitude ?',\n",
    " 'Fut-il, au milieu d’une de ces distractions et de ces affections qui occupaient sa vie, subitement atteint d’un de ces coups mystérieux et terribles qui viennent quelquefois renverser, en le frappant au cœur, l’homme que les catastrophes publiques n’ébranleraient pas en le frappant dans son existence et dans sa fortune ? Nul n’aurait pu le dire ; tout ce qu’on savait, c’est que, lorsqu’il revint d’Italie, il était prêtre.',\n",
    " 'En 1804, M. Myriel était curé de Brignolles. Il était déjà vieux, et vivait dans une retraite profonde.',\n",
    " 'Vers l’époque du couronnement, une petite affaire de sa cure, on ne sait plus trop quoi, l’amena à Paris. Entre autres personnes puissantes, il alla solliciter pour ses paroissiens M. le cardinal Fesch. Un jour que l’empereur était venu faire visite à son oncle, le digne curé, qui attendait dans l’antichambre, se trouva sur le passage de sa majesté. Napoléon, se voyant regardé avec une certaine curiosité par ce vieillard, se retourna, et dit brusquement :',\n",
    " '— Quel est ce bonhomme qui me regarde ?',\n",
    " '— Sire, dit M. Myriel, vous regardez un bonhomme, et moi je regarde un grand homme. Chacun de nous peut profiter.']\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7783ad7-fc2d-44f1-b20e-dafa8c468e4e",
   "metadata": {},
   "source": [
    "3. Mise en forme de la base\n",
    "\n",
    "Pour compter le nombre d'occurrences d'un mot, il faut mettre tous les mots en forme de la même façon :\n",
    "\n",
    "    - Mettre toutes les lettres en minuscule grâce à la méthode lower des chaînes des caractères.\n",
    "    - Remplacer la ponctuation collée aux mots ( .  ,  -  ' ) par des espaces grâce à la méthode replace des chaînes de caractères, qui s'applique de la façon suivante :\n",
    "    str.replace(',', ' ')\n",
    "    \n",
    "- (a) Créer un RDD nommé miserables_clean, qui contient le texte des Misérables en minuscule et sans ponctuation à l'aide des méthodes map, lower et replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72235067-4513-4771-8d22-36cc3a21d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un RDD nettoyé\n",
    "miserables_clean = miserables.map(lambda x : x.lower().replace(',', ' ').replace('.', ' ').replace('-', ' ').replace('’', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82e0864-1c3f-44df-bd7a-bac514285845",
   "metadata": {},
   "source": [
    "Maintenant que tous les mots sont bien séparés par des espaces, il est possible utiliser la méthode split pour séparer les mots de chaque ligne.\n",
    "\n",
    "Il est possible d'utiliser la méthode map mais le problème ici sera que map crée une liste de listes.\n",
    "\n",
    "En effet, un split transforme chaque ligne en liste et la base de données étant une liste de lignes, le résultat final sera une liste de listes. Dans ce cas, il est coûteux de comparer un même mot sur deux lignes différentes car ils sont tous les deux dans des listes différentes.\n",
    "\n",
    "Pour pallier ce problème, il existe une méthode flatMap, qui s'utilise exactement de la même façon que map, à la différence que le résultat final est un RDD à une seule dimension.\n",
    "\n",
    "- (b) Créer un RDD nommé miserables_flat qui contient l'ensemble des mots sous une seule dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71024fe4-4d97-48d4-bf63-5e031ec5932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ->\n",
    "['les', 'misérables', 'tome', '1', '', '', 'fantine', 'livre', 'premier', '']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5487cce1-641c-4e65-9b5b-7f2201b08402",
   "metadata": {},
   "source": [
    "## 4. Map-Reduce\n",
    "\n",
    "- (a) À partir de miserables_flat, créer un RDD mots contenant l'ensemble des couples (mot, nb_occurences) à l'aide des méthodes map et reduceByKey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088e976b-9df4-4903-a3b9-0f609b3355e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création d'un RDD contenant l'ensemble des couples (mot, nb_occurences) \n",
    "mots = miserables_flat.map(lambda x : (x,1)) \\\n",
    "                      .reduceByKey(lambda x,y : x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e80c29-f991-4cf5-899c-2127c43a8ee7",
   "metadata": {},
   "source": [
    " Pour compter le nombre d'occurrences d'un élément, une technique consiste à :\n",
    "\n",
    "    - Utiliser la méthode map pour créer un couple clé/valeur où chaque mot est une clé, chaque valeur vaut 1.\n",
    "    - Utiliser reduceByKey pour additionner les valeurs pour chaque mot.\n",
    "\n",
    "- (b) Créer une liste contenant les couples (mot, occurrence) ordonnée dans l'ordre croissant des occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6834dd8c-3c3a-4ccd-adfa-79eca7509f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Première méthode de tri\n",
    "\n",
    "# Tri en utilisant la fonction 'sorted' des RDD\n",
    "mots_sorted  = sorted(mots.collect(),\n",
    "                     key= lambda x: x[1],\n",
    "                     reverse= 0)\n",
    "\n",
    "### Deuxième méthode de tri\n",
    "\n",
    "# Tri en utilisant la fonction 'sortBy' des RDD puis convertir en liste en utilisant collect\n",
    "mots_sorted_2 = mots.sortBy(lambda couple: couple[1], ascending = True) \\\n",
    "                    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be08e8-d3ca-40a6-a5ec-dfb02d316765",
   "metadata": {},
   "source": [
    "## Succession de méthodes\n",
    "\n",
    "Enfin, beaucoup d'objets ont été créés jusque-là pour des raisons pédagogiques mais il serait plus optimal de créer l'objet final directement grâce à un enchaînement de méthodes.\n",
    "\n",
    "- (a) Créer une liste mots_sorted_3 directement à partir du fichier miserables_full.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6966bfc0-729d-4617-af65-37f20d57d466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une liste à partir du fichier texte\n",
    "mots_sorted_3 = sc.textFile(\"miserables_full.txt\") \\\n",
    "                  .map(lambda x : x.lower().replace(',', ' ').replace('.', ' ').replace('-', ' ').replace('’', ' ')) \\\n",
    "                  .flatMap(lambda line: line.split(\" \")) \\\n",
    "                  .map(lambda x : (x,1)) \\\n",
    "                  .reduceByKey(lambda x,y : x + y) \\\n",
    "                  .sortBy(lambda couple: couple[1], ascending = True) \\\n",
    "                  .collect()\n",
    "                \n",
    "mots_sorted_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcbf3ea-92e4-4e3d-8ead-2d126f4bbace",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ->\n",
    "[('hériter', 1),\n",
    " ('1804', 1),\n",
    " ('racontages', 1),\n",
    " ('maturité', 1),\n",
    " ('replète', 1),\n",
    " ('affairée', 1),\n",
    " ('impériaux', 1),\n",
    " ('classent', 1),\n",
    " ('installation', 1),\n",
    " ('attenant', 1),\n",
    " ('1712', 1),\n",
    " ('brûlart', 1),\n",
    " ('mesgrigny', 1),\n",
    " ('lérins', 1),\n",
    " ('berton', 1),\n",
    " ('vence', 1),\n",
    " ('décoraient', 1),\n",
    " ('épidémies', 1),\n",
    " ('montdidier', 1),\n",
    " ('missions', 1),\n",
    " ('supplément', 1),\n",
    " ('adhérait', 1),\n",
    " ('vota', 1),\n",
    " ('confidentiel', 1),\n",
    " ('extrayons', 1),\n",
    " ('authentiques', 1),\n",
    " ('durance', 1),\n",
    " ('calotins', 1),\n",
    " ('brouillaient', 1),\n",
    " ('draguignan', 1),\n",
    " ('rachats', 1),\n",
    " ('dispenses', 1),\n",
    " ('chapelles', 1),\n",
    " ('offrandes', 1),\n",
    " ('affluèrent', 1),\n",
    " ('trésorier', 1),\n",
    " ('bienfaits', 1),\n",
    " ('changeât', 1),\n",
    " ('ajoutât', 1),\n",
    " ('énoncent', 1),\n",
    " ('modèles', 1),\n",
    " ('rapportent', 1),\n",
    " ('divisées', 1),\n",
    " ('taxe', 1),échez', 1),\n",
    " ('instruisait', 1),\n",
    " ('extasiait', 1),\n",
    " ('cabanon', 1),\n",
    " ('bénir', 1),\n",
    " ('consolant', 1),\n",
    " ('réconciliée', 1),\n",
    " ('dressent', 1),\n",
    " ('absorber', 1),\n",
    "\n",
    " ........ \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a20d343-2ba9-428b-ba9e-434a6f189ea9",
   "metadata": {},
   "source": [
    "- (b) Fermer le SparkContext sc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69aa644-f9fe-4874-a931-ddc5733852ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fermeture du SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5c1e20-3bf6-44d0-8f4b-b9d0effb29ca",
   "metadata": {},
   "source": [
    "# C - Les DataFrames \n",
    "\n",
    "## Introduction\n",
    "La structure RDD n'est pas optimisée pour effectuer des tâches par colonne ou du Machine Learning. La structure DataFrame a été créé pour répondre à ce besoin. Elle utilise de façon sous-jacente les bases d'un RDD mais a été structurée en colonnes autant qu'en lignes dans une structure SQL et une forme inspirée des DataFrame du module pandas.\n",
    "\n",
    "La structure DataFrame possède deux grands avantages. Tout d'abord cette structure est similaire au DataFrame pandas et est donc facile à prendre en main. Elle est également performante : un DataFrame en PySpark est aussi rapide qu'un DataFrame en Scala et est la structure distribuée la plus optimisée en Machine Learning. Grâce à la structure DataFrame, nous pouvons donc faire des calculs performants à travers un langage familier, en évitant le coût d'entrée d'apprentissage d'un nouveau langage fonctionnel : Scala.\n",
    "\n",
    "Dans cet exercice, vous apprendrez à manipuler un DataFrame PySpark pour explorer les données.\n",
    "\n",
    "## 1. Spark SQL\n",
    "Spark SQL est un module de Spark qui permet de travailler sur des données structurées. C'est donc au sein de ce module qu'a été développé le DataFrame de Spark et Spark SQL vient se rajouter à Spark pour y apporter de nombreux éléments de structure. En particulier, il introduit la notion de SparkSession, originellement le point d'entrée de Spark SQL, mais qui est devenu le point d'entrée unifié de Spark.\n",
    "\n",
    "\n",
    "\n",
    "- (a) Exécuter la cellule ci-dessous pour construire la SparkSession de l'exercice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab8ae2c8-6fe4-4189-b070-b42aab5618b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://HappyNous:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1714e42b100>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import de Spark Session et SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Définition d'un SparkContext\n",
    "SparkContext.getOrCreate() \n",
    "\n",
    "# Définition d'une SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Introduction au DataFrame\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c0a000-19e2-41a1-a3db-ac23ec4c7d43",
   "metadata": {},
   "source": [
    "Deux options ont été mises en place dans la construction de la SparkSession :\n",
    "\n",
    "    - La méthode appName qui permet de donner à la SparkSession un nom pour connaître l'environnement de travail.\n",
    "    - La méthode getOrCreate qui permet d'utiliser la session en cours si une session est déjà ouverte.   \n",
    "    \n",
    "En effet, cette dernière est importante car il n'est pas possible d'ouvrir deux sessions sur une seule machine.\n",
    "\n",
    "Le nom sert surtout d'indicateur pour connaître la session en cours : si une session existe déjà, elle ne sera pas renommée et affichera l'ancien nom.\n",
    "\n",
    "SparkSession est une couche supérieure à SparkContext. Ce dernier est avantageux car il permet de travailler avec des RDD.\n",
    "\n",
    "- (b) Exécuter la cellule ci-dessous pour créer un raccourci vers le SparkContext déjà créé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "757756bb-e9ef-4152-9e98-64e975eb4dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://HappyNous:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Création d'un raccourci vers le SparkContext déjà créé\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91da04ab-632a-4c52-bf53-360f1a85a5f6",
   "metadata": {},
   "source": [
    "     - Spark SQL possède une documentation en une seule page (idéale pour faire des recherches automatiques à l'aide de CTRL+F) assez riche et qui propose des exemples en plus des explications. Contrairement à ce que vous pouvez trouver sur internet, cette documentation est le seul document régulièrement mis à jour avec la dernière version de Spark.  \n",
    "     \n",
    "## 2. Créer un DataFrame Spark\n",
    "\n",
    "Il est possible de créer un DataFrame soit à partir d'un RDD soit directement à partir d'un fichier CSV.\n",
    "\n",
    "## 1. Créer Un DataFrame à partir d'un RDD\n",
    "\n",
    "La structure DataFrame a été implémentée par Spark SQL, le module Spark pour le traitement de données structurées. Dans sa forme, c'est tout simplement un RDD dont chaque ligne est un Row. Les Rows permettent de donner des noms à chacune des colonnes pour insérer une structure supplémentaire à la donnée.\n",
    "\n",
    "Par exemple, pour un RDD nommé rdd contenant deux éléments sur chaque ligne, le nom d'un individu et son âge, il est possible d'appliquer sur chaque ligne le constructeur de la classe Row en donnant un nom à chaque variable.\n",
    "\n",
    "La création d'un DataFrame à partir d'un rdd se fait en appliquant la méthode createDataFrame de SparkContext.\n",
    "\n",
    "Exemple : Créer un DataFrame à partir d'un RDD\n",
    "    rdd_row = rdd.map(lambda line: Row(name = line[0],\n",
    "                                       age = line[1]))                                       \n",
    "    df = spark.createDataFrame(rdd_row)\n",
    "    \n",
    "- (a) Importer la classe Row à partir du module pyspark.sql.\n",
    "- (b) Importer la base de données 2008_raw.csv.\n",
    "- (c) Créer un rdd à partir de cette base de données.\n",
    "- (d) Créer un rdd_row à l'aide de la méthode map en appliquant sur chaque ligne la structure Row avec les variables explicatives annee, mois, jours et flightNum.\n",
    "- (e) Créer un DataFrame df à partir de rdd_row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9d0417-9afb-4a98-a06e-f19d52dfdb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de Row du package pyspark.sql\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Chargement du fichier '2008_raw.csv'\n",
    "rdd = sc.textFile('2009_raw.csv').map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Création d'un nouveau RDD en sélectionnant les variables explicatives\n",
    "rdd_row = rdd.map(lambda line: Row(annee = line[0],\n",
    "                                   mois = line[1],\n",
    "                                   jours = line[2],\n",
    "                                   flightNum = line[5]))\n",
    "\n",
    "# Créer d'un data frame à partir d'un rdd\n",
    "df = spark.createDataFrame(rdd_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b077ede2-d925-47a4-bcd8-c8f0893dde89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ne marche pas\n",
    "j'ai récupéré le csv, sav en , séparateur, pas d'index ni noms de colonnes vues avec pandas sur le csv...\n",
    "meme en basculant les colonnes en lignes et avec , ou ; \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1e51a7-e196-44f0-952d-cb409c4bd9ef",
   "metadata": {},
   "source": [
    "Il est important de savoir afficher quelques lignes d'un DataFrame pour vérifier que le tableau de données est bien construit. De plus, il est pertinent d'utiliser la méthode show(n) qui affiche de façon claire les n premières lignes du DataFrame.\n",
    "\n",
    "   Il est possible d'exécuter la méthode take qui affiche les n premières lignes en revenant sous la forme d'un RDD de Rows peu lisible.\n",
    "- (f) Afficher les 5 premières lignes de df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d99a08-1866-4fc3-869f-a47d22435b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des 5 premières lignes\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1df85fc-da0f-4de7-b5ca-a2c6e9d577d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----+---------+-----+----+\n",
    "|annee|flightNum|jours|mois|\n",
    "+-----+---------+-----+----+\n",
    "| 2008|      324|    1|   1|\n",
    "| 2008|      572|    1|   1|\n",
    "| 2008|      511|    1|   1|\n",
    "| 2008|      376|    1|   1|\n",
    "| 2008|      729|    1|   1|\n",
    "+-----+---------+-----+----+\n",
    "only showing top 5 rows\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c4ca59-d25c-48b7-9721-a0f4a7e222af",
   "metadata": {},
   "source": [
    "     - Créer un DataFrame à partir d'un RDD est théorique et permet de comprendre la structure sous-jacente. En général, la création d'un DataFrame se fait en une seule ligne en important un fichier CSV.\n",
    "     \n",
    "## 2. Créer un DataFrame à partir d'un fichier CSV\n",
    "La fonction read.csv de SparkSession permet de créer un DataFrame à partir d'un fichier CSV. Cette fonction permet de spécifier s'il existe une première ligne contenant les noms de variables en utilisant l'option header :\n",
    "\n",
    "header = True   # Cet argument signifie que le DataFrame contient une première \n",
    "                # ligne contenant les noms des variables   \n",
    "                \n",
    "- (g) Importer la base de données 2008.csv avec le header dans un DataFrame raw_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87fe3a58-6768-4f20-b571-efa41b23bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture du fichier '2008.csv'\n",
    "raw_df = spark.read.csv('2088.csv', header=True, sep=';') # sep par défaut = ,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c48ca30-8d61-4c9e-8efd-7a50add7b819",
   "metadata": {},
   "source": [
    "- (h) Afficher le schéma du DataFrame raw_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b11beae4-f32c-4586-840d-6249ed95431d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- annee: string (nullable = true)\n",
      " |-- jours: string (nullable = true)\n",
      " |-- mois: string (nullable = true)\n",
      " |-- heure: string (nullable = true)\n",
      " |-- uniqueCarrier: string (nullable = true)\n",
      " |-- flightNum: string (nullable = true)\n",
      " |-- tailNum: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- distance: string (nullable = true)\n",
      " |-- canceled: string (nullable = true)\n",
      " |-- cancelationCode: string (nullable = true)\n",
      " |-- diverted: string (nullable = true)\n",
      " |-- carrierDelay: string (nullable = true)\n",
      " |-- weatherDelay: string (nullable = true)\n",
      " |-- nasDelay: string (nullable = true)\n",
      " |-- securityDelay: string (nullable = true)\n",
      " |-- lateAircraftDelay: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Affichage du schéma des variables\n",
    "raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64928608-1ce1-4911-bfb2-e9d8f297afa0",
   "metadata": {},
   "source": [
    " !!!  Spark SQL n'infère pas correctement le type des variables : toutes sont considérées de **type string** ce qui gêne une partie des calculs, notamment en termes d'exploration de données. Afin de faire des calculs ou d'explorer les données, il est nécessaire de changer le type de certaines colonnes.  !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f5bd0a-a43a-4c9c-919b-27772c509aae",
   "metadata": {},
   "source": [
    "## 3. Explorer et manipuler un DataFrame\n",
    "Maintenant que les données sont dans un DataFrame, il est possible d'effectuer de nombreuses transformations similaires au langage SQL. Pour sélectionner les variables, il faut utiliser la méthode select.\n",
    "\n",
    "Exemple : Sélection des variables name et age\n",
    "    new_df = df.select('name','age')   \n",
    "    \n",
    "- (a) Créer flights1 un DataFrame contenant uniquement les variables : 'annee', 'mois', 'jours', 'flightNum', 'origin', 'dest', 'distance', 'canceled', 'cancellationCode', 'carrierDelay'.\n",
    "- (b) Afficher les 20 premières lignes de flights1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6921839-cf7a-42f6-8e6f-106f89d3af27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+---------+------+----+--------+--------+---------------+------------+\n",
      "|annee|mois|jours|flightNum|origin|dest|distance|canceled|cancelationCode|carrierDelay|\n",
      "+-----+----+-----+---------+------+----+--------+--------+---------------+------------+\n",
      "| 2008|   1|    1|      324|   SEA| SJC|     697|       0|            NaN|          NA|\n",
      "| 2008|   1|    1|      572|   SEA| PSP|     987|       0|            NaN|         NaN|\n",
      "| 2008|   1|    1|      511|   SAN| SEA|    1050|       0|            NaN|         0.0|\n",
      "| 2008|   1|    1|      376|   SEA| GEG|     224|       0|            NaN|         NaN|\n",
      "| 2008|   1|    1|      729|   TUS| SEA|    1216|       0|            NaN|         NaN|\n",
      "| 2008|   1|    1|      283|   LAX| SEA|     954|       0|            NaN|         NaN|\n",
      "| 2008|   1|    1|      211|   LAX| SEA|     954|       0|            NaN|         NaN|\n",
      "| 2008|   1|    1|      100|   ANC| PDX|    1542|       0|            NaN|         0.0|\n",
      "| 2008|   1|    1|      665|   LAS| SEA|     866|       0|            NaN|         NaN|\n",
      "| 2008|   1|    1|      531|   SJC| SEA|     697|       0|            NaN|         NaN|\n",
      "| 2008|   1|    1|      571|   SEA| DEN|    1024|       0|            NaN|        22.0|\n",
      "| 2008|   1|    1|      154|   ANC| SEA|    1449|       1|              A|         NaN|\n",
      "| 2008|   1|    1|      728|   SEA| TUS|    1216|       0|            NaN|         NaN|\n",
      "| 2008|   1|    1|      518|   SEA| SAN|    1050|       0|            NaN|        92.0|\n",
      "| 2008|   1|    1|      580|   SEA| SAN|    1050|       0|            NaN|        21.0|\n",
      "| 2008|   1|    1|       85|   SEA| ANC|    1449|       0|            NaN|         NaN|\n",
      "| 2008|   1|    1|      640|   SEA| LAS|     866|       0|            NaN|         NaN|\n",
      "| 2008|   1|    1|      292|   SEA| LAX|     954|       0|            NaN|         0.0|\n",
      "| 2008|   1|    1|      478|   SEA| PSP|     987|       0|            NaN|        15.0|\n",
      "| 2008|   1|    1|      485|   LAX| SEA|     954|       0|            NaN|         NaN|\n",
      "+-----+----+-----+---------+------+----+--------+--------+---------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Création d'un data frame ne contenant que les variables explicatives\n",
    "flights1 = raw_df.select('annee', 'mois', 'jours', 'flightNum', 'origin', 'dest', 'distance', 'canceled', 'cancelationCode', 'carrierDelay')\n",
    "\n",
    "# Affichage de 20 premières lignes\n",
    "flights1.show() # 'show' affiche 20 lignes par défaut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85377dc-e063-4d09-b5c8-585dc16e2cf2",
   "metadata": {},
   "source": [
    "   L'attribut columns permet d'obtenir une liste des variables. Par exemple : print(raw_df.columns) renvoie une liste complète des variables de df.\n",
    "Spark SQL offre également la structure Columns. Cette structure s'obtient en tapant le nom d'une colonne comme attribut du DataFrame séparée par un point '.'.\n",
    "\n",
    "Exemple : sélection des colonnes name et age\n",
    "    df.name\n",
    "    df.age\n",
    "Cette méthode de sélection de variables, certes plus lourde mais plus riche, offre une deuxième façon de sélectionner les variables :\n",
    "\n",
    "Exemple : utilisation de select pour sélectionner des colonnes\n",
    "\n",
    "    new_df = df.select( df.name, df.age )\n",
    "    \n",
    "Cette structure est intéressante, en particulier grâce à la méthode cast des Columns. Cette méthode permet de spécifier un type particulier d'une colonne. Lors de l'affichage du schéma d'un DataFrame, toutes les variables sont considérées comme de type string, ce qui ne correspond pas toujours au type souhaité en pratique.\n",
    "\n",
    "Exemple : cast pour changer le type\n",
    "\n",
    "    new_df = df.select(df.name.cast(\"string\"),\n",
    "                        df.age.cast(\"int\"))   \n",
    "                        \n",
    "- (c) Créer le DataFrame flights avec les mêmes variables que fligths1 en spécifiant pour chaque colonne le type adapté.\n",
    "- (d) Afficher les 20 premières lignes de flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7971a334-8ff5-4a50-99c1-3603583475ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+---------+------+----+--------+--------+---------------+------------+\n",
      "|annee|mois|jours|flightNum|origin|dest|distance|canceled|cancelationCode|carrierDelay|\n",
      "+-----+----+-----+---------+------+----+--------+--------+---------------+------------+\n",
      "| 2008|   1|    1|      324|   SEA| SJC|     697|   false|            NaN|        null|\n",
      "| 2008|   1|    1|      572|   SEA| PSP|     987|   false|            NaN|        null|\n",
      "| 2008|   1|    1|      511|   SAN| SEA|    1050|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      376|   SEA| GEG|     224|   false|            NaN|        null|\n",
      "| 2008|   1|    1|      729|   TUS| SEA|    1216|   false|            NaN|        null|\n",
      "| 2008|   1|    1|      283|   LAX| SEA|     954|   false|            NaN|        null|\n",
      "| 2008|   1|    1|      211|   LAX| SEA|     954|   false|            NaN|        null|\n",
      "| 2008|   1|    1|      100|   ANC| PDX|    1542|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      665|   LAS| SEA|     866|   false|            NaN|        null|\n",
      "| 2008|   1|    1|      531|   SJC| SEA|     697|   false|            NaN|        null|\n",
      "| 2008|   1|    1|      571|   SEA| DEN|    1024|   false|            NaN|          22|\n",
      "| 2008|   1|    1|      154|   ANC| SEA|    1449|    true|              A|        null|\n",
      "| 2008|   1|    1|      728|   SEA| TUS|    1216|   false|            NaN|        null|\n",
      "| 2008|   1|    1|      518|   SEA| SAN|    1050|   false|            NaN|          92|\n",
      "| 2008|   1|    1|      580|   SEA| SAN|    1050|   false|            NaN|          21|\n",
      "| 2008|   1|    1|       85|   SEA| ANC|    1449|   false|            NaN|        null|\n",
      "| 2008|   1|    1|      640|   SEA| LAS|     866|   false|            NaN|        null|\n",
      "| 2008|   1|    1|      292|   SEA| LAX|     954|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      478|   SEA| PSP|     987|   false|            NaN|          15|\n",
      "| 2008|   1|    1|      485|   LAX| SEA|     954|   false|            NaN|        null|\n",
      "+-----+----+-----+---------+------+----+--------+--------+---------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Création d'un DataFrame en spécifiant le type des colonnes\n",
    "flights = raw_df.select(raw_df.annee.cast(\"int\"),\n",
    "                        raw_df.mois.cast(\"int\"),\n",
    "                        raw_df.jours.cast(\"int\"),\n",
    "                        raw_df.flightNum.cast(\"int\"),\n",
    "                        raw_df.origin.cast(\"string\"),\n",
    "                        raw_df.dest.cast(\"string\"),\n",
    "                        raw_df.distance.cast(\"int\"),\n",
    "                        raw_df.canceled.cast(\"boolean\"),\n",
    "                        raw_df.cancelationCode.cast(\"string\"),\n",
    "                        raw_df.carrierDelay.cast(\"int\"))\n",
    "\n",
    "# Affichage de 20 premières lignes\n",
    "flights.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a676782-20fe-4595-86f9-4cb3ae63c3c8",
   "metadata": {},
   "source": [
    "   Pour la suite de l'exercice, faites attention à bien avoir spécifié les mêmes types que la correction.\n",
    "Comme pour les RDD, il est possible de compter le nombre de lignes grâce à la méthode count. La méthode distinct, appliquée à une seule variable, permet de filtrer tous les doublons.\n",
    "\n",
    "Exemple : Compter le nombre de modalités d'une variable\n",
    "\n",
    "    df.select('age').distinct().count()\n",
    "    \n",
    "- e) Déterminer les numéros de vols distincts dans flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "367f841f-ab83-480e-9a5e-670b57e6cbb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcul du nombre de vols ayant des numéros de vol distincts\n",
    "flights.select('flightNum').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aea70a1-9ca4-48d9-8558-5ca8ebfaf98b",
   "metadata": {},
   "source": [
    "La méthode describe permet d'obtenir un résumé riche en informations pour un DataFrame. En effet, cette méthode génère un DataFrame qui s'affiche avec la méthode show.\n",
    "\n",
    "La méthode show possède une option truncate = n qui tronque les résultats au n-ième caractère. Cela permet notamment d'afficher correctement un tableau contenant trop de décimales.\n",
    "\n",
    "Exemple : Affichage du résumé avec troncature\n",
    "\n",
    "    df.describe().show(truncate = 8)\n",
    "    \n",
    "- (f) Afficher un résumé des données du DataFrame flights.\n",
    "- (g) Tronquer les données de façon à ce que le DataFrame s'affiche correctement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ddc1d5b-6dd3-42f5-ba3a-d35c7e6614c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+--------+---------+------+----+--------+---------------+------------+\n",
      "|summary| annee|    mois|   jours|flightNum|origin|dest|distance|cancelationCode|carrierDelay|\n",
      "+-------+------+--------+--------+---------+------+----+--------+---------------+------------+\n",
      "|  count|   121|     121|     121|      121|   121| 121|     121|            121|          47|\n",
      "|   mean|2008.0|15.87...|6.454...| 347.7...|  null|null|925.8...|            NaN|    20.25...|\n",
      "| stddev|   0.0|15.06...|5.522...| 232.2...|  null|null|551.9...|            NaN|    32.60...|\n",
      "|    min|  2008|       1|       1|       15|   ADQ| ANC|      95|              A|           0|\n",
      "|    max|  2008|      31|      12|      871|   TUS| TUS|    2777|            NaN|         133|\n",
      "+-------+------+--------+--------+---------+------+----+--------+---------------+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>annee</th>\n",
       "      <th>mois</th>\n",
       "      <th>jours</th>\n",
       "      <th>flightNum</th>\n",
       "      <th>origin</th>\n",
       "      <th>dest</th>\n",
       "      <th>distance</th>\n",
       "      <th>cancelationCode</th>\n",
       "      <th>carrierDelay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>15.87603305785124</td>\n",
       "      <td>6.454545454545454</td>\n",
       "      <td>347.7107438016529</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>925.8925619834711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.25531914893617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.061855932528085</td>\n",
       "      <td>5.52268050859363</td>\n",
       "      <td>232.28102369100696</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>551.9630543441123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.60999319034259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>ADQ</td>\n",
       "      <td>ANC</td>\n",
       "      <td>95</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>2008</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>871</td>\n",
       "      <td>TUS</td>\n",
       "      <td>TUS</td>\n",
       "      <td>2777</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary   annee                mois              jours           flightNum  \\\n",
       "0   count     121                 121                121                 121   \n",
       "1    mean  2008.0   15.87603305785124  6.454545454545454   347.7107438016529   \n",
       "2  stddev     0.0  15.061855932528085   5.52268050859363  232.28102369100696   \n",
       "3     min    2008                   1                  1                  15   \n",
       "4     max    2008                  31                 12                 871   \n",
       "\n",
       "  origin  dest           distance cancelationCode       carrierDelay  \n",
       "0    121   121                121             121                 47  \n",
       "1   None  None  925.8925619834711             NaN  20.25531914893617  \n",
       "2   None  None  551.9630543441123             NaN  32.60999319034259  \n",
       "3    ADQ   ANC                 95               A                  0  \n",
       "4    TUS   TUS               2777             NaN                133  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Affichage d'un résumé en utilisant l'option truncate de la méthode show\n",
    "flights.describe().show(truncate = 8)\n",
    "\n",
    "### Deuxième méthode\n",
    "# Affichage d'un résumé en utilisant la méthode toPandas\n",
    "flights.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446d102e-e53c-4d56-a1ae-acc7b4769f21",
   "metadata": {},
   "source": [
    "!!! Pour ne pas avoir à passer du temps sur la troncature de données et se simplifier la tâche, il existe la méthode toPandas qui permet de transformer un DataFrame Spark de petite taille en DataFrame Pandas.  !!!\n",
    "\n",
    "   La méthode toPandas est à utiliser avec des DataFrames de petite taille tels que des résumés d'informations, sinon elle peut affecter la distribution des données.\n",
    "Dans la question précédente, la variable 'canceled' n'apparaît pas dans la description. De même, la description donne peu d'informations sur les variables catégorielles.\n",
    "\n",
    "Pour les variables qualitatives, il est plus intéressant de faire apparaître la fréquence des modalités. La méthode groupBy permet de grouper les données selon une variable puis d'y appliquer une transformation supplémentaire telle que count.\n",
    "\n",
    "- (h) Résumer la variable cancelationCode en affichant le nombre d'observations de chaque modalité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d69ca04d-1028-450c-9408-2d22366bda2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|cancelationCode|count|\n",
      "+---------------+-----+\n",
      "|              A|    2|\n",
      "|            NaN|  119|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Affichage du résumé de la variable catégorielle 'cancellationCode'\n",
    "flights.groupBy('cancelationCode').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0b2104-244a-4f56-bde5-f42cfb904077",
   "metadata": {},
   "source": [
    "- (i) Résumer ensemble les variables 'cancelationCode' et 'canceled' pour vérifier la cohérence entre ces deux variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b45cfb2-a6e8-47fe-abf5-df0454cf7682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+-----+\n",
      "|cancelationCode|canceled|count|\n",
      "+---------------+--------+-----+\n",
      "|              A|    true|    2|\n",
      "|            NaN|   false|  119|\n",
      "+---------------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Affichage du résumé de la variable catégorielle 'cancellationCode' et 'canceled'\n",
    "flights.groupBy('cancelationCode', 'canceled').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ac1041-3f97-48a9-b489-f4e865f8a908",
   "metadata": {},
   "source": [
    "Une autre manière de sélectionner de variables est la méthode filter. Elle permet de filtrer les données par rapport à certaines conditions.\n",
    "\n",
    "Exemple : Filtrage avec condition\n",
    "\n",
    "    df.filter( df.age < 20 )\n",
    "    # renvoie un DataFrame contenant uniquement les individus de moins de 20 ans\n",
    "    \n",
    "- (j) Afficher les premières lignes de flights ne contenant que les vols annulés pour la raison 'C'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4cd290c1-14ed-401d-a7c5-70908d42ed3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+---------+------+----+--------+--------+---------------+------------+\n",
      "|annee|mois|jours|flightNum|origin|dest|distance|canceled|cancelationCode|carrierDelay|\n",
      "+-----+----+-----+---------+------+----+--------+--------+---------------+------------+\n",
      "+-----+----+-----+---------+------+----+--------+--------+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Affichage des 20 premièrs vols annulés pour la raison \"C\"\n",
    "flights.filter(flights.cancelationCode == 'C').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4abe48-8856-4559-bf24-fb361e8d356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" sur vrai dataset\n",
    "+-----+----+-----+---------+------+----+--------+--------+----------------+------------+\n",
    "|annee|mois|jours|flightNum|origin|dest|distance|canceled|cancellationCode|carrierDelay|\n",
    "+-----+----+-----+---------+------+----+--------+--------+----------------+------------+\n",
    "| 2008|   1|    5|      345|   SFO| PDX|     550|    true|               C|        null|\n",
    "| 2008|   1|    8|      345|   SFO| PDX|     550|    true|               C|        null|\n",
    "| 2008|   1|    8|      526|   SEA| SFO|     679|    true|               C|        null|\n",
    "| 2008|   1|   10|      526|   SEA| SFO|     679|    true|               C|        null|\n",
    "| 2008|   1|   10|      345|   SFO| PDX|     550|    true|               C|        null|\n",
    "| 2008|   1|   11|      345|   SFO| PDX|     550|    true|               C|        null|\n",
    "| 2008|   1|   11|      526|   SEA| SFO|     679|    true|               C|        null|\n",
    "| 2008|   1|   22|      526|   SEA| SFO|     679|    true|               C|        null|\n",
    "| 2008|   1|   22|      345|   SFO| PDX|     550|    true|               C|        null|\n",
    "| 2008|   1|   25|      341|   SFO| PDX|     550|    true|               C|        null|\n",
    "| 2008|   1|   25|      378|   SFO| PSP|     421|    true|               C|        null|\n",
    "| 2008|   1|   25|      310|   PDX| SFO|     550|    true|               C|        null|....\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f8c344-bfb3-4bab-9d4b-3c562b223900",
   "metadata": {},
   "source": [
    "Filtrer et grouper les données est très important dans la manipulation des données. Ces étapes permettent de comprendre une base de données et d'en tirer des conclusions :\n",
    "\n",
    "- (k) Quel est le mois où il y a le plus d'annulations de vols ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e96ba00-d84d-42c2-88dc-40dcb2fb21a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|mois|count|\n",
      "+----+-----+\n",
      "|   1|    2|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcul du nombre vols annulés par mois\n",
    "flights.filter(flights.canceled == True).groupBy('mois').count().show()\n",
    "\n",
    "\n",
    "# On remarque que le mois de Décembre compte beaucoup plus d'annulations que les autres mois,\n",
    "# cela peut être lié à une planification anticipée plus grande des vacances de Noël."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d19c62e-7701-425b-bfc2-5f2023de2d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "+----+-----+\n",
    "|mois|count|\n",
    "+----+-----+\n",
    "|  12|  627|\n",
    "|   1|  355|\n",
    "|   6|  104|\n",
    "|   3|   85|\n",
    "|   5|  127|\n",
    "|   9|   67|\n",
    "|   4|  158|\n",
    "|   8|  154|\n",
    "|   7|   98|\n",
    "|  10|   93|\n",
    "|  11|   65|\n",
    "|   2|  206|\n",
    "+----+-----+ \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac9e84-e221-4ec8-9239-56e4c28047be",
   "metadata": {},
   "source": [
    "## 4. Création et aggrégation de variables\n",
    "La méthode withColumn permet de créer une nouvelle colonne :\n",
    "\n",
    "Exemple : Création des colonnes ageInMonth et isMinor\n",
    "\n",
    "df.withColumn( 'ageInMonth', df.age * 12) # Variable entière contenant l'âge d'une personne en mois\n",
    "df.withColumn( 'isMinor', df.age < 18 )   # Variable booléenne indiquant si une personne est mineure ou pas\n",
    "\n",
    "- (a) Créer une nouvelle variable booléenne 'isLongFlight' qui vaut True si le vol parcourt une distance supérieure à 1000 miles, ou False sinon.\n",
    "- (b) Afficher les 10 premières lignes de flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b585d5d-158e-45e1-ad59-7ee54c9a8edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+---------+------+----+--------+--------+---------------+------------+------------+\n",
      "|annee|mois|jours|flightNum|origin|dest|distance|canceled|cancelationCode|carrierDelay|isLongFlight|\n",
      "+-----+----+-----+---------+------+----+--------+--------+---------------+------------+------------+\n",
      "| 2008|   1|    1|      324|   SEA| SJC|     697|   false|            NaN|        null|       false|\n",
      "| 2008|   1|    1|      572|   SEA| PSP|     987|   false|            NaN|        null|       false|\n",
      "| 2008|   1|    1|      511|   SAN| SEA|    1050|   false|            NaN|           0|        true|\n",
      "| 2008|   1|    1|      376|   SEA| GEG|     224|   false|            NaN|        null|       false|\n",
      "| 2008|   1|    1|      729|   TUS| SEA|    1216|   false|            NaN|        null|        true|\n",
      "| 2008|   1|    1|      283|   LAX| SEA|     954|   false|            NaN|        null|       false|\n",
      "| 2008|   1|    1|      211|   LAX| SEA|     954|   false|            NaN|        null|       false|\n",
      "| 2008|   1|    1|      100|   ANC| PDX|    1542|   false|            NaN|           0|        true|\n",
      "| 2008|   1|    1|      665|   LAS| SEA|     866|   false|            NaN|        null|       false|\n",
      "| 2008|   1|    1|      531|   SJC| SEA|     697|   false|            NaN|        null|       false|\n",
      "+-----+----+-----+---------+------+----+--------+--------+---------------+------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Création d'une nouvelle variable 'isLongFlight' et affichage des 10 premières lignes\n",
    "flights.withColumn('isLongFlight', flights.distance > 1000 ).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78646995-a4ea-4518-8342-dea809b13d07",
   "metadata": {},
   "source": [
    "   L'enregistrement de la nouvelle colonne ne s'effectue nulle part. A cause du caractère immuable, aucune modification ne se fait par remplacement (in place). Pour enregistrer une nouvelle variable, il faut créer un nouvel objet ou de la créer dès la création du DataFrame.\n",
    "   \n",
    "## 5. Gestion des valeurs manquantes\n",
    "Les valeurs manquantes apparaissent comme null dans la base de données. Il existe des fonctions telles que dropna ou fillna, comme présenté dans le module Pandas, ayant la syntaxe suivante :\n",
    "\n",
    "    df.fillna(newValue, 'columnName')\n",
    "    \n",
    "Exemple : Remplacement des noms non renseignés par 'unknown' et des âges non renseignés par 23\n",
    "\n",
    "    df.fillna('unknown', 'name') # donne un nom 'unknown' aux inconnus\n",
    "    df.fillna(23, 'age')         # assigne l’âge des personnes dont l’âge est inconnu à 23\n",
    "    \n",
    "- (a) Remplacer les valeurs manquantes de 'carrierDelay' par des 0.\n",
    "- (b) Afficher les 6 premières lignes de flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1d98bb6-d12c-44a0-ac9d-678fd4f2abc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+---------+------+----+--------+--------+---------------+------------+\n",
      "|annee|mois|jours|flightNum|origin|dest|distance|canceled|cancelationCode|carrierDelay|\n",
      "+-----+----+-----+---------+------+----+--------+--------+---------------+------------+\n",
      "| 2008|   1|    1|      324|   SEA| SJC|     697|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      572|   SEA| PSP|     987|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      511|   SAN| SEA|    1050|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      376|   SEA| GEG|     224|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      729|   TUS| SEA|    1216|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      283|   LAX| SEA|     954|   false|            NaN|           0|\n",
      "+-----+----+-----+---------+------+----+--------+--------+---------------+------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remplacement des valeurs manquantes par des 0 et affichage des 6 premières lignes\n",
    "flights = flights.fillna(0, 'carrierDelay')\n",
    "flights.show(6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ae265e-05da-4091-a031-8da54035d1de",
   "metadata": {},
   "source": [
    "Remplacer n'importe quelle valeur est également possible en utilisant la méthode replace avec l'une des syntaxes suivantes :\n",
    "\n",
    "    df.replace(oldValue, newValue)\n",
    "    # remplace sur l'ensemble de la base\n",
    "\n",
    "    df.replace(oldValue, newValue, 'columnName')\n",
    "    # remplace uniquement sur les colonnes spécifiées\n",
    "\n",
    "    df.replace([oldValue1, oldValue2], [newValue1, newValue2], 'columnName')\n",
    "    # si plusieurs valeurs à remplacer\n",
    "    \n",
    "- (c) Remplacer les codes d'annulation 'A','B','C' respectivement par '1', '2' et '3'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7574ecb-a208-418d-a979-a140402a645a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+---------+------+----+--------+--------+---------------+------------+\n",
      "|annee|mois|jours|flightNum|origin|dest|distance|canceled|cancelationCode|carrierDelay|\n",
      "+-----+----+-----+---------+------+----+--------+--------+---------------+------------+\n",
      "| 2008|   1|    1|      324|   SEA| SJC|     697|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      572|   SEA| PSP|     987|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      511|   SAN| SEA|    1050|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      376|   SEA| GEG|     224|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      729|   TUS| SEA|    1216|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      283|   LAX| SEA|     954|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      211|   LAX| SEA|     954|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      100|   ANC| PDX|    1542|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      665|   LAS| SEA|     866|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      531|   SJC| SEA|     697|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      571|   SEA| DEN|    1024|   false|            NaN|          22|\n",
      "| 2008|   1|    1|      154|   ANC| SEA|    1449|    true|              1|           0|\n",
      "| 2008|   1|    1|      728|   SEA| TUS|    1216|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      518|   SEA| SAN|    1050|   false|            NaN|          92|\n",
      "| 2008|   1|    1|      580|   SEA| SAN|    1050|   false|            NaN|          21|\n",
      "| 2008|   1|    1|       85|   SEA| ANC|    1449|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      640|   SEA| LAS|     866|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      292|   SEA| LAX|     954|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      478|   SEA| PSP|     987|   false|            NaN|          15|\n",
      "| 2008|   1|    1|      485|   LAX| SEA|     954|   false|            NaN|           0|\n",
      "+-----+----+-----+---------+------+----+--------+--------+---------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remplacement des codes d'annulation\n",
    "flights = flights.replace(['A','B','C'],['1','2','3'],'cancelationCode')\n",
    "flights.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ada95f-4da1-457d-b462-a091c9f90cb2",
   "metadata": {},
   "source": [
    "La fonction orderBy fonctionne comme pour le langage SQL. Appliquée à un DataFrame, elle permet de l'ordonner selon les valeurs d'une de ses variables.\n",
    "\n",
    "Exemple : Ordonner df par âge croissant puis par âge décroissant\n",
    "\n",
    "    df.orderBy(df.age)\n",
    "    # ordonne par la variable 'age'\n",
    "\n",
    "    df.orderBy(df.age.desc())\n",
    "    # ordonne de façon décroissante\n",
    "    \n",
    "- (d) Afficher les premières lignes de la base de données ordonnées de façon décroissante par le numéro de vol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b1a23a4-e84c-42bb-bdbd-4425b4394100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+---------+------+----+--------+--------+---------------+------------+\n",
      "|annee|mois|jours|flightNum|origin|dest|distance|canceled|cancelationCode|carrierDelay|\n",
      "+-----+----+-----+---------+------+----+--------+--------+---------------+------------+\n",
      "| 2008|  31|   12|      871|   HNL| ANC|    2777|   false|            NaN|           0|\n",
      "| 2008|  31|   12|      854|   LIH| SEA|    2701|   false|            NaN|           0|\n",
      "| 2008|  31|   12|      852|   HNL| SEA|    2677|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      811|   DFW| SEA|    1660|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      729|   TUS| SEA|    1216|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      728|   SEA| TUS|    1216|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      690|   SFO| PSP|     421|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      686|   SEA| LAS|     866|   false|            NaN|           0|\n",
      "| 2008|   1|    1|      665|   LAS| SEA|     866|   false|            NaN|           0|\n",
      "| 2008|  31|   12|      664|   SEA| DFW|    1660|   false|            NaN|           0|\n",
      "| 2008|  31|   12|      652|   PDX| PHX|    1009|   false|            NaN|           4|\n",
      "| 2008|   1|    1|      640|   SEA| LAS|     866|   false|            NaN|           0|\n",
      "| 2008|  31|   12|      637|   PHX| SEA|    1107|   false|            NaN|           6|\n",
      "| 2008|  31|   12|      630|   SEA| PHX|    1107|   false|            NaN|          29|\n",
      "| 2008|  31|   12|      625|   LAS| PDX|     762|   false|            NaN|           0|\n",
      "| 2008|  31|   12|      622|   PDX| LAS|     762|   false|            NaN|          25|\n",
      "| 2008|  31|   12|      610|   SEA| LAS|     866|   false|            NaN|           0|\n",
      "| 2008|  31|   12|      609|   LAS| SEA|     866|   false|            NaN|           0|\n",
      "| 2008|  31|   12|      603|   LAS| SEA|     866|   false|            NaN|          53|\n",
      "| 2008|  31|   12|      589|   SNA| PDX|     859|   false|            NaN|           0|\n",
      "+-----+----+-----+---------+------+----+--------+--------+---------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ordonner le data frame par numéro de vol décroissant\n",
    "flights = flights.orderBy(flights.flightNum.desc())\n",
    "flights.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e76939-d208-4f66-97bc-cc9eeec5016e",
   "metadata": {},
   "source": [
    "## 6. Requêtes SQL \n",
    "Spark SQL permet également d'utiliser le langage SQL. Il est possible de faire fonctionner PySpark à l'aide de la méthode sql.\n",
    "\n",
    "   Utiliser la méthode sql est plus lent que les méthodes ci-dessus et peut entraîner un temps de calcul significativement plus long sur des bases de données massives. La documentation de pyspark-sql contient tous les concepts SQL non évoqués jusqu'ici.\n",
    "La première étape consiste à créer une vue SQL (SQL view), référencée dans le code SQL grâce à la méthode createOrReplaceTempView.\n",
    "\n",
    "Exemple : Création d'une vue et utilisation de la méthode SQL pour envoyer une requête\n",
    "\n",
    "    df.createOrReplaceTempView(\"people\")\n",
    "    sqlDF = spark.sql(\"SELECT * FROM people\")  \n",
    "    \n",
    "- (a) Créer une vue SQL de flights que l'on appellera \"flightsView\".\n",
    "- (b) Créer un DataFrame appelé sqlDF contenant uniquement la variable carrierDelay grâce à une requête SQL.\n",
    "- (c) Afficher les premières lignes de sqlDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "593bd676-597a-42f0-86ed-e8094fa3bcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|carrierDelay|\n",
      "+------------+\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Création d'une vue SQL\n",
    "flights.createOrReplaceTempView(\"flightsView\")\n",
    "\n",
    "# Création d'un DataFrame ne contenant que la variable \"flightsView\"\n",
    "sqlDF = spark.sql(\"SELECT carrierDelay FROM flightsView\")\n",
    "\n",
    "# Affichage des 10 premières lignes\n",
    "sqlDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a721d8e-ab07-4956-9cf2-0a503590e6cb",
   "metadata": {},
   "source": [
    "## 7. Sample & astuces d'affichage\n",
    "\n",
    "L'inconvénient de la méthode show est qu'elle a un mauvais rendu lorsqu'une base de données contient un grand nombre de variables. Il est possible d'utiliser la méthode toPandas pour corriger ce problème. Cette méthode ne fonctionne que sur une base de données de petite taille. Pour cela, la méthode sample renvoie un extrait des données en prenant essentiellement 3 arguments :\n",
    "\n",
    "- withReplacement : un booléen qui vaut False si l'on veut un tirage sans remise et True si l'on veut un tirage avec.\n",
    "- fraction : la fraction des données à conserver.\n",
    "- seed : un entier quelconque qui permet de reproduire les résultats: pour un même seed, une fonction, bien qu'aléatoire, donnera toujours les mêmes résultats.   \n",
    "\n",
    "Exemple : Un extrait du DataFrame contenant 1% des données aléatoirement choisies\n",
    "\n",
    "df.sample(False, .01, seed = 1234)\n",
    "\n",
    "- (a) Afficher, de façon élégante, une dizaine de lignes de la base de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "861b9a81-4263-4662-932d-d9a03988df6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annee</th>\n",
       "      <th>mois</th>\n",
       "      <th>jours</th>\n",
       "      <th>flightNum</th>\n",
       "      <th>origin</th>\n",
       "      <th>dest</th>\n",
       "      <th>distance</th>\n",
       "      <th>canceled</th>\n",
       "      <th>cancelationCode</th>\n",
       "      <th>carrierDelay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>516</td>\n",
       "      <td>SEA</td>\n",
       "      <td>SNA</td>\n",
       "      <td>978</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>JNU</td>\n",
       "      <td>KTN</td>\n",
       "      <td>234</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   annee  mois  jours  flightNum origin dest  distance  canceled  \\\n",
       "0   2008    31     12        516    SEA  SNA       978     False   \n",
       "1   2008     1      1         60    JNU  KTN       234     False   \n",
       "\n",
       "  cancelationCode  carrierDelay  \n",
       "0             NaN             0  \n",
       "1             NaN             0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Affichage d'un dizaine de lignes de la base de données\n",
    "flights.sample(False, .01, seed = 222).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecd290c-1dd1-411f-9d3a-6f0d3fff1348",
   "metadata": {},
   "source": [
    "   La méthode sample n'est pas simplement utile pour l'affichage, elle est également importante pour tester différentes méthodes lorsque la puissance de calcul disponible est trop faible.   \n",
    "   \n",
    "- (b) Fermer la session spark en utilisant la méthode stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9fcb0d49-fbba-490c-8dcd-3609b1958a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fermeture de la session Spark\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acd0b00-98a8-40d1-9c20-19fc4315ca92",
   "metadata": {},
   "source": [
    "## conclusion \n",
    "\n",
    "Vous avez maintenant toutes les clés nécessaires en main pour effectuer de l'exploration de données avec une performance optimale grâce à PySpark. Vous êtes donc prêt à aborder Spark ML, le module permettant de programmer des algorithmes de Machine Learning utilisant la structure DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b767a4-a76f-4985-a9b6-69b626cf3824",
   "metadata": {},
   "source": [
    "# Régression avec PySpark \n",
    "\n",
    "## Introduction\n",
    "Spark ML est un module très récent, développé en parallèle par Databricks et UC Berkeley AMPLab et lancé fin 2015. Spark ML permet d'exécuter la majorité des algorithmes de Machine Learning de façon distribuée pour un très grand gain en performance.\n",
    "\n",
    "Dans cet exercice, nous étudierons le cas d'une régression simple de façon à comprendre comment préparer les données et faire face à un problème de Machine Learning grâce à Spark ML. Des algorithmes plus poussés font l'objectif de l'exercice suivant.\n",
    "\n",
    "\n",
    "\n",
    "- (a) Exécuter la cellule ci-dessous pour construire une SparkSession pour notre exercice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ad908c3b-ca91-4abe-ad3c-5d019445facb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://HappyNous:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x17154206160>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import de SparkSession et SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Définition d'un SparkContext en local\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Construction d'une session Spark\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Introduction à Spark ML\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393bc1cb-1b09-4a0d-848c-9e47556126ba",
   "metadata": {},
   "source": [
    "## 1. Importation de la base de données\n",
    "Dans cet exercice, la base de données utilisée est Year Prediction MSD. Elle contient des caractéristiques audio de 515345 chansons parues entre 1922 et 2011. Ces chansons sont essentiellement des tubes commerciaux occidentaux.\n",
    "Cette base de données contient 91 variables :\n",
    "\n",
    "    - Une variable contenant l'année de la chanson.\n",
    "    - 12 variables contenant une projection à 12 dimensions du timbre audio de la chanson.\n",
    "    - 78 variables contenant des informations de covariance du timbre audio.  \n",
    "    \n",
    "L'objectif est d'estimer l'année de sortie d'une chanson en fonction de ses caractéristiques audio. Pour cela nous allons implémenter une régression linéaire simple sur les informations du timbre pour prédire l'année de sortie.\n",
    "\n",
    "- (a) Charger le fichier YearPredictionMSD.txt dans un DataFrame nommé df_raw.\n",
    "- (b) Afficher un extrait de la base de données avec une méthode de votre choix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4f8f664-effc-4a0b-ac3e-fb231e8dfe0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "| _c0| _c1| _c2| _c3| _c4| _c5| _c6| _c7| _c8| _c9|_c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20|_c21|_c22|_c23|_c24|_c25|_c26|_c27|_c28|_c29|_c30|_c31|_c32|_c33|_c34|_c35|_c36|_c37|_c38|_c39|_c40|_c41|_c42|_c43|_c44|_c45|_c46|_c47|_c48|_c49|_c50|_c51|_c52|_c53|_c54|_c55|_c56|_c57|_c58|_c59|_c60|_c61|_c62|_c63|_c64|_c65|_c66|_c67|_c68|_c69|_c70|_c71|_c72|_c73|_c74|_c75|_c76|_c77|_c78|_c79|_c80|_c81|_c82|_c83|_c84|_c85|_c86|_c87|_c88|_c89|_c90|\n",
      "+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|2001|4...|2...|7...|8...|-...|-...|-...|-...|7...|-...|3...|-...|1...|6...|9...|6...|4...|3...|3...|2...|2...|1...|1...|1...|-...|-...|9...|4...|-...|-...|1...|3...|1...|1...|7...|-...|-...|-...|-...|7...|-...|-...|-...|-...|1...|-...|8...|2...|-...|3...|-...|-...|1...|4...|-...|-...|1...|6...|2...|-...|-...|7...|-...|-...|-...|-...|4...|7...|2...|6...|-...|-...|-...|7...|-...|-...|-...|1...|-...|4...|1...|-...|5...|1...|1...|-...|6...|-...|-...|2...|\n",
      "|2001|4...|1...|7...|1...|-...|-...|8...|-...|1...|4...|2...|0...|4...|2...|6...|4...|7...|4...|7...|3...|3...|2...|1...|3...|1...|-...|1...|1...|-...|-...|1...|4...|3...|-...|4...|2...|-...|-...|-...|2...|-...|1...|-...|-...|-...|-...|-...|1...|1...|4...|2...|1...|7...|-...|-...|1...|7...|-...|-...|-...|-...|-...|1...|-...|4...|1...|-...|4...|4...|-...|4...|1...|1...|-...|1...|-...|-...|1...|-...|-...|5...|-...|3...|4...|-...|-...|7...|1...|5...|2...|\n",
      "+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>_c1</th>\n",
       "      <th>_c2</th>\n",
       "      <th>_c3</th>\n",
       "      <th>_c4</th>\n",
       "      <th>_c5</th>\n",
       "      <th>_c6</th>\n",
       "      <th>_c7</th>\n",
       "      <th>_c8</th>\n",
       "      <th>_c9</th>\n",
       "      <th>...</th>\n",
       "      <th>_c81</th>\n",
       "      <th>_c82</th>\n",
       "      <th>_c83</th>\n",
       "      <th>_c84</th>\n",
       "      <th>_c85</th>\n",
       "      <th>_c86</th>\n",
       "      <th>_c87</th>\n",
       "      <th>_c88</th>\n",
       "      <th>_c89</th>\n",
       "      <th>_c90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009</td>\n",
       "      <td>36.80862</td>\n",
       "      <td>-78.42632</td>\n",
       "      <td>37.84541</td>\n",
       "      <td>-15.35611</td>\n",
       "      <td>12.32949</td>\n",
       "      <td>-19.87961</td>\n",
       "      <td>-17.85985</td>\n",
       "      <td>-2.45005</td>\n",
       "      <td>10.38729</td>\n",
       "      <td>...</td>\n",
       "      <td>18.45879</td>\n",
       "      <td>-52.67297</td>\n",
       "      <td>56.11795</td>\n",
       "      <td>95.66717</td>\n",
       "      <td>5.94237</td>\n",
       "      <td>152.50178</td>\n",
       "      <td>218.82833</td>\n",
       "      <td>3.52899</td>\n",
       "      <td>106.35575</td>\n",
       "      <td>20.23352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>41.54830</td>\n",
       "      <td>-22.11942</td>\n",
       "      <td>14.11619</td>\n",
       "      <td>32.45852</td>\n",
       "      <td>31.27106</td>\n",
       "      <td>4.88209</td>\n",
       "      <td>-3.91581</td>\n",
       "      <td>7.37319</td>\n",
       "      <td>-7.52489</td>\n",
       "      <td>...</td>\n",
       "      <td>82.00966</td>\n",
       "      <td>-71.06712</td>\n",
       "      <td>-139.20629</td>\n",
       "      <td>-223.15997</td>\n",
       "      <td>-10.66484</td>\n",
       "      <td>95.41865</td>\n",
       "      <td>-53.16736</td>\n",
       "      <td>-19.79265</td>\n",
       "      <td>-7.46794</td>\n",
       "      <td>-0.00180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007</td>\n",
       "      <td>30.07664</td>\n",
       "      <td>-104.02965</td>\n",
       "      <td>8.02798</td>\n",
       "      <td>6.88924</td>\n",
       "      <td>3.96845</td>\n",
       "      <td>-2.15339</td>\n",
       "      <td>17.18638</td>\n",
       "      <td>7.76519</td>\n",
       "      <td>-7.09728</td>\n",
       "      <td>...</td>\n",
       "      <td>44.34641</td>\n",
       "      <td>-438.58100</td>\n",
       "      <td>-57.57235</td>\n",
       "      <td>-19.82384</td>\n",
       "      <td>-21.04993</td>\n",
       "      <td>55.38797</td>\n",
       "      <td>205.65896</td>\n",
       "      <td>2.71854</td>\n",
       "      <td>-18.65339</td>\n",
       "      <td>24.52659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970</td>\n",
       "      <td>46.15827</td>\n",
       "      <td>37.86410</td>\n",
       "      <td>0.81071</td>\n",
       "      <td>-8.17771</td>\n",
       "      <td>-12.99806</td>\n",
       "      <td>-9.92252</td>\n",
       "      <td>5.09238</td>\n",
       "      <td>0.74653</td>\n",
       "      <td>8.73862</td>\n",
       "      <td>...</td>\n",
       "      <td>4.99854</td>\n",
       "      <td>54.87302</td>\n",
       "      <td>-12.11495</td>\n",
       "      <td>34.49671</td>\n",
       "      <td>6.71429</td>\n",
       "      <td>8.41753</td>\n",
       "      <td>22.13314</td>\n",
       "      <td>-1.23688</td>\n",
       "      <td>-55.94937</td>\n",
       "      <td>14.30917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008</td>\n",
       "      <td>46.93720</td>\n",
       "      <td>10.29771</td>\n",
       "      <td>34.93550</td>\n",
       "      <td>0.84467</td>\n",
       "      <td>6.94009</td>\n",
       "      <td>-3.33567</td>\n",
       "      <td>-7.78061</td>\n",
       "      <td>-8.61743</td>\n",
       "      <td>9.32840</td>\n",
       "      <td>...</td>\n",
       "      <td>34.02145</td>\n",
       "      <td>-82.39453</td>\n",
       "      <td>56.79210</td>\n",
       "      <td>39.72003</td>\n",
       "      <td>22.32642</td>\n",
       "      <td>-42.65458</td>\n",
       "      <td>41.34373</td>\n",
       "      <td>-12.45617</td>\n",
       "      <td>152.70814</td>\n",
       "      <td>-13.05494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    _c0       _c1         _c2       _c3        _c4        _c5        _c6  \\\n",
       "0  2009  36.80862   -78.42632  37.84541  -15.35611   12.32949  -19.87961   \n",
       "1  2001  41.54830   -22.11942  14.11619   32.45852   31.27106    4.88209   \n",
       "2  2007  30.07664  -104.02965   8.02798    6.88924    3.96845   -2.15339   \n",
       "3  1970  46.15827    37.86410   0.81071   -8.17771  -12.99806   -9.92252   \n",
       "4  2008  46.93720    10.29771  34.93550    0.84467    6.94009   -3.33567   \n",
       "\n",
       "         _c7       _c8       _c9  ...      _c81        _c82        _c83  \\\n",
       "0  -17.85985  -2.45005  10.38729  ...  18.45879   -52.67297    56.11795   \n",
       "1   -3.91581   7.37319  -7.52489  ...  82.00966   -71.06712  -139.20629   \n",
       "2   17.18638   7.76519  -7.09728  ...  44.34641  -438.58100   -57.57235   \n",
       "3    5.09238   0.74653   8.73862  ...   4.99854    54.87302   -12.11495   \n",
       "4   -7.78061  -8.61743   9.32840  ...  34.02145   -82.39453    56.79210   \n",
       "\n",
       "         _c84       _c85       _c86       _c87       _c88       _c89  \\\n",
       "0    95.66717    5.94237  152.50178  218.82833    3.52899  106.35575   \n",
       "1  -223.15997  -10.66484   95.41865  -53.16736  -19.79265   -7.46794   \n",
       "2   -19.82384  -21.04993   55.38797  205.65896    2.71854  -18.65339   \n",
       "3    34.49671    6.71429    8.41753   22.13314   -1.23688  -55.94937   \n",
       "4    39.72003   22.32642  -42.65458   41.34373  -12.45617  152.70814   \n",
       "\n",
       "        _c90  \n",
       "0   20.23352  \n",
       "1   -0.00180  \n",
       "2   24.52659  \n",
       "3   14.30917  \n",
       "4  -13.05494  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chargement du fichier \" YearPredictionMSD.txt\" dans un DataFrame\n",
    "df_raw = spark.read.csv('YearPredictionMSD.txt')\n",
    "\n",
    "# Première méthode d'affichage \n",
    "df_raw.show(2, truncate = 4)\n",
    "# Modifier les valeurs de 'truncate' ne permet pas de bien visualiser les données\n",
    "# à cause du nombre de variables\n",
    "\n",
    "# Deuxième méthode d'affichage\n",
    "df_raw.sample(False, .00001, seed = 222).toPandas()\n",
    "# Utiliser toPandas permet de mieux visualiser les données\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aebb360-f540-4905-82bc-f0bbf0c74fab",
   "metadata": {},
   "source": [
    "    - Afficher une base données avec la méthode show() est plus rapide. Cependant, cet affichage peut parfois être incompréhensible lorsqu'il y a trop de variables.\n",
    "    - On peut alors sélectionner quelques variables et tronquer l'affichage pour le rendre propre ou alors privilégier la succession des méthodes sample() et toPandas() en faisant attention à choisir un nombre raisonnable de lignes à afficher. Il faut bien garder en tête que même si la méthode sample() est relativement rapide, elle passe néanmoins par un décompte du nombre de ligne, une opération simple mais qui n'est pas rapide en Spark.   \n",
    "    \n",
    "L'exercice précédent montre que le parsing effectué par PySpark a tendance à enregistrer toutes les variables en string même lorsqu'elles sont numériques. La vérification de cette information peut s'effectuer en utilisant la méthode printSchema() :\n",
    "\n",
    "    df_raw.printSchema()\n",
    "\n",
    "Pour modifier le type de chacune des variables, il faudrait changer son type comme suit :\n",
    "\n",
    "    df_raw.select(df_raw._c0.cast(\"double\"),\n",
    "                  df_raw._c1.cast(\"double\"),\n",
    "                  df_raw._c2.cast(\"double\"),\n",
    "                  df_raw._c3.cast(\"double\"),\n",
    "                  ...)\n",
    "Une telle tâche devient de plus en plus fastidieuse quand le nombre de variables devient important. Cette démarche peut être automatisée grâce à la fonction col issue du sous-module pyspark.sql.functions. La fonction col permet de nommer directement une colonne et d'automatiser ce type de démarche au sein d'une boucle. Les deux lignes suivantes permettent alors de changer toutes les colonnes en double dans un nouveau DataFrame df :\n",
    "\n",
    "exprs = [col(c).cast(\"double\") for c in df_raw.columns]\n",
    "df = df_raw.select(*exprs)   \n",
    "\n",
    "- (c) Importer la fonction col du sous-module pyspark.sql.functions.\n",
    "- (d) Créer un DataFrame df à partir de df_raw en changeant les types des colonnes relatives au timbre en double et l'année en int.\n",
    "- (e) Afficher le schéma des variables du df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e995e95f-ee8a-4940-b76d-1ea3e451204e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: double (nullable = true)\n",
      " |-- _c2: double (nullable = true)\n",
      " |-- _c3: double (nullable = true)\n",
      " |-- _c4: double (nullable = true)\n",
      " |-- _c5: double (nullable = true)\n",
      " |-- _c6: double (nullable = true)\n",
      " |-- _c7: double (nullable = true)\n",
      " |-- _c8: double (nullable = true)\n",
      " |-- _c9: double (nullable = true)\n",
      " |-- _c10: double (nullable = true)\n",
      " |-- _c11: double (nullable = true)\n",
      " |-- _c12: double (nullable = true)\n",
      " |-- _c13: double (nullable = true)\n",
      " |-- _c14: double (nullable = true)\n",
      " |-- _c15: double (nullable = true)\n",
      " |-- _c16: double (nullable = true)\n",
      " |-- _c17: double (nullable = true)\n",
      " |-- _c18: double (nullable = true)\n",
      " |-- _c19: double (nullable = true)\n",
      " |-- _c20: double (nullable = true)\n",
      " |-- _c21: double (nullable = true)\n",
      " |-- _c22: double (nullable = true)\n",
      " |-- _c23: double (nullable = true)\n",
      " |-- _c24: double (nullable = true)\n",
      " |-- _c25: double (nullable = true)\n",
      " |-- _c26: double (nullable = true)\n",
      " |-- _c27: double (nullable = true)\n",
      " |-- _c28: double (nullable = true)\n",
      " |-- _c29: double (nullable = true)\n",
      " |-- _c30: double (nullable = true)\n",
      " |-- _c31: double (nullable = true)\n",
      " |-- _c32: double (nullable = true)\n",
      " |-- _c33: double (nullable = true)\n",
      " |-- _c34: double (nullable = true)\n",
      " |-- _c35: double (nullable = true)\n",
      " |-- _c36: double (nullable = true)\n",
      " |-- _c37: double (nullable = true)\n",
      " |-- _c38: double (nullable = true)\n",
      " |-- _c39: double (nullable = true)\n",
      " |-- _c40: double (nullable = true)\n",
      " |-- _c41: double (nullable = true)\n",
      " |-- _c42: double (nullable = true)\n",
      " |-- _c43: double (nullable = true)\n",
      " |-- _c44: double (nullable = true)\n",
      " |-- _c45: double (nullable = true)\n",
      " |-- _c46: double (nullable = true)\n",
      " |-- _c47: double (nullable = true)\n",
      " |-- _c48: double (nullable = true)\n",
      " |-- _c49: double (nullable = true)\n",
      " |-- _c50: double (nullable = true)\n",
      " |-- _c51: double (nullable = true)\n",
      " |-- _c52: double (nullable = true)\n",
      " |-- _c53: double (nullable = true)\n",
      " |-- _c54: double (nullable = true)\n",
      " |-- _c55: double (nullable = true)\n",
      " |-- _c56: double (nullable = true)\n",
      " |-- _c57: double (nullable = true)\n",
      " |-- _c58: double (nullable = true)\n",
      " |-- _c59: double (nullable = true)\n",
      " |-- _c60: double (nullable = true)\n",
      " |-- _c61: double (nullable = true)\n",
      " |-- _c62: double (nullable = true)\n",
      " |-- _c63: double (nullable = true)\n",
      " |-- _c64: double (nullable = true)\n",
      " |-- _c65: double (nullable = true)\n",
      " |-- _c66: double (nullable = true)\n",
      " |-- _c67: double (nullable = true)\n",
      " |-- _c68: double (nullable = true)\n",
      " |-- _c69: double (nullable = true)\n",
      " |-- _c70: double (nullable = true)\n",
      " |-- _c71: double (nullable = true)\n",
      " |-- _c72: double (nullable = true)\n",
      " |-- _c73: double (nullable = true)\n",
      " |-- _c74: double (nullable = true)\n",
      " |-- _c75: double (nullable = true)\n",
      " |-- _c76: double (nullable = true)\n",
      " |-- _c77: double (nullable = true)\n",
      " |-- _c78: double (nullable = true)\n",
      " |-- _c79: double (nullable = true)\n",
      " |-- _c80: double (nullable = true)\n",
      " |-- _c81: double (nullable = true)\n",
      " |-- _c82: double (nullable = true)\n",
      " |-- _c83: double (nullable = true)\n",
      " |-- _c84: double (nullable = true)\n",
      " |-- _c85: double (nullable = true)\n",
      " |-- _c86: double (nullable = true)\n",
      " |-- _c87: double (nullable = true)\n",
      " |-- _c88: double (nullable = true)\n",
      " |-- _c89: double (nullable = true)\n",
      " |-- _c90: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importation de col du sous-module pyspark.sql.functions\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convertir des colonnes relatives au timbre en double et l'année en int\n",
    "exprs = [col(c).cast(\"double\") for c in df_raw.columns[1:91]]\n",
    "df = df_raw.select(df_raw._c0.cast('int'), *exprs)\n",
    "\n",
    "# Affichage du schéma des variables \"df\"\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cac5e07-6719-4321-8c96-b795abd38e4f",
   "metadata": {},
   "source": [
    "    - Inférer le bon type des variables peut paraître superflu et certains algorithmes fonctionnent même lorsque les variables numériques sont de type string. Cependant, il s'agit d'une mesure de sécurité importante car cela peut entraîner beaucoup de bugs potentiels.  \n",
    "    \n",
    "    - Une deuxième mesure de sécurité à prendre en compte est de supprimer ou remplacer les valeurs manquantes. La base de données est ici dépourvue de valeurs manquantes.  \n",
    "    \n",
    "- (g) Afficher un résumé descriptif de la base de données df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b3ac35f2-c2dc-43c9-a2a9-b3ccb250c1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>_c0</th>\n",
       "      <th>_c1</th>\n",
       "      <th>_c2</th>\n",
       "      <th>_c3</th>\n",
       "      <th>_c4</th>\n",
       "      <th>_c5</th>\n",
       "      <th>_c6</th>\n",
       "      <th>_c7</th>\n",
       "      <th>_c8</th>\n",
       "      <th>...</th>\n",
       "      <th>_c81</th>\n",
       "      <th>_c82</th>\n",
       "      <th>_c83</th>\n",
       "      <th>_c84</th>\n",
       "      <th>_c85</th>\n",
       "      <th>_c86</th>\n",
       "      <th>_c87</th>\n",
       "      <th>_c88</th>\n",
       "      <th>_c89</th>\n",
       "      <th>_c90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>515345</td>\n",
       "      <td>515345</td>\n",
       "      <td>515345</td>\n",
       "      <td>515345</td>\n",
       "      <td>515345</td>\n",
       "      <td>515345</td>\n",
       "      <td>515345</td>\n",
       "      <td>515345</td>\n",
       "      <td>515345</td>\n",
       "      <td>...</td>\n",
       "      <td>515345</td>\n",
       "      <td>515345</td>\n",
       "      <td>515345</td>\n",
       "      <td>515345</td>\n",
       "      <td>515345</td>\n",
       "      <td>515345</td>\n",
       "      <td>515345</td>\n",
       "      <td>515345</td>\n",
       "      <td>515345</td>\n",
       "      <td>515345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>1998.3970815667174</td>\n",
       "      <td>43.38712562576559</td>\n",
       "      <td>1.2895541971106745</td>\n",
       "      <td>8.658347088513548</td>\n",
       "      <td>1.164124465959703</td>\n",
       "      <td>-6.55360070455717</td>\n",
       "      <td>-9.521975199836968</td>\n",
       "      <td>-2.3910894249095267</td>\n",
       "      <td>-1.7932355097264918</td>\n",
       "      <td>...</td>\n",
       "      <td>15.755406044028739</td>\n",
       "      <td>-73.46149977267655</td>\n",
       "      <td>41.54242155146537</td>\n",
       "      <td>37.93411873529419</td>\n",
       "      <td>0.3157512716723748</td>\n",
       "      <td>17.669213222656623</td>\n",
       "      <td>-26.315335961986488</td>\n",
       "      <td>4.458641107180611</td>\n",
       "      <td>20.035136407688167</td>\n",
       "      <td>1.329105437794092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>10.931046354331743</td>\n",
       "      <td>6.067558307507002</td>\n",
       "      <td>51.580350830148205</td>\n",
       "      <td>35.26858489649706</td>\n",
       "      <td>16.32278987099367</td>\n",
       "      <td>22.86078541054064</td>\n",
       "      <td>12.857751456762786</td>\n",
       "      <td>14.571873168680446</td>\n",
       "      <td>7.9638274827628495</td>\n",
       "      <td>...</td>\n",
       "      <td>32.09963498838547</td>\n",
       "      <td>175.6188893766889</td>\n",
       "      <td>122.22879912808428</td>\n",
       "      <td>95.0506305580601</td>\n",
       "      <td>16.161764077993197</td>\n",
       "      <td>114.42790475450803</td>\n",
       "      <td>173.9773360443016</td>\n",
       "      <td>13.346556689429255</td>\n",
       "      <td>185.55824667696402</td>\n",
       "      <td>22.088576378854253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>1922</td>\n",
       "      <td>1.749</td>\n",
       "      <td>-337.0925</td>\n",
       "      <td>-301.00506</td>\n",
       "      <td>-154.18358</td>\n",
       "      <td>-181.95337</td>\n",
       "      <td>-81.79429</td>\n",
       "      <td>-188.214</td>\n",
       "      <td>-72.50385</td>\n",
       "      <td>...</td>\n",
       "      <td>-437.72203</td>\n",
       "      <td>-4402.37644</td>\n",
       "      <td>-1810.68919</td>\n",
       "      <td>-3098.35031</td>\n",
       "      <td>-341.78912</td>\n",
       "      <td>-3168.92457</td>\n",
       "      <td>-4319.99232</td>\n",
       "      <td>-236.03926</td>\n",
       "      <td>-7458.37815</td>\n",
       "      <td>-381.42443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>2011</td>\n",
       "      <td>61.97014</td>\n",
       "      <td>384.06573</td>\n",
       "      <td>322.85143</td>\n",
       "      <td>335.77182</td>\n",
       "      <td>262.06887</td>\n",
       "      <td>166.23689</td>\n",
       "      <td>172.40268</td>\n",
       "      <td>126.74127</td>\n",
       "      <td>...</td>\n",
       "      <td>840.97338</td>\n",
       "      <td>4469.45487</td>\n",
       "      <td>3210.7017</td>\n",
       "      <td>1734.07969</td>\n",
       "      <td>260.5449</td>\n",
       "      <td>3662.06565</td>\n",
       "      <td>2833.60895</td>\n",
       "      <td>463.4195</td>\n",
       "      <td>7393.39844</td>\n",
       "      <td>677.89963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                 _c0                _c1                 _c2  \\\n",
       "0   count              515345             515345              515345   \n",
       "1    mean  1998.3970815667174  43.38712562576559  1.2895541971106745   \n",
       "2  stddev  10.931046354331743  6.067558307507002  51.580350830148205   \n",
       "3     min                1922              1.749           -337.0925   \n",
       "4     max                2011           61.97014           384.06573   \n",
       "\n",
       "                 _c3                _c4                _c5  \\\n",
       "0             515345             515345             515345   \n",
       "1  8.658347088513548  1.164124465959703  -6.55360070455717   \n",
       "2  35.26858489649706  16.32278987099367  22.86078541054064   \n",
       "3         -301.00506         -154.18358         -181.95337   \n",
       "4          322.85143          335.77182          262.06887   \n",
       "\n",
       "                  _c6                  _c7                  _c8  ...  \\\n",
       "0              515345               515345               515345  ...   \n",
       "1  -9.521975199836968  -2.3910894249095267  -1.7932355097264918  ...   \n",
       "2  12.857751456762786   14.571873168680446   7.9638274827628495  ...   \n",
       "3           -81.79429             -188.214            -72.50385  ...   \n",
       "4           166.23689            172.40268            126.74127  ...   \n",
       "\n",
       "                 _c81                _c82                _c83  \\\n",
       "0              515345              515345              515345   \n",
       "1  15.755406044028739  -73.46149977267655   41.54242155146537   \n",
       "2   32.09963498838547   175.6188893766889  122.22879912808428   \n",
       "3          -437.72203         -4402.37644         -1810.68919   \n",
       "4           840.97338          4469.45487           3210.7017   \n",
       "\n",
       "                _c84                _c85                _c86  \\\n",
       "0             515345              515345              515345   \n",
       "1  37.93411873529419  0.3157512716723748  17.669213222656623   \n",
       "2   95.0506305580601  16.161764077993197  114.42790475450803   \n",
       "3        -3098.35031          -341.78912         -3168.92457   \n",
       "4         1734.07969            260.5449          3662.06565   \n",
       "\n",
       "                  _c87                _c88                _c89  \\\n",
       "0               515345              515345              515345   \n",
       "1  -26.315335961986488   4.458641107180611  20.035136407688167   \n",
       "2    173.9773360443016  13.346556689429255  185.55824667696402   \n",
       "3          -4319.99232          -236.03926         -7458.37815   \n",
       "4           2833.60895            463.4195          7393.39844   \n",
       "\n",
       "                 _c90  \n",
       "0              515345  \n",
       "1   1.329105437794092  \n",
       "2  22.088576378854253  \n",
       "3          -381.42443  \n",
       "4           677.89963  \n",
       "\n",
       "[5 rows x 92 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Affichage d'un résumé descriptif des données\n",
    "df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16130d20-2b1c-4656-bbaa-f8481c02025d",
   "metadata": {},
   "source": [
    " Une bonne pratique est de mettre la variable à prédire en première position. \n",
    "    \n",
    "## 2. Mise en forme de la base en format svmlib\n",
    "Pour pouvoir être utilisée par les algorithmes de Machine Learning de Spark ML, la base de données doit être un DataFrame contenant 2 colonnes :\n",
    "\n",
    "La colonne label contenant la variable à prédire (label en anglais).\n",
    "La colonne features contenant les variables explicatives (features en anglais).\n",
    "La fonction DenseVector() issue du package pyspark.ml.linalg permet de regrouper plusieurs variables en une seule variable.\n",
    "\n",
    "   Pour pouvoir utiliser la fonction DenseVector(), il faut utiliser la méthode map après avoir transformer le DataFrame en rdd.\n",
    "Exemple :\n",
    "\n",
    "rdd_ml = df.rdd.map(lambda x: (x[0], DenseVector(x[1:]))) # en supposant que la variable à expliquer est en première position\n",
    "Exemple :\n",
    "\n",
    "df_ml = spark.createDataFrame(rdd_ml, ['label', 'features'])   \n",
    "\n",
    "- (a) Importer la fonction DenseVector du package pyspark.ml.linalg.\n",
    "- (b) Créer un rdd rdd_ml séparant la variable à expliquer des features (à mettre sous forme DenseVector).\n",
    "- (c) Créer un DataFrame df_ml contenant notre base de données sous deux variables : 'label' et 'features'.\n",
    "- (d) Afficher un extrait de df_ml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c07ee01e-150a-44ce-a65e-5e858297b564",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 11) (HappyNous executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, Le fichier spécifié est introuvable\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:167)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: CreateProcess error=2, Le fichier spécifié est introuvable\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 16 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, Le fichier spécifié est introuvable\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:167)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, Le fichier spécifié est introuvable\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 16 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m rdd_ml \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: (x[\u001b[38;5;241m0\u001b[39m], DenseVector(x[\u001b[38;5;241m1\u001b[39m:])))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Création d'un DataFrame composé de deux variables : label et features\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m df_ml \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd_ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Affichage des 10 premières lignes du DataFrame\u001b[39;00m\n\u001b[0;32m     11\u001b[0m df_ml\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py:894\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[0;32m    893\u001b[0m     )\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    896\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py:934\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, RDD):\n\u001b[1;32m--> 934\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromRDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    936\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py:600\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[1;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 600\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    601\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[0;32m    602\u001b[0m     tupled_rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(converter)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py:546\u001b[0m, in \u001b[0;36mSparkSession._inferSchema\u001b[1;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inferSchema\u001b[39m(\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    527\u001b[0m     rdd: RDD[Any],\n\u001b[0;32m    528\u001b[0m     samplingRatio: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    529\u001b[0m     names: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    530\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m StructType:\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;124;03m    Infer schema from an RDD of Row, dict, or tuple.\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03m    :class:`pyspark.sql.types.StructType`\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 546\u001b[0m     first \u001b[38;5;241m=\u001b[39m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m first:\n\u001b[0;32m    548\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe first row in RDD is empty, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan not infer schema\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py:1903\u001b[0m, in \u001b[0;36mRDD.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   1891\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1892\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[0;32m   1893\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1901\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[0;32m   1902\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1903\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1904\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[0;32m   1905\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py:1883\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1880\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1882\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[1;32m-> 1883\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1885\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m   1886\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py:1486\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   1484\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1486\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 11) (HappyNous executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, Le fichier spécifié est introuvable\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:167)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: CreateProcess error=2, Le fichier spécifié est introuvable\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 16 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, Le fichier spécifié est introuvable\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:167)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, Le fichier spécifié est introuvable\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 16 more\r\n"
     ]
    }
   ],
   "source": [
    "# Import de DenseVector du package pyspark.ml.linalg\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Création d'un rdd en séparant la variable à expliquer des features\n",
    "rdd_ml = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "# Création d'un DataFrame composé de deux variables : label et features\n",
    "df_ml = spark.createDataFrame(rdd_ml, ['label', 'features'])\n",
    "\n",
    "# Affichage des 10 premières lignes du DataFrame\n",
    "df_ml.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b77c0b-58a3-4229-a0e8-b0c23a7e9202",
   "metadata": {},
   "source": [
    "fin d'évaluer les performances du modèle de régression, il faut mettre de côté une partie des données qui attesteront de la qualité du modèle une fois entraîné. Pour cela, il faut systématiquement diviser les données en un ensemble d'entraînement et un ensemble de test.\n",
    "\n",
    "     - Usuellement, la taille de jeu de test est comprise entre 15% et 30% de la quantité totale de données disponibles. Le choix de la répartition dépend essentiellement de la quantité et de la qualité des données disponibles.\n",
    "     - La méthode randomSplit permet de séparer un DataFrame en deux. Par exemple, la création d'un DataFrame d'entraînement contenant 70% des données et un de test en contenant 30%, se fait de la façon suivante :   \n",
    "     \n",
    "train, test = df.randomSplit([.7, .3], seed= 222)   \n",
    "\n",
    "     - Imposer un seed sert simplement à rendre les résultats reproductibles.   \n",
    "     \n",
    "- (e) Créer deux DataFrames appelés train et test contenant respectivement 80% et 20% des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad4c628-3355-43dd-91d3-cd5951fccae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Décomposition des données en deux ensembles d'entraînement et de test\n",
    "# Par défaut l'échantillon est aléatoirement réparti\n",
    "train, test = df_ml.randomSplit([.8, .2], seed= 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f94bf41-5d7b-48a0-a187-95eacd9940ec",
   "metadata": {},
   "source": [
    "## 3. Régression linéaire\n",
    "\n",
    "Spark ML contient de nombreuses fonctions de Machine Learning, commençons ici par la plus basique : la régression linéaire. Elle est présente sous le nom LinearRegression dans le module pyspark.ml.regression. Cette fonction permet d'effectuer une régression linéaire de façon distribuée et effectue les calculs sur les différents clusters prédéfinis dans la SparkSession, quel que soit leur nombre ou la taille de la base de données.\n",
    "\n",
    "Pour l'utiliser, il faut procéder avec les deux étapes habituelles :\n",
    "\n",
    "Créer la fonction avec les paramètres spécifiques au contexte.\n",
    "Utiliser la méthode fit pour l'appliquer aux données.  \n",
    "\n",
    "     - Cliquez ici pour plus d'informations sur les différents paramètres disponibles de LinearRegression.\n",
    "Exemple :\n",
    "\n",
    "lr = LinearRegression(labelCol='label', featuresCol= 'features', maxIter=10, regParam=0.3)   \n",
    "\n",
    "- (a) Importer la fonction LinearRegression du sous-module pyspark.ml.regression.\n",
    "- (b) Créer lr, une fonction de régression linéaire distribuée pour l'appliquer à l'ensemble train.\n",
    "- (c) Créer linearModel, le modèle issu de lr appliqué à train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158b311f-62d5-49b9-9d3f-a0b211d94e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de LinearRegression du package pyspark.ml.regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Création d'une fonction de régression linéaire\n",
    "lr = LinearRegression(labelCol='label', featuresCol= 'features')\n",
    "\n",
    "# Apprentissage sur les données d'entraînement \n",
    "linearModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f67c4b-ea91-468d-916b-bf7bcca78016",
   "metadata": {},
   "source": [
    "Maintenant que le modèle d'apprentissage est construit, il est possible d'effectuer des prédictions sur les données test. Pour cela, les modèles Spark ML possèdent une méthode transform() permettant d'effectuer des prédictions en prenant en seul argument la base de données de test.\n",
    "\n",
    "- (d) Créer un DataFrame predicted contenant les données prédites et les labels correspondants.\n",
    "- (e) Afficher un extrait de predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55cbc75-eee8-4d13-bc92-55cf454bb50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des prédictions sur les données test\n",
    "predicted = linearModel.transform(test)\n",
    "\n",
    "# Affichage des prédictions\n",
    "predicted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750b8f2f-1638-41e7-8481-74af7d23d92b",
   "metadata": {},
   "source": [
    "## 4. Evaluation du modèle\n",
    "\n",
    "Afin d'évaluer la qualité du modèle, il est possible de chercher les informations au sein de l'attribut summary de notre modèle.\n",
    "\n",
    "     - Taper linearModel.summary. puis appuyez sur Tab pour faire apparaître tous les différents attributs du summary   \n",
    "     \n",
    "- (a) Calculer et afficher le RMSE (Root Mean Squared Error) de la régression.\n",
    "- (b) Calculer et afficher le R 22  de la régression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056833a7-dad5-43a6-b8f2-4b50d28f4790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul et affichage du RMSE\n",
    "print(\"RMSE:\", linearModel.summary.rootMeanSquaredError)\n",
    "\n",
    "# Calcul et affichage du R2\n",
    "print(\"R2:  \", linearModel.summary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17276a01-af97-4c14-a9c3-83874f638464",
   "metadata": {},
   "source": [
    "Bien que la RMSE indique une erreur moyenne relativement faible (inférieur à 10), le R 22  reste très faible (inférieur à 0.25). On peut donc penser que le modèle de régression est dans ce cas un mauvais indicateur de la décennie dans laquelle la chanson a été composée.   \n",
    "\n",
    "Une fois les résultats obtenus, il est possible d'optimiser le modèle par rapport à ces mesures. De façon générale, l'optimisation des modèles se fait en modifiant les variables explicatives utilisées, en utilisant un autre modèle de régression et/ou en changeant les paramètres du modèle grâce à la documentation de PySpark.\n",
    "\n",
    "Une fois que le modèle prédit adéquatement l'année de sortie, Spark ML offre la possibilité de récupérer les coefficients grâce aux attributs coefficients et intercept du modèle.\n",
    "\n",
    "- (c) Afficher les coefficients coefficients du modèle. La fonction pprint du module pprint permet d'avoir un affichage plus élégant des données.\n",
    "- (d) Fermer la session spark en utilisant la méthode stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754c3aad-6aa9-4618-809e-174c44bde555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Affichage des Coefficients du modèle linéaire\n",
    "pprint(linearModel.coefficients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "779ae5e3-2fcc-4372-a5bd-605dceccfccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fermeture de la session Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db13b7d-6421-4517-9fc5-0ef1abb65a08",
   "metadata": {},
   "source": [
    "Aller plus loin - Autres algorithmes de régression\n",
    "\n",
    "Maintenant que vous avez appris à programmer une régression linéaire en utilisant Spark ML, vous n'êtes qu'à quelques pas de maîtriser tout algorithme de régression distribué sous Spark. Pour vous aider à retenir l'essentiel, en voici un aperçu :\n",
    "-  1. Transformer la base de données en format svmlib :\n",
    "  • Sélectionner les variables numériques à utiliser pour la régression.\n",
    "  • Placer la variable à expliquer en première position.\n",
    "  • Mapper un couple (label, vecteur de features) dans un RDD.\n",
    "  • Convertir ce RDD en DataFrame et nommer les variables 'label' et 'features'.\n",
    "- 2. Séparer la base de données en deux échantillons train et test.\n",
    "- 3. Appliquer un modèle de classification.\n",
    "-  4. Evaluer le modèle.   \n",
    "\n",
    "   Spark est en constante amélioration et possède aujourd'hui quelques régresseurs notables. Ils sont utilisables de la même façon en important ces fonction depuis pyspark.ml.regression. Vous êtes invité à consulter la documentation pour observer les différents paramètres à prendre en compte pour optimiser ces algorithmes :   \n",
    "   \n",
    "  • LinearRegression() pour effectuer une régression linéaire lorsque le label est présupposé suivre une loi normale.\n",
    "  • GeneralizedLinearRegression() pour effectuer une régression linéaire généralisée lorsque le label est présupposé suivre une autre loi que l'on spécifie dans le paramètre family (gaussian, binomial, poisson, gamma).\n",
    "  • AFTSurvivalRegression() pour effectuer une analyse de survie.\n",
    "\n",
    "Il est également possible d'utiliser les algorithmes, qui gèrent également les variables catégorielles, détaillés dans l'exercice suivant :\n",
    "\n",
    "  • DecisionTreeRegressor() pour un arbre de décision.\n",
    "  • RandomForestRegressor() pour une forêt aléatoire d'arbres de décision.\n",
    "  • GBTRegressor() pour une forêt d'arbres gradient-boosted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68e2cd0-57ee-4657-b358-c605e9096f67",
   "metadata": {},
   "source": [
    "# D - Utilisation des ML Pipelines \n",
    "\n",
    "## Introduction\n",
    "\n",
    "Jusqu'ici, nous avons appris à maîtriser l'utilisation d'un objet DataFrame et la structure svmlib d'une base de données pour pouvoir appliquer des algorithmes de régression.\n",
    "\n",
    "Dans ce chapitre, vous allez approfondir vos connaissances en Spark ML de façon à pouvoir appliquer tout algorithme de classification sur n'importe quelle base de données structurée.\n",
    "\n",
    "Cela passe par deux étapes importantes :\n",
    "\n",
    "Comprendre comment Spark ML gère les variables catégorielles.\n",
    "Apprendre à utiliser une ML Pipeline.\n",
    "Les ML Pipelines permettent de faire enchaîner une succession d'estimateurs ou de transformateurs de façon à définir un processus de Machine Learning.\n",
    "\n",
    "- (a) Exécuter la cellule ci-dessous pour construire une SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1921d848-7c1a-4f7a-97ae-8e3c08177b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://HappyNous:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x17154206160>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import de SparkSession et SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Définition d'un SparkContext en local\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Construction d'une Session Spark\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Pipelines Spark ML\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff99a41-7916-4bda-89d0-f8e2b354d65e",
   "metadata": {},
   "source": [
    "## 1. Les variables catégorielles\n",
    "Avant d'entraîner les modèles de classification, il est important de comprendre que le format svmlib ne supporte pas les strings. Il faut donc passer par un indexeur, c'est-à-dire un modèle permettant de transformer une variable catégorielle en une série d'indices.\n",
    "\n",
    "Pour comprendre cette démarche, nous allons travailler sur la base de données Human Ressources Analytics qui contient des variables continues et des variables catégorielles.\n",
    "\n",
    "Cette base de données est relativement petite (14999 lignes) et ne requiert pas l'utilisation de Spark. Elle reste cependant convenable pour un exercice d'apprentissage.\n",
    "\n",
    "La base de données prend en compte différents indices de satisfaction et d'implication d'employés dans une société fictive. Ces données doivent permettre de déterminer quels employés sont susceptibles de quitter l'entreprise.\n",
    "\n",
    "- (a) Importer le fichier HR_comma_sep.csv dans un DataFrame nommé hr.\n",
    "- (b) Afficher un extrait du DataFrame hr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "66fd3274-aa92-4de0-a8a8-f57de9adbf55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>sales</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.71</td>\n",
       "      <td>3</td>\n",
       "      <td>132</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3</td>\n",
       "      <td>187</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>marketing</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.77</td>\n",
       "      <td>3</td>\n",
       "      <td>169</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>IT</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.51</td>\n",
       "      <td>3</td>\n",
       "      <td>237</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>technical</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.58</td>\n",
       "      <td>5</td>\n",
       "      <td>161</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>support</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.95</td>\n",
       "      <td>3</td>\n",
       "      <td>242</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.85</td>\n",
       "      <td>4</td>\n",
       "      <td>279</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>support</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.77</td>\n",
       "      <td>5</td>\n",
       "      <td>191</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>technical</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.78</td>\n",
       "      <td>3</td>\n",
       "      <td>178</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>technical</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.89</td>\n",
       "      <td>4</td>\n",
       "      <td>136</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3</td>\n",
       "      <td>217</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.57</td>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.91</td>\n",
       "      <td>4</td>\n",
       "      <td>229</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>support</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.88</td>\n",
       "      <td>3</td>\n",
       "      <td>159</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>technical</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.54</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.83</td>\n",
       "      <td>6</td>\n",
       "      <td>306</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>support</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   satisfaction_level last_evaluation number_project average_montly_hours  \\\n",
       "0                0.87            0.71              3                  132   \n",
       "1                0.88            0.86              3                  187   \n",
       "2                0.59            0.77              3                  169   \n",
       "3                0.96            0.51              3                  237   \n",
       "4                0.67            0.58              5                  161   \n",
       "5                0.87            0.95              3                  242   \n",
       "6                 0.9            0.85              4                  279   \n",
       "7                0.92            0.77              5                  191   \n",
       "8                0.62            0.78              3                  178   \n",
       "9                0.83            0.89              4                  136   \n",
       "10               0.79            0.98              3                  217   \n",
       "11               0.46            0.57              2                  139   \n",
       "12               0.81            0.91              4                  229   \n",
       "13               0.59            0.88              3                  159   \n",
       "14               0.45            0.54              2                  135   \n",
       "15               0.11            0.83              6                  306   \n",
       "\n",
       "   time_spend_company Work_accident left promotion_last_5years      sales  \\\n",
       "0                   2             0    0                     0      sales   \n",
       "1                   3             0    0                     0  marketing   \n",
       "2                   3             0    0                     0         IT   \n",
       "3                   4             0    0                     0  technical   \n",
       "4                   3             1    0                     0    support   \n",
       "5                   5             0    0                     0      sales   \n",
       "6                   6             0    0                     0    support   \n",
       "7                   2             0    0                     0  technical   \n",
       "8                   3             1    0                     0  technical   \n",
       "9                   3             0    0                     0      sales   \n",
       "10                  3             0    0                     0      sales   \n",
       "11                  3             0    1                     0      sales   \n",
       "12                  5             0    1                     0    support   \n",
       "13                  2             0    0                     0  technical   \n",
       "14                  3             0    1                     0      sales   \n",
       "15                  4             0    1                     0    support   \n",
       "\n",
       "    salary  \n",
       "0      low  \n",
       "1      low  \n",
       "2      low  \n",
       "3     high  \n",
       "4      low  \n",
       "5      low  \n",
       "6   medium  \n",
       "7   medium  \n",
       "8      low  \n",
       "9   medium  \n",
       "10     low  \n",
       "11     low  \n",
       "12     low  \n",
       "13  medium  \n",
       "14     low  \n",
       "15     low  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chargement du fichier 'HR_comma_sep.csv'\n",
    "hr = spark.read.csv('HR_comma_sep.csv', header = True)\n",
    "\n",
    "# Affichage d'un extrait du DataFrame\n",
    "hr.sample(False, 0.001, seed=222).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd4b951-e756-47c5-8823-22c21733bba3",
   "metadata": {},
   "source": [
    "La variable à prédire est la variable left. Elle indique si l'employé a quitté la boîte volontairement ou non.\n",
    "\n",
    "- (c) Réordonner les variables de façon à placer la variable left en première colonne.\n",
    "- (d) Afficher un résumé des variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8ac97f57-b02a-4b27-9aae-ac3df8c4b6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>left</th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>sales</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>14999</td>\n",
       "      <td>14999</td>\n",
       "      <td>14999</td>\n",
       "      <td>14999</td>\n",
       "      <td>14999</td>\n",
       "      <td>14999</td>\n",
       "      <td>14999</td>\n",
       "      <td>14999</td>\n",
       "      <td>14999</td>\n",
       "      <td>14999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.2380825388359224</td>\n",
       "      <td>0.6128335222348166</td>\n",
       "      <td>0.7161017401159978</td>\n",
       "      <td>3.80305353690246</td>\n",
       "      <td>201.0503366891126</td>\n",
       "      <td>3.498233215547703</td>\n",
       "      <td>0.1446096406427095</td>\n",
       "      <td>0.021268084538969265</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>0.42592409938029885</td>\n",
       "      <td>0.24863065106114257</td>\n",
       "      <td>0.17116911062327556</td>\n",
       "      <td>1.2325923553183513</td>\n",
       "      <td>49.94309937128406</td>\n",
       "      <td>1.4601362305354808</td>\n",
       "      <td>0.35171855238017957</td>\n",
       "      <td>0.1442814645785825</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.36</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>IT</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>99</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>technical</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                 left   satisfaction_level      last_evaluation  \\\n",
       "0   count                14999                14999                14999   \n",
       "1    mean   0.2380825388359224   0.6128335222348166   0.7161017401159978   \n",
       "2  stddev  0.42592409938029885  0.24863065106114257  0.17116911062327556   \n",
       "3     min                    0                 0.09                 0.36   \n",
       "4     max                    1                    1                    1   \n",
       "\n",
       "       number_project average_montly_hours  time_spend_company  \\\n",
       "0               14999                14999               14999   \n",
       "1    3.80305353690246    201.0503366891126   3.498233215547703   \n",
       "2  1.2325923553183513    49.94309937128406  1.4601362305354808   \n",
       "3                   2                  100                  10   \n",
       "4                   7                   99                   8   \n",
       "\n",
       "         Work_accident promotion_last_5years      sales  salary  \n",
       "0                14999                 14999      14999   14999  \n",
       "1   0.1446096406427095  0.021268084538969265       None    None  \n",
       "2  0.35171855238017957    0.1442814645785825       None    None  \n",
       "3                    0                     0         IT    high  \n",
       "4                    1                     1  technical  medium  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ordonner les variables pour avoir le label en première colonne\n",
    "hr = hr.select( 'left',\n",
    "               'satisfaction_level',\n",
    "               'last_evaluation',\n",
    "               'number_project',\n",
    "               'average_montly_hours',\n",
    "               'time_spend_company',\n",
    "               'Work_accident',\n",
    "               'promotion_last_5years',\n",
    "               'sales',\n",
    "               'salary')\n",
    "\n",
    "# Affichage d'une description des variables\n",
    "hr.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a54ac0-79ac-47c1-9d1f-be4a776f9c7b",
   "metadata": {},
   "source": [
    "   La transformation directe de la base de données en svmlib, comme vu dans l'exercice précédent, génère l'erreur suivante :  ValueError: could not convert string to float: 'sales'\n",
    "   En effet, la fonction DenseVector ne gère pas les strings, il faut donc indexer les variables non numériques.\n",
    "\n",
    "\n",
    "La fonction StringIndexer du package pyspark.ml.feature permet d'indexer les variables selon la fréquence de leurs modalités: la modalité la plus fréquente aura pour indice 0.0, la modalité suivante 1.0, etc. Pour cela, la fonction prend en entrée une variable et en créé une variable indexée.\n",
    "\n",
    "La fonction StringIndexer est comme un estimateur, de la même façon qu'une régression ou un arbre de décision. Elle s'utilise donc en deux étapes :\n",
    "\n",
    "Créer un indexeur en spécifiant les colonnes d'entrée et de sortie (paramètres inputCol et outputCol) et chercher des modalités dans la base de données grâce à la méthode fit.\n",
    "Appliquer l'indexeur à la base de données par le biais de la méthode transform.\n",
    "Exemple :\n",
    "\n",
    "# Deux étapes pour indexer la variable 'name' d'un DataFrame df:\n",
    "\n",
    "# Première étape: création de l'indexeur.\n",
    "\n",
    "nameIndexer = StringIndexer(inputCol='name', outputCol='indexedName').fit(df)\n",
    "\n",
    "# Deuxième étape: application au DataFrame.\n",
    "\n",
    "indexed = nameIndexer.transform(df)\n",
    "\n",
    "- (e) Importer la fonction StringIndexer depuis le package pyspark.ml.feature.\n",
    "- (f) Créer un indexeur SalesIndexer transformant une variable sales en une variable indexedSales.\n",
    "- (g) Indexer la variable sales de hr dans un nouveau DataFrame nommé hrSalesIndexed.\n",
    "- (h) Afficher un extrait de hrSalesIndexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2d82f630-951e-4dbb-a116-6bd4e7655dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left</th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>sales</th>\n",
       "      <th>salary</th>\n",
       "      <th>indexedSales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.71</td>\n",
       "      <td>3</td>\n",
       "      <td>132</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3</td>\n",
       "      <td>187</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>marketing</td>\n",
       "      <td>low</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.77</td>\n",
       "      <td>3</td>\n",
       "      <td>169</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>IT</td>\n",
       "      <td>low</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.51</td>\n",
       "      <td>3</td>\n",
       "      <td>237</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>technical</td>\n",
       "      <td>high</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.58</td>\n",
       "      <td>5</td>\n",
       "      <td>161</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>support</td>\n",
       "      <td>low</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.95</td>\n",
       "      <td>3</td>\n",
       "      <td>242</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.85</td>\n",
       "      <td>4</td>\n",
       "      <td>279</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>support</td>\n",
       "      <td>medium</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.77</td>\n",
       "      <td>5</td>\n",
       "      <td>191</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>technical</td>\n",
       "      <td>medium</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.78</td>\n",
       "      <td>3</td>\n",
       "      <td>178</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>technical</td>\n",
       "      <td>low</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.89</td>\n",
       "      <td>4</td>\n",
       "      <td>136</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3</td>\n",
       "      <td>217</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.57</td>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.91</td>\n",
       "      <td>4</td>\n",
       "      <td>229</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>support</td>\n",
       "      <td>low</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.88</td>\n",
       "      <td>3</td>\n",
       "      <td>159</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>technical</td>\n",
       "      <td>medium</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.54</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.83</td>\n",
       "      <td>6</td>\n",
       "      <td>306</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>support</td>\n",
       "      <td>low</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   left satisfaction_level last_evaluation number_project  \\\n",
       "0     0               0.87            0.71              3   \n",
       "1     0               0.88            0.86              3   \n",
       "2     0               0.59            0.77              3   \n",
       "3     0               0.96            0.51              3   \n",
       "4     0               0.67            0.58              5   \n",
       "5     0               0.87            0.95              3   \n",
       "6     0                0.9            0.85              4   \n",
       "7     0               0.92            0.77              5   \n",
       "8     0               0.62            0.78              3   \n",
       "9     0               0.83            0.89              4   \n",
       "10    0               0.79            0.98              3   \n",
       "11    1               0.46            0.57              2   \n",
       "12    1               0.81            0.91              4   \n",
       "13    0               0.59            0.88              3   \n",
       "14    1               0.45            0.54              2   \n",
       "15    1               0.11            0.83              6   \n",
       "\n",
       "   average_montly_hours time_spend_company Work_accident  \\\n",
       "0                   132                  2             0   \n",
       "1                   187                  3             0   \n",
       "2                   169                  3             0   \n",
       "3                   237                  4             0   \n",
       "4                   161                  3             1   \n",
       "5                   242                  5             0   \n",
       "6                   279                  6             0   \n",
       "7                   191                  2             0   \n",
       "8                   178                  3             1   \n",
       "9                   136                  3             0   \n",
       "10                  217                  3             0   \n",
       "11                  139                  3             0   \n",
       "12                  229                  5             0   \n",
       "13                  159                  2             0   \n",
       "14                  135                  3             0   \n",
       "15                  306                  4             0   \n",
       "\n",
       "   promotion_last_5years      sales  salary  indexedSales  \n",
       "0                      0      sales     low           0.0  \n",
       "1                      0  marketing     low           5.0  \n",
       "2                      0         IT     low           3.0  \n",
       "3                      0  technical    high           1.0  \n",
       "4                      0    support     low           2.0  \n",
       "5                      0      sales     low           0.0  \n",
       "6                      0    support  medium           2.0  \n",
       "7                      0  technical  medium           1.0  \n",
       "8                      0  technical     low           1.0  \n",
       "9                      0      sales  medium           0.0  \n",
       "10                     0      sales     low           0.0  \n",
       "11                     0      sales     low           0.0  \n",
       "12                     0    support     low           2.0  \n",
       "13                     0  technical  medium           1.0  \n",
       "14                     0      sales     low           0.0  \n",
       "15                     0    support     low           2.0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import de StringIndexer du package pyspark.ml.feature\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Création d'un indexeur transformant une variable sales en indexedSales\n",
    "salesIndexer = StringIndexer(inputCol='sales', outputCol='indexedSales').fit(hr)\n",
    "\n",
    "# Création d'un DataFrame hrSalesIndexed indexant la variable sales\n",
    "hrSalesIndexed = salesIndexer.transform(hr)\n",
    "\n",
    "# Affichage d'un extrait du DataFrame hrSalesIndexed \n",
    "hrSalesIndexed.sample(False, 0.001, seed = 222).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd50bd25-fc27-491a-bc2c-18ae88096b30",
   "metadata": {},
   "source": [
    "Un indexeur ainsi construit garde en mémoire les modalités indexées. La fonction IndexToString permet de retrouver une variable d'origine à partir d'une variable indexée ou à partir d'une prédiction de cette variable.\n",
    "\n",
    "Elle fonctionne avec les arguments suivants :\n",
    "\n",
    "inputCol : le nom de la colonne d'entrée indexée.\n",
    "outputCol : le nom de la colonne de sortie à reconstruire.\n",
    "labels : l'emplacement des labels.\n",
    "Exemple :\n",
    "\n",
    "nameReconstructor = IndexToString(inputCol='indexedName',\n",
    "                                  outputCol='nameReconstructed',\n",
    "                                  labels = nameIndexer.labels)    \n",
    "                                  \n",
    "# on récupère les labels depuis l'indexeur préalablement créé grâce à l'argument 'labels'.\n",
    "\n",
    "   L'utilisation de IndexToString ne nécessite pas l'utilisation de la méthode fit (l'adaptation à la base de données est déjà faite dans l'étape d'indexation). Il suffit donc d'appliquer la méthode transform pour créer une nouvelle base de données contenant la nouvelle variable.   \n",
    "   \n",
    "- (i) Importer IndexToString depuis le package pyspark.ml.feature.\n",
    "- (j) Créer une variable salesReconstructed à partir de indexedSales.\n",
    "- (k) Appliquer ce transformateur à hrSalesIndexed en créant une nouvelle table hrSalesReconstructed.\n",
    "- (l) Afficher un extrait de cette table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e6f25592-c73c-4fcf-b982-5b0200ead826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left</th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>sales</th>\n",
       "      <th>salary</th>\n",
       "      <th>indexedSales</th>\n",
       "      <th>salesReconstructed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.71</td>\n",
       "      <td>3</td>\n",
       "      <td>132</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3</td>\n",
       "      <td>187</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>marketing</td>\n",
       "      <td>low</td>\n",
       "      <td>5.0</td>\n",
       "      <td>marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.77</td>\n",
       "      <td>3</td>\n",
       "      <td>169</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>IT</td>\n",
       "      <td>low</td>\n",
       "      <td>3.0</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.51</td>\n",
       "      <td>3</td>\n",
       "      <td>237</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>technical</td>\n",
       "      <td>high</td>\n",
       "      <td>1.0</td>\n",
       "      <td>technical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.58</td>\n",
       "      <td>5</td>\n",
       "      <td>161</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>support</td>\n",
       "      <td>low</td>\n",
       "      <td>2.0</td>\n",
       "      <td>support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.95</td>\n",
       "      <td>3</td>\n",
       "      <td>242</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.85</td>\n",
       "      <td>4</td>\n",
       "      <td>279</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>support</td>\n",
       "      <td>medium</td>\n",
       "      <td>2.0</td>\n",
       "      <td>support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.77</td>\n",
       "      <td>5</td>\n",
       "      <td>191</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>technical</td>\n",
       "      <td>medium</td>\n",
       "      <td>1.0</td>\n",
       "      <td>technical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.78</td>\n",
       "      <td>3</td>\n",
       "      <td>178</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>technical</td>\n",
       "      <td>low</td>\n",
       "      <td>1.0</td>\n",
       "      <td>technical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.89</td>\n",
       "      <td>4</td>\n",
       "      <td>136</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3</td>\n",
       "      <td>217</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.57</td>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.91</td>\n",
       "      <td>4</td>\n",
       "      <td>229</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>support</td>\n",
       "      <td>low</td>\n",
       "      <td>2.0</td>\n",
       "      <td>support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.88</td>\n",
       "      <td>3</td>\n",
       "      <td>159</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>technical</td>\n",
       "      <td>medium</td>\n",
       "      <td>1.0</td>\n",
       "      <td>technical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.54</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.83</td>\n",
       "      <td>6</td>\n",
       "      <td>306</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>support</td>\n",
       "      <td>low</td>\n",
       "      <td>2.0</td>\n",
       "      <td>support</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   left satisfaction_level last_evaluation number_project  \\\n",
       "0     0               0.87            0.71              3   \n",
       "1     0               0.88            0.86              3   \n",
       "2     0               0.59            0.77              3   \n",
       "3     0               0.96            0.51              3   \n",
       "4     0               0.67            0.58              5   \n",
       "5     0               0.87            0.95              3   \n",
       "6     0                0.9            0.85              4   \n",
       "7     0               0.92            0.77              5   \n",
       "8     0               0.62            0.78              3   \n",
       "9     0               0.83            0.89              4   \n",
       "10    0               0.79            0.98              3   \n",
       "11    1               0.46            0.57              2   \n",
       "12    1               0.81            0.91              4   \n",
       "13    0               0.59            0.88              3   \n",
       "14    1               0.45            0.54              2   \n",
       "15    1               0.11            0.83              6   \n",
       "\n",
       "   average_montly_hours time_spend_company Work_accident  \\\n",
       "0                   132                  2             0   \n",
       "1                   187                  3             0   \n",
       "2                   169                  3             0   \n",
       "3                   237                  4             0   \n",
       "4                   161                  3             1   \n",
       "5                   242                  5             0   \n",
       "6                   279                  6             0   \n",
       "7                   191                  2             0   \n",
       "8                   178                  3             1   \n",
       "9                   136                  3             0   \n",
       "10                  217                  3             0   \n",
       "11                  139                  3             0   \n",
       "12                  229                  5             0   \n",
       "13                  159                  2             0   \n",
       "14                  135                  3             0   \n",
       "15                  306                  4             0   \n",
       "\n",
       "   promotion_last_5years      sales  salary  indexedSales salesReconstructed  \n",
       "0                      0      sales     low           0.0              sales  \n",
       "1                      0  marketing     low           5.0          marketing  \n",
       "2                      0         IT     low           3.0                 IT  \n",
       "3                      0  technical    high           1.0          technical  \n",
       "4                      0    support     low           2.0            support  \n",
       "5                      0      sales     low           0.0              sales  \n",
       "6                      0    support  medium           2.0            support  \n",
       "7                      0  technical  medium           1.0          technical  \n",
       "8                      0  technical     low           1.0          technical  \n",
       "9                      0      sales  medium           0.0              sales  \n",
       "10                     0      sales     low           0.0              sales  \n",
       "11                     0      sales     low           0.0              sales  \n",
       "12                     0    support     low           2.0            support  \n",
       "13                     0  technical  medium           1.0          technical  \n",
       "14                     0      sales     low           0.0              sales  \n",
       "15                     0    support     low           2.0            support  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import de IndexToString du package pyspark.ml.feature\n",
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "# Création d'une nouvelle colonne salesReconstructed\n",
    "SalesReconstructor = IndexToString(inputCol='indexedSales',\n",
    "                                   outputCol='salesReconstructed',\n",
    "                                   labels = salesIndexer.labels)\n",
    "\n",
    "# Appliquer le transformateur SalesReconstructor\n",
    "hrSalesReconstructed = SalesReconstructor.transform(hrSalesIndexed)\n",
    "\n",
    "# Affichage d'un extrait de la base de données\n",
    "hrSalesReconstructed.sample(False, 0.001 , seed = 222).toPandas()\n",
    "\n",
    "### On voit apparaître une nouvelle colonne 'salesReconstructed' égale à la colonne 'sales'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f58f31-9d88-4ea5-abd2-5c874c0c88f6",
   "metadata": {},
   "source": [
    "## 2. Les Pipelines\n",
    "Pour appliquer de manière élégante et efficace une succession d'estimateurs ou de transformateurs, Spark ML propose la fonction Pipeline.\n",
    "\n",
    "Les Pipelines sont définies essentiellement par une série d'étapes d'estimateurs ou de transformateurs appelées stages :\n",
    "\n",
    "Pipeline(stages=[estimator1, estimator2, estimator3, ...])   \n",
    "\n",
    "- (a) Importer la fonction Pipeline depuis le package pyspark.ml.\n",
    "- (b) Créer un indexeur SalesIndexer transformant une variable sales en indexedSales.\n",
    "- (c) Créer un indexeur SalaryIndexer transformant une variable salary en indexedSalary.\n",
    "- (d) Créer une Pipeline indexer qui applique les deux transformations.\n",
    "- (e) Indexer les variables de hr dans un nouveau DataFrame nommé hrIndexed.\n",
    "- (f) Afficher un extrait de hrIndexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e73e4797-18a9-4f68-9071-e8925b5eae51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left</th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>sales</th>\n",
       "      <th>salary</th>\n",
       "      <th>indexedSalary</th>\n",
       "      <th>indexedSales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.71</td>\n",
       "      <td>3</td>\n",
       "      <td>132</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3</td>\n",
       "      <td>187</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>marketing</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.77</td>\n",
       "      <td>3</td>\n",
       "      <td>169</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>IT</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.51</td>\n",
       "      <td>3</td>\n",
       "      <td>237</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>technical</td>\n",
       "      <td>high</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.58</td>\n",
       "      <td>5</td>\n",
       "      <td>161</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>support</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.95</td>\n",
       "      <td>3</td>\n",
       "      <td>242</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.85</td>\n",
       "      <td>4</td>\n",
       "      <td>279</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>support</td>\n",
       "      <td>medium</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.77</td>\n",
       "      <td>5</td>\n",
       "      <td>191</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>technical</td>\n",
       "      <td>medium</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.78</td>\n",
       "      <td>3</td>\n",
       "      <td>178</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>technical</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.89</td>\n",
       "      <td>4</td>\n",
       "      <td>136</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3</td>\n",
       "      <td>217</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.57</td>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.91</td>\n",
       "      <td>4</td>\n",
       "      <td>229</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>support</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.88</td>\n",
       "      <td>3</td>\n",
       "      <td>159</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>technical</td>\n",
       "      <td>medium</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.54</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.83</td>\n",
       "      <td>6</td>\n",
       "      <td>306</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>support</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   left satisfaction_level last_evaluation number_project  \\\n",
       "0     0               0.87            0.71              3   \n",
       "1     0               0.88            0.86              3   \n",
       "2     0               0.59            0.77              3   \n",
       "3     0               0.96            0.51              3   \n",
       "4     0               0.67            0.58              5   \n",
       "5     0               0.87            0.95              3   \n",
       "6     0                0.9            0.85              4   \n",
       "7     0               0.92            0.77              5   \n",
       "8     0               0.62            0.78              3   \n",
       "9     0               0.83            0.89              4   \n",
       "10    0               0.79            0.98              3   \n",
       "11    1               0.46            0.57              2   \n",
       "12    1               0.81            0.91              4   \n",
       "13    0               0.59            0.88              3   \n",
       "14    1               0.45            0.54              2   \n",
       "15    1               0.11            0.83              6   \n",
       "\n",
       "   average_montly_hours time_spend_company Work_accident  \\\n",
       "0                   132                  2             0   \n",
       "1                   187                  3             0   \n",
       "2                   169                  3             0   \n",
       "3                   237                  4             0   \n",
       "4                   161                  3             1   \n",
       "5                   242                  5             0   \n",
       "6                   279                  6             0   \n",
       "7                   191                  2             0   \n",
       "8                   178                  3             1   \n",
       "9                   136                  3             0   \n",
       "10                  217                  3             0   \n",
       "11                  139                  3             0   \n",
       "12                  229                  5             0   \n",
       "13                  159                  2             0   \n",
       "14                  135                  3             0   \n",
       "15                  306                  4             0   \n",
       "\n",
       "   promotion_last_5years      sales  salary  indexedSalary  indexedSales  \n",
       "0                      0      sales     low            0.0           0.0  \n",
       "1                      0  marketing     low            0.0           5.0  \n",
       "2                      0         IT     low            0.0           3.0  \n",
       "3                      0  technical    high            2.0           1.0  \n",
       "4                      0    support     low            0.0           2.0  \n",
       "5                      0      sales     low            0.0           0.0  \n",
       "6                      0    support  medium            1.0           2.0  \n",
       "7                      0  technical  medium            1.0           1.0  \n",
       "8                      0  technical     low            0.0           1.0  \n",
       "9                      0      sales  medium            1.0           0.0  \n",
       "10                     0      sales     low            0.0           0.0  \n",
       "11                     0      sales     low            0.0           0.0  \n",
       "12                     0    support     low            0.0           2.0  \n",
       "13                     0  technical  medium            1.0           1.0  \n",
       "14                     0      sales     low            0.0           0.0  \n",
       "15                     0    support     low            0.0           2.0  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import de Pipeline du package pyspark.ml\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Création des indexeurs\n",
    "SalesIndexer = StringIndexer(inputCol='sales', outputCol='indexedSales')\n",
    "SalaryIndexer = StringIndexer(inputCol='salary', outputCol='indexedSalary')\n",
    "\n",
    "# Création d'une Pipeline\n",
    "indexer = Pipeline(stages =  [SalaryIndexer, SalesIndexer])\n",
    "\n",
    "# Indexer les variables de hr\n",
    "hrIndexed = indexer.fit(hr).transform(hr)\n",
    "\n",
    "# Affichage d'un extrait de hrIndexed\n",
    "hrIndexed.sample(False, 0.001 , seed = 222).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00bf34c-bfb4-481d-9f4f-e9047db64a8b",
   "metadata": {},
   "source": [
    "## 3. Mise en forme de la base en format svmlib   \n",
    "\n",
    "À ce stade, les variables sont de type numérique. Il est donc possible de transformer la base de données.\n",
    "\n",
    "Il faut d'abord exclure les versions non indexées des variables.\n",
    "\n",
    "- (a) Créer une base de données hrNumeric excluant les anciennes variables non indexées.\n",
    "- (b) Créer hrLibsvm, une base de données au format svmlib à partir de hrNumeric.\n",
    "- (c) Afficher un extrait de la nouvelle base de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d0018285-d1f6-4d6b-bc7b-197c4e4a25f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de DenseVector du package pyspark.ml.linalg\n",
    "from pyspark.ml.linalg import DenseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "afd2ceba-7c4f-4c6a-82be-248dc7424fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une base de données excluant les variables non indexées\n",
    "hrNumeric = hrIndexed.select('left',\n",
    "                             'satisfaction_level',\n",
    "                             'last_evaluation',\n",
    "                             'number_project',\n",
    "                             'average_montly_hours',\n",
    "                             'time_spend_company',\n",
    "                             'Work_accident',\n",
    "                             'promotion_last_5years',\n",
    "                             'indexedSales',\n",
    "                             'indexedSalary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c09f9896-8075-40d0-9846-7d02332f37ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une variable DenseVector contenant les features en passant par la structure RDD\n",
    "hrRdd = hrNumeric.rdd.map(lambda x: (x[0], DenseVector(x[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bd05b9-78d6-45ee-ae60-32d4e8b728f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation en DataFrame et nommage des variables pour obtenir une base au format svmlib\n",
    "hrLibsvm = spark.createDataFrame(hrRdd, ['label', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07466df9-208e-4c60-99ce-6200154288f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
    ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 25.0 failed 1 times, most recent failure: Lost task 0.0 in stage 25.0 (TID 26)\n",
    "(HappyNous executor driver): \n",
    "java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, Le fichier spécifié est introuvable\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c539f4e5-acdb-414a-b70e-319dbb15d466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage d'un extrait de hrLibsvm\n",
    "hrLibsvm.sample(False, .001, seed = 222).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d49413-78f8-407a-be27-89bad0ca42a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\tlabel\tfeatures\n",
    "0\t0\t[0.54, 0.77, 4.0, 271.0, 3.0, 0.0, 0.0, 2.0, 1.0]\n",
    "1\t0\t[0.72, 0.85, 3.0, 186.0, 4.0, 0.0, 0.0, 1.0, 0.0]\n",
    "2\t0\t[0.57, 0.65, 5.0, 177.0, 2.0, 0.0, 0.0, 3.0, 2.0]\n",
    "3\t0\t[0.89, 0.91, 5.0, 224.0, 3.0, 1.0, 0.0, 0.0, 0.0]\n",
    "4\t0\t[0.83, 0.72, 4.0, 161.0, 3.0, 0.0, 0.0, 8.0, 0.0]\n",
    "5\t0\t[0.89, 0.88, 3.0, 165.0, 4.0, 0.0, 0.0, 0.0, 1.0]\n",
    "6\t0\t[1.0, 0.85, 3.0, 150.0, 3.0, 0.0, 0.0, 1.0, 0.0]\n",
    "7\t0\t[0.81, 0.69, 5.0, 109.0, 2.0, 0.0, 0.0, 3.0, 2.0]\n",
    "8\t0\t[0.6, 0.76, 5.0, 168.0, 2.0, 1.0, 0.0, 7.0, 2.0]\n",
    "9\t0\t[0.71, 0.91, 3.0, 261.0, 3.0, 0.0, 0.0, 0.0, 0.0]\n",
    "10\t1\t[0.77, 0.87, 5.0, 266.0, 5.0, 0.0, 0.0, 7.0, 1.0]\n",
    "11\t0\t[0.94, 0.73, 3.0, 196.0, 3.0, 0.0, 0.0, 8.0, 1.0]\n",
    "12\t0\t[0.54, 0.83, 4.0, 201.0, 8.0, 1.0, 0.0, 0.0, 1.0]\n",
    "13\t0\t[0.55, 0.96, 3.0, 194.0, 3.0, 0.0, 0.0, 4.0, 1.0]\n",
    "14\t1\t[0.84, 1.0, 5.0, 242.0, 5.0, 0.0, 0.0, 0.0, 0.0]  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1738578d-6682-4614-9799-f357f999f7c8",
   "metadata": {},
   "source": [
    "La conversion d'une base de données au format svmlib se fait comme suit :\n",
    "\n",
    "Importer la fonction DenseVector.\n",
    "Transformer chaque ligne en un couple contenant la variable label et un vecteur de features en utilisant la méthode map.\n",
    "Transformer ce rdd en un DataFrame dont les variables sont bien nommées label et features.   \n",
    "\n",
    "## 4. Application d'un classifieur Spark ML\n",
    "La base de données d'apprentissage actuelle est au format svmlib. Il ne reste qu'à spécifier que:\n",
    "\n",
    "La variable label est catégorielle.\n",
    "Certaines features sont catégorielles et d'autres sont continues.\n",
    "Pour cela, deux transformateurs seront créés et intégrés dans une Pipeline générale :\n",
    "\n",
    "\n",
    "- Pour pouvoir classifier sur notre label,\n",
    "- on crée une variable 'indexedLabel' comme vu précédemment.\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(hr_ml)\n",
    "\n",
    "- Pour les features, on utilise la fonction 'VectorIndexer'\n",
    "- qui définie un seuil de nombre de modalités:\n",
    "- ici, si une variable a plus de 5 modalités, alors elle sera considérée comme continue\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\",\n",
    "                               outputCol=\"indexedFeatures\",\n",
    "                               maxCategories = 5).fit(hr_ml)   \n",
    "                               \n",
    "La fonction VectorIndexer a été développée de façon à n'indexer que les variables qui ont moins d'un certain nombre de modalités. Ce nombre est spécifié par l'argument maxCategories. Il faut donc déterminer ce paramètre en regardant le nombre maximal de modalités des variables catégorielles.\n",
    "\n",
    "- (a) Combien de modalités maximales possèdent les features catégorielles ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950ca6b4-0137-4c2b-8fd0-4073b2afdaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hrNumeric.describe().toPandas()\n",
    "\n",
    "# Nous avons 2 variables catégorielles: \"indexedSales\" et \"indexedSalary\"\n",
    "# On peut obtenir le nombre de modalités en regardant 'min' et 'max' des variables indexées:\n",
    "# 'sales' possède le plus de modalités et en possède 10 (10 entiers entre 0.0 et 9.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aea0cd-f0b8-436c-9b9e-e74dac175e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "summary\tleft\tsatisfaction_level\tlast_evaluation\tnumber_project\taverage_montly_hours\ttime_spend_company\tWork_accident\tpromotion_last_5years\tindexedSales\tindexedSalary\n",
    "0\tcount\t14999\t14999\t14999\t14999\t14999\t14999\t14999\t14999\t14999\t14999\n",
    "1\tmean\t0.2380825388359224\t0.6128335222348166\t0.7161017401159978\t3.80305353690246\t201.0503366891126\t3.498233215547703\t0.1446096406427095\t0.021268084538969265\t2.69551303420228\t0.5947063137542503\n",
    "2\tstddev\t0.42592409938029885\t0.24863065106114257\t0.17116911062327556\t1.2325923553183513\t49.94309937128406\t1.4601362305354808\t0.35171855238017957\t0.1442814645785825\t2.754845263313967\t0.6371829504695818\n",
    "3\tmin\t0\t0.09\t0.36\t2\t100\t10\t0\t0\t0.0\t0.0\n",
    "4\tmax\t1\t1\t1\t7\t99\t8\t1\t1\t9.0\t2.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf3fa21-c42a-4fc5-b0ef-bc5670297419",
   "metadata": {},
   "source": [
    "- (b) Importer VectorIndexer depuis le package pyspark.ml.feature.\n",
    "- (c) Créer un transformateur featureIndexer indexant les features pour un nombre maximal de modalités : maxCategories = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900410e0-cc94-4363-948d-a97621f2d341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de VectorIndexer du package pyspark.ml.feature\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "# Création d'un transformateur indexant les features\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\",\n",
    "                               outputCol=\"indexedFeatures\",\n",
    "                               maxCategories = 10).fit(hrLibsvm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3213ab1e-7ac1-46b0-aa59-f615d6c47d5b",
   "metadata": {},
   "source": [
    "   Un nombre maximal de modalités est une des limitations de Spark. En effet, la version actuelle de Spark ML ne permet pas au data scientist de sélectionner soi-même les features continues et catégorielles.\n",
    "\n",
    "   Par exemple, 'sales' est une variable catégorielle possédant 10 modalités, mais la variable 'number_project' contient des entiers entre 2 et 7. Placer le seuil à 10 pousserait donc l'algorithme à considérer la variable 'number_project' comme catégorielle. L'algorithme perd alors l'ordre de cette variable et considère que la différence est la même entre 2 et 3 projets effectués qu'entre 2 et 7. Inversement, si l'on place le seuil à 4, l'algorithme crée un ordre artificiel (dépendant de la fréquence des modalités) dans la variable 'sales'.\n",
    "\n",
    "   Face à ce problème, il faut tester plusieurs seuils et regarder l'importance des variables concernées. Vous êtes également invités à regarder sur la documentation Spark ML si ce problème a été résolu — Spark est en effet très récent et en constante amélioration.\n",
    "Random Forest est le classifieur proposé comme algorithme d'apprentissage. Les forêts aléatoires sont composées d'un ensemble d'arbres décisionnels. Ces arbres se distinguent les uns des autres par le sous-échantillon de données sur lequel ils sont entraînés. Ces sous-échantillons sont tirés au hasard dans le jeu de données initial.\n",
    "\n",
    "Le principe de fonctionnement des forêts aléatoires est simple : de nombreux petits arbres de classification sont produits sur une fraction aléatoire de données. Random Forest fait voter les arbres de classification afin de déduire l'ordre et l'importance des variables explicatives.\n",
    "\n",
    "La fonction RandomForestClassifier() du package pyspark.ml.classification créée un classifieur et prend les arguments suivants:\n",
    "\n",
    "- labelCol : nom de la colonne à utiliser comme label.\n",
    "- featuresCol : nom de la colonne à utiliser contenant le vecteur des features.\n",
    "- predictionCol : nom de la colonne créée contenant les prédictions (par défaut = prediction).\n",
    "- seed : entier fixe permettant de rendre les résultats reproductibles.   \n",
    "\n",
    "D'autres paramètres pour affiner l'analyse, disponibles sur la documentation de Random Forest.\n",
    "Exemple :\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\",\n",
    "                            featuresCol=\"indexedFeatures\",\n",
    "                            predictionCol='prediction',\n",
    "                            seed = 222)   \n",
    "                            \n",
    "Vous êtes invités par la suite à créer une Pipeline permettant d'effectuer un Random Forest sur la base de données :\n",
    "\n",
    "- (d) Importer la fonction RandomForestClassifier à partir du package pyspark.ml.classification.\n",
    "- (e) Créer deux transformateurs labelIndexer et featureIndexer indexant label et features de hrLibsvm.\n",
    "- (f) Créer un objet RandomForestClassifier nommé rf, travaillant sur les versions indexées des features et labels.\n",
    "- (g) En utilisant IndexToString, créer un transformateur labelConverter permettant de rétablir les labels des prédictions.\n",
    "- (h) Créer une Pipeline, pipeline, contenant les 4 estimateurs/transformateurs créés.\n",
    "- (i) Créer deux ensembles train et test contenant respectivement 70% et 30% de hrLibsvm.\n",
    "- (j) Créer un modèle model qui adapte la Pipeline à la base de données train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab05312-25c0-41f9-82e5-3f40b03cc52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import du classifieur RandomForestClassifier du package pyspark.ml.classification\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Création des transformateurs\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(hrLibsvm)\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories = 10).fit(hrLibsvm)\n",
    "\n",
    "# Création d'un classifieur \n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", predictionCol='prediction', seed = 222)\n",
    "\n",
    "# Création d'un transformateur permettant de rétablir les labels des prédictions\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "# Création d'une Pipeline \n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "# Décomposition des données en deux ensembles: données d'entraînement et de test\n",
    "(train, test) = hrLibsvm.randomSplit([0.7, 0.3], seed = 222)\n",
    "\n",
    "# Apprentissage du modèle en utilisant les données d'entraînement\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e81e0b-a67f-4e52-834b-2cde155db633",
   "metadata": {},
   "source": [
    "- (k) Créer un DataFrame predictions qui applique le modèle à la base test.\n",
    "- (l) Afficher les premières lignes de predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b9106-08ec-4c5b-a907-a7065b7fdbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\tlabel\t        features\t                         indexedLabel\t    indexedFeatures\t                                 rawPrediction\t                             probability\t           prediction\tpredictedLabel\n",
    "0\t0\t[0.82, 0.99, 5.0, 252.0, 3.0, 1.0, 0.0, 6.0, 0.0]\t0.0\t[0.82, 0.99, 3.0, 252.0, 1.0, 1.0, 0.0, 6.0, 0.0]\t[18.82616440064275, 1.1738355993572502]\t[0.9413082200321374, 0.05869177996786251]\t0.0\t      0\n",
    "1\t0\t[0.92, 0.58, 3.0, 261.0, 3.0, 1.0, 0.0, 0.0, 1.0]\t0.0\t[0.92, 0.58, 1.0, 261.0, 1.0, 1.0, 0.0, 0.0, 1.0]\t[19.636707405486806, 0.3632925945131942]\t[0.9818353702743403, 0.018164629725659708]\t0.0  \t0\n",
    "2\t0\t[0.97, 0.52, 4.0, 207.0, 3.0, 0.0, 0.0, 4.0, 0.0]\t0.0\t[0.97, 0.52, 2.0, 207.0, 1.0, 0.0, 0.0, 4.0, 0.0]\t[19.445186560955065, 0.5548134390449357]\t[0.9722593280477533, 0.027740671952246786]\t0.0\t    0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c58a9c6-2872-4323-918f-934565dcc8a0",
   "metadata": {},
   "source": [
    "## 5. Evaluation du modèle\n",
    "\n",
    "Une fois le modèle d'apprentissage automatique construit, il est important de vérifier la fiabilité des prédictions, à la fois pour le comparer à d'autres modèles de classification mais également pour optimiser ses paramètres.\n",
    "\n",
    "Pour cela, il existe un sous-module pyspark.ml.evaluation contenant toutes les métriques d'évaluation. En particulier, vous y trouverez la fonction MulticlassClassificationEvaluator permettant d'évaluer des modèles de classification.\n",
    "\n",
    "Cette fonction prend 3 arguments principaux :\n",
    "\n",
    "- metricName : métrique à utiliser, typiquement : 'accuracy'.\n",
    "- labelCol : nom de la colonne à prédire.\n",
    "- predictionCol : nom de la colonne de prédictions.   \n",
    "\n",
    "L'évaluateur créé possède une méthode evaluate permettant de l'appliquer à un échantillon.\n",
    "\n",
    "- (a) Importer la fonction MulticlassClassificationEvaluator.\n",
    "- (b) Créer evaluator l'évaluateur d'accuracy du modèle.\n",
    "- (c) Calculer et afficher la précision accuracy de la prédiction sur l'échantillon test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dd58e7-779c-4a9f-b382-ce9c456a693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import d'un évaluateur MulticlassClassificationEvaluator du package pyspark.ml.evaluation\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Création d'un évaluateur \n",
    "evaluator = MulticlassClassificationEvaluator(metricName='accuracy',\n",
    "                                              labelCol= 'indexedLabel',\n",
    "                                              predictionCol= 'prediction')\n",
    "# Calcul et affichage de la précision du modèle \n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07640939-c27a-45e6-b49f-0e44fcdb9f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" -> 0.9704168534289557 \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9250cd3-db43-4b5a-ae0e-4cbe246c6cb0",
   "metadata": {},
   "source": [
    "   La métrique accuracy correspond au nombre de prédictions correctes divisé par le nombre de prédictions effectuées. Elle est donc comprise entre 0 et 1 ; une accuracy de 0 correspond à des prédictions toutes fausses et une accuracy de 1 correspond à l'absence d'erreur dans la prédiction.\n",
    "   Aller plus loin — Autres algorithmes de classification\n",
    "\n",
    "Vous avez maintenant tous les outils en main pour effectuer tout type de classification en Spark ML. Pour résumer de façon concise, une classification s'effectue de la façon suivante :\n",
    "\n",
    "• 1. Transformer toutes les variables en variables numériques.\n",
    "• 2. Transformer la base en format svmlib.\n",
    "• 3. Créer une Pipeline contenant :\n",
    "  • La transformation de la variable label en catégorie.\n",
    "  • La transformation des features catégorielles.\n",
    "  • Un modèle de classification.\n",
    "  • Un transformateur inverse de l'indexation pour les prédictions créées.\n",
    "• 4. Evaluer le modèle.\n",
    "\n",
    "   Spark est en constante amélioration et possède aujourd'hui quelques classifieurs notables que vous pouvez utiliser de la même façon en important ces fonctions depuis le package pyspark.ml.classification. Vous êtes invités à consulter la documentation pour observer les différents paramètres à prendre en compte pour optimiser ces algorithmes :\n",
    "\n",
    "  • LogisticRegression() pour effectuer une régression logistique.\n",
    "  • DecisionTreeClassifier() pour un arbre de régression simple.\n",
    "  • RandomForestClassifier() pour une forêt aléatoire.\n",
    "  • GBTClassifier() pour une forêt d'arbres gradient-boosted.\n",
    "  • LinearSVC() pour un SVM de régression à noyau linéaire.\n",
    "  • NaiveBayes() pour une classification naïve bayesienne.\n",
    "\n",
    "Pour une liste complète des algorithmes de Spark ML, vous pouvez également consulter la documentation générale de pyspark.ml; la liste des algorithmes disponibles ne cesse de grandir au fur et à mesure du développement de Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661cfcd8-a2b4-487f-9887-d3cc0f4500ac",
   "metadata": {},
   "source": [
    "# E - Model Tuning \n",
    "## Introduction\n",
    "Il existe plusieurs méthodes de tuning qui permettent de régler les paramètres d'un modèle, de façon à éviter le sur-apprentissage (overfitting en anglais). La méthode la plus utilisée est la validation croisée (cross-validation en anglais).\n",
    "\n",
    "Dans ce cas, l'apprentissage des données se fait en 4 étapes :\n",
    "\n",
    "Choix du modèle à optimiser.\n",
    "Création d'une grille de paramètres.\n",
    "Choix d'une métrique d'évaluation.\n",
    "Mise en place d'un crossValidator.\n",
    "\n",
    "\n",
    "- (a) Exécuter la cellule ci-dessous pour construire une SparkSession pour notre exercice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fbd1d6a8-48dd-4801-bfa8-700ccd82dce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://HappyNous:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x17154a353a0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import de SparkSession et de SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Création d'un SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Création d'une session Spark\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ML Tuning\") \\\n",
    "    .getOrCreate()\n",
    "        \n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68fcfd-feff-45ea-aba7-858170b5ef6a",
   "metadata": {},
   "source": [
    "# 1. Importation de la base de données\n",
    "Dans cet exercice, nous allons travailler sur une base de données appelée Year Prediction MSD contenant des caractéristiques audio de 515345 chansons parues entre 1922 et 2011. Cette base de données a été utilisée dans l'exercice 4 pour obtenir une estimation de l'année de sortie de la chanson. Nous allons maintenant régler les paramètres de façon à rendre l'algorithme plus robuste.\n",
    "\n",
    "   La robustesse, en Machine Learning, est la propriété qui caractérise à quel point un algorithme est efficace face à une nouvelle base de données indépendante.\n",
    "   Par exemple, il peut arriver d'entraîner un algorithme avec un très grand nombre de paramètres et très peu de données de façon à obtenir un taux d'erreur proche de 0, mais cela est généralement lié à un sur-apprentissage sur notre échantillon. Un tel algorithme obtient des résultats généralement mauvais sur un nouveau jeu de données : il n'est pas robuste.\n",
    "Pour des questions de temps de calcul et parce qu'il s'agit d'un exercice d'apprentissage, nous allons travailler sur un extrait de la base de données.\n",
    "\n",
    "Nous allons considérer uniquement 10% de la base de données et seulement les 12 premières variables de caractéristiques audio (toute variable de covariance est oubliée).\n",
    "\n",
    "Cela pousse évidemment à obtenir des résultats moins bons mais permet aux algorithmes de s'exécuter en quelques minutes plutôt que quelques heures.\n",
    "\n",
    "- (a) Exécuter la cellule ci-dessous pour charger un extrait de la base de données df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "daa569fc-9e4b-4835-b5b6-d2aafabf39c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de la base de données brute\n",
    "df_full = spark.read.csv('YearPredictionMSD.txt', header=False)\n",
    "\n",
    "# On infère les bons types des colonnes\n",
    "from pyspark.sql.functions import col\n",
    "exprs = [col(c).cast(\"double\") for c in df_full.columns[1:13]]\n",
    "\n",
    "df_casted = df_full.select(df_full._c0.cast('int'),\n",
    "                           *exprs)\n",
    "\n",
    "# Enfin, par soucis de rapidité des calculs,\n",
    "# on ne traitera qu'un extrait de la base de données \n",
    "df = df_casted.sample(False, .1, seed = 222)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a344a55e-86a0-4eb3-921f-04d7369b0446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>_c1</th>\n",
       "      <th>_c2</th>\n",
       "      <th>_c3</th>\n",
       "      <th>_c4</th>\n",
       "      <th>_c5</th>\n",
       "      <th>_c6</th>\n",
       "      <th>_c7</th>\n",
       "      <th>_c8</th>\n",
       "      <th>_c9</th>\n",
       "      <th>_c10</th>\n",
       "      <th>_c11</th>\n",
       "      <th>_c12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2002</td>\n",
       "      <td>30.01595</td>\n",
       "      <td>-110.54782</td>\n",
       "      <td>-37.18609</td>\n",
       "      <td>-37.55900</td>\n",
       "      <td>-38.40125</td>\n",
       "      <td>-30.61400</td>\n",
       "      <td>25.10474</td>\n",
       "      <td>4.18086</td>\n",
       "      <td>-12.28940</td>\n",
       "      <td>-7.92266</td>\n",
       "      <td>-2.35719</td>\n",
       "      <td>14.84515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1984</td>\n",
       "      <td>41.04548</td>\n",
       "      <td>24.49906</td>\n",
       "      <td>28.46745</td>\n",
       "      <td>22.35687</td>\n",
       "      <td>-47.34565</td>\n",
       "      <td>-5.87048</td>\n",
       "      <td>4.26506</td>\n",
       "      <td>-1.73138</td>\n",
       "      <td>0.47459</td>\n",
       "      <td>3.39679</td>\n",
       "      <td>7.83760</td>\n",
       "      <td>0.63716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008</td>\n",
       "      <td>40.94855</td>\n",
       "      <td>-3.70642</td>\n",
       "      <td>32.88170</td>\n",
       "      <td>-1.99998</td>\n",
       "      <td>0.11643</td>\n",
       "      <td>-10.94099</td>\n",
       "      <td>-10.83270</td>\n",
       "      <td>-3.17955</td>\n",
       "      <td>15.79696</td>\n",
       "      <td>8.13097</td>\n",
       "      <td>1.94546</td>\n",
       "      <td>-11.25440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006</td>\n",
       "      <td>50.52707</td>\n",
       "      <td>17.36443</td>\n",
       "      <td>13.54798</td>\n",
       "      <td>9.25023</td>\n",
       "      <td>-12.80509</td>\n",
       "      <td>-1.84387</td>\n",
       "      <td>1.91220</td>\n",
       "      <td>-0.89241</td>\n",
       "      <td>8.36482</td>\n",
       "      <td>10.13537</td>\n",
       "      <td>0.75602</td>\n",
       "      <td>2.72802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990</td>\n",
       "      <td>39.55030</td>\n",
       "      <td>3.34344</td>\n",
       "      <td>-38.88473</td>\n",
       "      <td>-6.86725</td>\n",
       "      <td>-14.86308</td>\n",
       "      <td>-0.98709</td>\n",
       "      <td>-22.38180</td>\n",
       "      <td>3.82114</td>\n",
       "      <td>13.18387</td>\n",
       "      <td>6.79409</td>\n",
       "      <td>0.13491</td>\n",
       "      <td>10.95415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2002</td>\n",
       "      <td>47.04752</td>\n",
       "      <td>-13.51855</td>\n",
       "      <td>22.31563</td>\n",
       "      <td>-6.80266</td>\n",
       "      <td>-47.47207</td>\n",
       "      <td>-12.05640</td>\n",
       "      <td>11.78804</td>\n",
       "      <td>-1.40178</td>\n",
       "      <td>-14.74664</td>\n",
       "      <td>-3.38668</td>\n",
       "      <td>-2.82051</td>\n",
       "      <td>3.07921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2008</td>\n",
       "      <td>44.31169</td>\n",
       "      <td>-42.04020</td>\n",
       "      <td>24.29367</td>\n",
       "      <td>-3.93380</td>\n",
       "      <td>6.25566</td>\n",
       "      <td>-10.15873</td>\n",
       "      <td>-6.90544</td>\n",
       "      <td>-11.30464</td>\n",
       "      <td>-2.99366</td>\n",
       "      <td>-10.19058</td>\n",
       "      <td>0.05695</td>\n",
       "      <td>-9.19643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1988</td>\n",
       "      <td>40.24099</td>\n",
       "      <td>44.83112</td>\n",
       "      <td>-23.83101</td>\n",
       "      <td>16.87212</td>\n",
       "      <td>9.76545</td>\n",
       "      <td>-21.55892</td>\n",
       "      <td>-6.28082</td>\n",
       "      <td>4.84618</td>\n",
       "      <td>4.48798</td>\n",
       "      <td>3.76576</td>\n",
       "      <td>7.18999</td>\n",
       "      <td>3.73851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2002</td>\n",
       "      <td>40.48732</td>\n",
       "      <td>-48.14848</td>\n",
       "      <td>-2.75518</td>\n",
       "      <td>23.62937</td>\n",
       "      <td>-28.73993</td>\n",
       "      <td>20.84762</td>\n",
       "      <td>23.70480</td>\n",
       "      <td>7.42180</td>\n",
       "      <td>20.28495</td>\n",
       "      <td>13.66604</td>\n",
       "      <td>-11.26391</td>\n",
       "      <td>-2.02038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2006</td>\n",
       "      <td>39.97222</td>\n",
       "      <td>-29.38402</td>\n",
       "      <td>56.72433</td>\n",
       "      <td>-0.60655</td>\n",
       "      <td>-13.25543</td>\n",
       "      <td>-6.67884</td>\n",
       "      <td>-30.20624</td>\n",
       "      <td>-0.97975</td>\n",
       "      <td>0.45859</td>\n",
       "      <td>-4.01848</td>\n",
       "      <td>2.19023</td>\n",
       "      <td>2.36963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2008</td>\n",
       "      <td>47.19106</td>\n",
       "      <td>-20.94216</td>\n",
       "      <td>4.84158</td>\n",
       "      <td>-4.00425</td>\n",
       "      <td>-13.76788</td>\n",
       "      <td>-12.15101</td>\n",
       "      <td>-6.53532</td>\n",
       "      <td>-10.40596</td>\n",
       "      <td>10.92870</td>\n",
       "      <td>-6.08920</td>\n",
       "      <td>-0.29055</td>\n",
       "      <td>-9.33880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1993</td>\n",
       "      <td>18.35963</td>\n",
       "      <td>-163.79664</td>\n",
       "      <td>-93.07785</td>\n",
       "      <td>-44.23759</td>\n",
       "      <td>83.88499</td>\n",
       "      <td>-38.67657</td>\n",
       "      <td>-4.16057</td>\n",
       "      <td>24.21348</td>\n",
       "      <td>-19.29345</td>\n",
       "      <td>10.72552</td>\n",
       "      <td>-1.46321</td>\n",
       "      <td>-14.02824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1976</td>\n",
       "      <td>40.57039</td>\n",
       "      <td>36.35769</td>\n",
       "      <td>2.91355</td>\n",
       "      <td>-3.43851</td>\n",
       "      <td>7.99126</td>\n",
       "      <td>-14.93364</td>\n",
       "      <td>4.25739</td>\n",
       "      <td>1.08308</td>\n",
       "      <td>0.38853</td>\n",
       "      <td>2.10504</td>\n",
       "      <td>-1.12272</td>\n",
       "      <td>1.93383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2007</td>\n",
       "      <td>51.27726</td>\n",
       "      <td>60.06329</td>\n",
       "      <td>21.51641</td>\n",
       "      <td>-0.75871</td>\n",
       "      <td>-58.06831</td>\n",
       "      <td>-20.83034</td>\n",
       "      <td>12.42315</td>\n",
       "      <td>-0.45001</td>\n",
       "      <td>8.16084</td>\n",
       "      <td>9.95128</td>\n",
       "      <td>-1.64830</td>\n",
       "      <td>7.39257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2006</td>\n",
       "      <td>33.65108</td>\n",
       "      <td>-98.34396</td>\n",
       "      <td>-3.43118</td>\n",
       "      <td>-3.62528</td>\n",
       "      <td>39.75279</td>\n",
       "      <td>3.44890</td>\n",
       "      <td>-32.22156</td>\n",
       "      <td>5.66247</td>\n",
       "      <td>-8.24809</td>\n",
       "      <td>0.87922</td>\n",
       "      <td>0.61897</td>\n",
       "      <td>-3.45012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2009</td>\n",
       "      <td>46.28578</td>\n",
       "      <td>39.78941</td>\n",
       "      <td>-28.58935</td>\n",
       "      <td>22.06994</td>\n",
       "      <td>3.60725</td>\n",
       "      <td>-0.20528</td>\n",
       "      <td>-9.18882</td>\n",
       "      <td>-8.25995</td>\n",
       "      <td>5.66617</td>\n",
       "      <td>-4.30788</td>\n",
       "      <td>4.82065</td>\n",
       "      <td>4.82386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1994</td>\n",
       "      <td>43.51669</td>\n",
       "      <td>-6.92523</td>\n",
       "      <td>-6.36367</td>\n",
       "      <td>-5.76381</td>\n",
       "      <td>12.84305</td>\n",
       "      <td>-11.44254</td>\n",
       "      <td>-12.96946</td>\n",
       "      <td>-0.09853</td>\n",
       "      <td>31.63224</td>\n",
       "      <td>6.66526</td>\n",
       "      <td>3.87354</td>\n",
       "      <td>21.70676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2006</td>\n",
       "      <td>39.91731</td>\n",
       "      <td>-61.86885</td>\n",
       "      <td>-38.90831</td>\n",
       "      <td>-16.03744</td>\n",
       "      <td>-11.60090</td>\n",
       "      <td>-0.52770</td>\n",
       "      <td>-11.61197</td>\n",
       "      <td>-11.39358</td>\n",
       "      <td>5.90889</td>\n",
       "      <td>-3.42222</td>\n",
       "      <td>-2.03460</td>\n",
       "      <td>10.24517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1999</td>\n",
       "      <td>44.27932</td>\n",
       "      <td>-0.58171</td>\n",
       "      <td>-12.24277</td>\n",
       "      <td>-5.06563</td>\n",
       "      <td>16.25142</td>\n",
       "      <td>-14.68137</td>\n",
       "      <td>-5.41814</td>\n",
       "      <td>2.65599</td>\n",
       "      <td>4.65865</td>\n",
       "      <td>-0.61067</td>\n",
       "      <td>2.30098</td>\n",
       "      <td>9.88158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2007</td>\n",
       "      <td>39.63962</td>\n",
       "      <td>-16.58889</td>\n",
       "      <td>5.51722</td>\n",
       "      <td>9.22267</td>\n",
       "      <td>3.92313</td>\n",
       "      <td>-31.82714</td>\n",
       "      <td>30.85162</td>\n",
       "      <td>-5.43540</td>\n",
       "      <td>17.89619</td>\n",
       "      <td>6.78530</td>\n",
       "      <td>6.21067</td>\n",
       "      <td>14.45103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1991</td>\n",
       "      <td>38.35145</td>\n",
       "      <td>66.68073</td>\n",
       "      <td>69.09327</td>\n",
       "      <td>-4.76214</td>\n",
       "      <td>-21.26586</td>\n",
       "      <td>15.82150</td>\n",
       "      <td>-0.22182</td>\n",
       "      <td>3.58241</td>\n",
       "      <td>9.78323</td>\n",
       "      <td>6.94536</td>\n",
       "      <td>8.03950</td>\n",
       "      <td>13.79405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2002</td>\n",
       "      <td>41.18288</td>\n",
       "      <td>32.22336</td>\n",
       "      <td>-33.14420</td>\n",
       "      <td>-10.71569</td>\n",
       "      <td>-20.01808</td>\n",
       "      <td>1.91290</td>\n",
       "      <td>-24.96773</td>\n",
       "      <td>-6.22413</td>\n",
       "      <td>23.71276</td>\n",
       "      <td>19.08527</td>\n",
       "      <td>-2.25533</td>\n",
       "      <td>13.73073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2008</td>\n",
       "      <td>27.81477</td>\n",
       "      <td>-49.89194</td>\n",
       "      <td>36.53543</td>\n",
       "      <td>0.51573</td>\n",
       "      <td>9.91198</td>\n",
       "      <td>34.43970</td>\n",
       "      <td>-28.00448</td>\n",
       "      <td>10.28704</td>\n",
       "      <td>-3.66040</td>\n",
       "      <td>8.89107</td>\n",
       "      <td>-12.46776</td>\n",
       "      <td>6.30600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1981</td>\n",
       "      <td>41.53853</td>\n",
       "      <td>33.99323</td>\n",
       "      <td>42.22156</td>\n",
       "      <td>13.48436</td>\n",
       "      <td>-7.56169</td>\n",
       "      <td>-6.08609</td>\n",
       "      <td>12.02293</td>\n",
       "      <td>2.79499</td>\n",
       "      <td>-5.19954</td>\n",
       "      <td>-1.15734</td>\n",
       "      <td>-0.89640</td>\n",
       "      <td>-5.28798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1998</td>\n",
       "      <td>41.43116</td>\n",
       "      <td>4.09623</td>\n",
       "      <td>5.75036</td>\n",
       "      <td>0.42435</td>\n",
       "      <td>5.51600</td>\n",
       "      <td>-11.47336</td>\n",
       "      <td>15.79923</td>\n",
       "      <td>-6.40719</td>\n",
       "      <td>10.76245</td>\n",
       "      <td>1.67674</td>\n",
       "      <td>-2.32241</td>\n",
       "      <td>1.04843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1997</td>\n",
       "      <td>47.24948</td>\n",
       "      <td>95.55908</td>\n",
       "      <td>-19.98023</td>\n",
       "      <td>-10.46284</td>\n",
       "      <td>-41.96201</td>\n",
       "      <td>-23.41792</td>\n",
       "      <td>5.18737</td>\n",
       "      <td>-1.48800</td>\n",
       "      <td>20.57714</td>\n",
       "      <td>14.35089</td>\n",
       "      <td>0.25984</td>\n",
       "      <td>8.74644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2007</td>\n",
       "      <td>46.93258</td>\n",
       "      <td>15.68495</td>\n",
       "      <td>-16.33290</td>\n",
       "      <td>31.56935</td>\n",
       "      <td>-21.99036</td>\n",
       "      <td>-6.09517</td>\n",
       "      <td>-2.38415</td>\n",
       "      <td>-9.79773</td>\n",
       "      <td>7.68515</td>\n",
       "      <td>8.98227</td>\n",
       "      <td>2.68005</td>\n",
       "      <td>6.18659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2009</td>\n",
       "      <td>51.15500</td>\n",
       "      <td>72.23928</td>\n",
       "      <td>43.02466</td>\n",
       "      <td>1.86617</td>\n",
       "      <td>19.18457</td>\n",
       "      <td>9.01233</td>\n",
       "      <td>-4.53690</td>\n",
       "      <td>10.82041</td>\n",
       "      <td>1.01368</td>\n",
       "      <td>9.63295</td>\n",
       "      <td>-2.85982</td>\n",
       "      <td>4.40291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2002</td>\n",
       "      <td>44.16603</td>\n",
       "      <td>-36.73031</td>\n",
       "      <td>33.21673</td>\n",
       "      <td>-0.90007</td>\n",
       "      <td>16.17939</td>\n",
       "      <td>-8.52197</td>\n",
       "      <td>-14.46917</td>\n",
       "      <td>-6.53421</td>\n",
       "      <td>-0.88133</td>\n",
       "      <td>-8.36621</td>\n",
       "      <td>3.46150</td>\n",
       "      <td>10.54291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1994</td>\n",
       "      <td>42.05570</td>\n",
       "      <td>14.70146</td>\n",
       "      <td>52.02550</td>\n",
       "      <td>24.67912</td>\n",
       "      <td>-4.29829</td>\n",
       "      <td>11.95730</td>\n",
       "      <td>-18.02036</td>\n",
       "      <td>-12.83135</td>\n",
       "      <td>15.25203</td>\n",
       "      <td>9.59564</td>\n",
       "      <td>2.54203</td>\n",
       "      <td>9.39495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1993</td>\n",
       "      <td>37.86244</td>\n",
       "      <td>-56.65283</td>\n",
       "      <td>29.28905</td>\n",
       "      <td>-10.25730</td>\n",
       "      <td>-30.69466</td>\n",
       "      <td>3.81732</td>\n",
       "      <td>2.33955</td>\n",
       "      <td>-1.93629</td>\n",
       "      <td>-4.94544</td>\n",
       "      <td>-7.54109</td>\n",
       "      <td>-4.73046</td>\n",
       "      <td>8.73309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2004</td>\n",
       "      <td>44.76919</td>\n",
       "      <td>34.92085</td>\n",
       "      <td>21.86728</td>\n",
       "      <td>2.09082</td>\n",
       "      <td>-14.97190</td>\n",
       "      <td>-6.47276</td>\n",
       "      <td>-0.73466</td>\n",
       "      <td>16.12112</td>\n",
       "      <td>-3.28734</td>\n",
       "      <td>2.59602</td>\n",
       "      <td>-2.85071</td>\n",
       "      <td>-5.35670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2009</td>\n",
       "      <td>48.63685</td>\n",
       "      <td>35.25918</td>\n",
       "      <td>-9.39042</td>\n",
       "      <td>-5.02868</td>\n",
       "      <td>9.28983</td>\n",
       "      <td>-30.46190</td>\n",
       "      <td>31.82252</td>\n",
       "      <td>4.94981</td>\n",
       "      <td>-1.09665</td>\n",
       "      <td>6.81644</td>\n",
       "      <td>-9.92324</td>\n",
       "      <td>-8.24287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2009</td>\n",
       "      <td>40.54473</td>\n",
       "      <td>13.94857</td>\n",
       "      <td>-111.50241</td>\n",
       "      <td>50.27658</td>\n",
       "      <td>7.25583</td>\n",
       "      <td>-16.75232</td>\n",
       "      <td>34.50193</td>\n",
       "      <td>12.73817</td>\n",
       "      <td>-4.42280</td>\n",
       "      <td>6.33745</td>\n",
       "      <td>-1.26373</td>\n",
       "      <td>-0.89250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2007</td>\n",
       "      <td>50.28028</td>\n",
       "      <td>22.07044</td>\n",
       "      <td>25.64653</td>\n",
       "      <td>-1.79646</td>\n",
       "      <td>-21.23677</td>\n",
       "      <td>-17.69542</td>\n",
       "      <td>16.22440</td>\n",
       "      <td>0.46425</td>\n",
       "      <td>13.05626</td>\n",
       "      <td>0.98613</td>\n",
       "      <td>-3.98354</td>\n",
       "      <td>6.79822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2004</td>\n",
       "      <td>44.77036</td>\n",
       "      <td>23.60873</td>\n",
       "      <td>-21.71613</td>\n",
       "      <td>-13.35861</td>\n",
       "      <td>-21.08630</td>\n",
       "      <td>-8.43774</td>\n",
       "      <td>1.52172</td>\n",
       "      <td>-3.22226</td>\n",
       "      <td>-0.17274</td>\n",
       "      <td>2.55715</td>\n",
       "      <td>-5.78026</td>\n",
       "      <td>-3.88686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2010</td>\n",
       "      <td>50.50000</td>\n",
       "      <td>10.15220</td>\n",
       "      <td>27.34059</td>\n",
       "      <td>-0.75004</td>\n",
       "      <td>1.78672</td>\n",
       "      <td>-10.92352</td>\n",
       "      <td>-8.21497</td>\n",
       "      <td>-4.60284</td>\n",
       "      <td>4.49600</td>\n",
       "      <td>-3.68910</td>\n",
       "      <td>0.81210</td>\n",
       "      <td>1.48959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2004</td>\n",
       "      <td>48.40884</td>\n",
       "      <td>31.13500</td>\n",
       "      <td>9.91777</td>\n",
       "      <td>0.28751</td>\n",
       "      <td>-15.62152</td>\n",
       "      <td>-5.49118</td>\n",
       "      <td>4.97771</td>\n",
       "      <td>-2.52504</td>\n",
       "      <td>10.33283</td>\n",
       "      <td>0.00178</td>\n",
       "      <td>-5.16821</td>\n",
       "      <td>-6.38352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2004</td>\n",
       "      <td>39.61511</td>\n",
       "      <td>-102.12848</td>\n",
       "      <td>-18.06679</td>\n",
       "      <td>-10.39156</td>\n",
       "      <td>16.06636</td>\n",
       "      <td>-14.79622</td>\n",
       "      <td>3.96682</td>\n",
       "      <td>-6.23173</td>\n",
       "      <td>-2.66770</td>\n",
       "      <td>-4.92328</td>\n",
       "      <td>0.83518</td>\n",
       "      <td>8.82748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2008</td>\n",
       "      <td>50.66256</td>\n",
       "      <td>-13.81389</td>\n",
       "      <td>21.18536</td>\n",
       "      <td>-3.41480</td>\n",
       "      <td>-34.07483</td>\n",
       "      <td>-23.48445</td>\n",
       "      <td>4.86706</td>\n",
       "      <td>5.66116</td>\n",
       "      <td>4.60750</td>\n",
       "      <td>8.84098</td>\n",
       "      <td>0.28798</td>\n",
       "      <td>-0.58842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2004</td>\n",
       "      <td>44.83835</td>\n",
       "      <td>20.14772</td>\n",
       "      <td>21.37151</td>\n",
       "      <td>-12.89110</td>\n",
       "      <td>14.30318</td>\n",
       "      <td>-9.53865</td>\n",
       "      <td>-11.54369</td>\n",
       "      <td>1.16675</td>\n",
       "      <td>2.57744</td>\n",
       "      <td>2.77692</td>\n",
       "      <td>-6.71109</td>\n",
       "      <td>1.33080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2005</td>\n",
       "      <td>42.82639</td>\n",
       "      <td>0.02988</td>\n",
       "      <td>-33.52695</td>\n",
       "      <td>-8.31738</td>\n",
       "      <td>-19.60415</td>\n",
       "      <td>0.17425</td>\n",
       "      <td>-4.02568</td>\n",
       "      <td>-11.75326</td>\n",
       "      <td>17.21337</td>\n",
       "      <td>-5.00630</td>\n",
       "      <td>-2.53273</td>\n",
       "      <td>3.44127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1989</td>\n",
       "      <td>42.33390</td>\n",
       "      <td>73.53657</td>\n",
       "      <td>15.68442</td>\n",
       "      <td>-3.32099</td>\n",
       "      <td>34.01856</td>\n",
       "      <td>6.99244</td>\n",
       "      <td>-21.72659</td>\n",
       "      <td>-6.19977</td>\n",
       "      <td>14.45299</td>\n",
       "      <td>5.02854</td>\n",
       "      <td>-1.37727</td>\n",
       "      <td>14.85490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1999</td>\n",
       "      <td>38.76044</td>\n",
       "      <td>-33.43009</td>\n",
       "      <td>-9.32789</td>\n",
       "      <td>-3.26242</td>\n",
       "      <td>0.40603</td>\n",
       "      <td>8.24997</td>\n",
       "      <td>-8.41986</td>\n",
       "      <td>-1.03153</td>\n",
       "      <td>-24.52371</td>\n",
       "      <td>-4.98285</td>\n",
       "      <td>-4.36728</td>\n",
       "      <td>-0.10840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2004</td>\n",
       "      <td>43.13062</td>\n",
       "      <td>56.26682</td>\n",
       "      <td>-16.07337</td>\n",
       "      <td>15.52141</td>\n",
       "      <td>46.56519</td>\n",
       "      <td>2.58477</td>\n",
       "      <td>-15.46518</td>\n",
       "      <td>1.89385</td>\n",
       "      <td>-15.29057</td>\n",
       "      <td>-1.60859</td>\n",
       "      <td>0.00168</td>\n",
       "      <td>-8.62262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2001</td>\n",
       "      <td>38.92511</td>\n",
       "      <td>-56.28321</td>\n",
       "      <td>-4.59506</td>\n",
       "      <td>-21.10762</td>\n",
       "      <td>30.02985</td>\n",
       "      <td>-7.05301</td>\n",
       "      <td>-5.93480</td>\n",
       "      <td>-6.77425</td>\n",
       "      <td>-3.26310</td>\n",
       "      <td>-12.59730</td>\n",
       "      <td>-4.34072</td>\n",
       "      <td>2.09024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2007</td>\n",
       "      <td>36.24478</td>\n",
       "      <td>-56.25952</td>\n",
       "      <td>29.95722</td>\n",
       "      <td>31.84781</td>\n",
       "      <td>-39.08352</td>\n",
       "      <td>8.49230</td>\n",
       "      <td>7.43033</td>\n",
       "      <td>2.25570</td>\n",
       "      <td>8.93655</td>\n",
       "      <td>3.68914</td>\n",
       "      <td>1.55900</td>\n",
       "      <td>0.40857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1994</td>\n",
       "      <td>50.36971</td>\n",
       "      <td>32.51961</td>\n",
       "      <td>11.38290</td>\n",
       "      <td>-5.56995</td>\n",
       "      <td>-23.18666</td>\n",
       "      <td>-20.63407</td>\n",
       "      <td>-9.75664</td>\n",
       "      <td>-11.86098</td>\n",
       "      <td>3.84577</td>\n",
       "      <td>4.53709</td>\n",
       "      <td>1.54454</td>\n",
       "      <td>-3.26180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2007</td>\n",
       "      <td>51.31211</td>\n",
       "      <td>50.22713</td>\n",
       "      <td>29.72590</td>\n",
       "      <td>0.30388</td>\n",
       "      <td>-15.20221</td>\n",
       "      <td>-26.31681</td>\n",
       "      <td>9.64328</td>\n",
       "      <td>-2.93011</td>\n",
       "      <td>15.12701</td>\n",
       "      <td>10.01870</td>\n",
       "      <td>0.74951</td>\n",
       "      <td>3.41399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     _c0       _c1        _c2        _c3       _c4       _c5       _c6  \\\n",
       "0   2002  30.01595 -110.54782  -37.18609 -37.55900 -38.40125 -30.61400   \n",
       "1   1984  41.04548   24.49906   28.46745  22.35687 -47.34565  -5.87048   \n",
       "2   2008  40.94855   -3.70642   32.88170  -1.99998   0.11643 -10.94099   \n",
       "3   2006  50.52707   17.36443   13.54798   9.25023 -12.80509  -1.84387   \n",
       "4   1990  39.55030    3.34344  -38.88473  -6.86725 -14.86308  -0.98709   \n",
       "5   2002  47.04752  -13.51855   22.31563  -6.80266 -47.47207 -12.05640   \n",
       "6   2008  44.31169  -42.04020   24.29367  -3.93380   6.25566 -10.15873   \n",
       "7   1988  40.24099   44.83112  -23.83101  16.87212   9.76545 -21.55892   \n",
       "8   2002  40.48732  -48.14848   -2.75518  23.62937 -28.73993  20.84762   \n",
       "9   2006  39.97222  -29.38402   56.72433  -0.60655 -13.25543  -6.67884   \n",
       "10  2008  47.19106  -20.94216    4.84158  -4.00425 -13.76788 -12.15101   \n",
       "11  1993  18.35963 -163.79664  -93.07785 -44.23759  83.88499 -38.67657   \n",
       "12  1976  40.57039   36.35769    2.91355  -3.43851   7.99126 -14.93364   \n",
       "13  2007  51.27726   60.06329   21.51641  -0.75871 -58.06831 -20.83034   \n",
       "14  2006  33.65108  -98.34396   -3.43118  -3.62528  39.75279   3.44890   \n",
       "15  2009  46.28578   39.78941  -28.58935  22.06994   3.60725  -0.20528   \n",
       "16  1994  43.51669   -6.92523   -6.36367  -5.76381  12.84305 -11.44254   \n",
       "17  2006  39.91731  -61.86885  -38.90831 -16.03744 -11.60090  -0.52770   \n",
       "18  1999  44.27932   -0.58171  -12.24277  -5.06563  16.25142 -14.68137   \n",
       "19  2007  39.63962  -16.58889    5.51722   9.22267   3.92313 -31.82714   \n",
       "20  1991  38.35145   66.68073   69.09327  -4.76214 -21.26586  15.82150   \n",
       "21  2002  41.18288   32.22336  -33.14420 -10.71569 -20.01808   1.91290   \n",
       "22  2008  27.81477  -49.89194   36.53543   0.51573   9.91198  34.43970   \n",
       "23  1981  41.53853   33.99323   42.22156  13.48436  -7.56169  -6.08609   \n",
       "24  1998  41.43116    4.09623    5.75036   0.42435   5.51600 -11.47336   \n",
       "25  1997  47.24948   95.55908  -19.98023 -10.46284 -41.96201 -23.41792   \n",
       "26  2007  46.93258   15.68495  -16.33290  31.56935 -21.99036  -6.09517   \n",
       "27  2009  51.15500   72.23928   43.02466   1.86617  19.18457   9.01233   \n",
       "28  2002  44.16603  -36.73031   33.21673  -0.90007  16.17939  -8.52197   \n",
       "29  1994  42.05570   14.70146   52.02550  24.67912  -4.29829  11.95730   \n",
       "30  1993  37.86244  -56.65283   29.28905 -10.25730 -30.69466   3.81732   \n",
       "31  2004  44.76919   34.92085   21.86728   2.09082 -14.97190  -6.47276   \n",
       "32  2009  48.63685   35.25918   -9.39042  -5.02868   9.28983 -30.46190   \n",
       "33  2009  40.54473   13.94857 -111.50241  50.27658   7.25583 -16.75232   \n",
       "34  2007  50.28028   22.07044   25.64653  -1.79646 -21.23677 -17.69542   \n",
       "35  2004  44.77036   23.60873  -21.71613 -13.35861 -21.08630  -8.43774   \n",
       "36  2010  50.50000   10.15220   27.34059  -0.75004   1.78672 -10.92352   \n",
       "37  2004  48.40884   31.13500    9.91777   0.28751 -15.62152  -5.49118   \n",
       "38  2004  39.61511 -102.12848  -18.06679 -10.39156  16.06636 -14.79622   \n",
       "39  2008  50.66256  -13.81389   21.18536  -3.41480 -34.07483 -23.48445   \n",
       "40  2004  44.83835   20.14772   21.37151 -12.89110  14.30318  -9.53865   \n",
       "41  2005  42.82639    0.02988  -33.52695  -8.31738 -19.60415   0.17425   \n",
       "42  1989  42.33390   73.53657   15.68442  -3.32099  34.01856   6.99244   \n",
       "43  1999  38.76044  -33.43009   -9.32789  -3.26242   0.40603   8.24997   \n",
       "44  2004  43.13062   56.26682  -16.07337  15.52141  46.56519   2.58477   \n",
       "45  2001  38.92511  -56.28321   -4.59506 -21.10762  30.02985  -7.05301   \n",
       "46  2007  36.24478  -56.25952   29.95722  31.84781 -39.08352   8.49230   \n",
       "47  1994  50.36971   32.51961   11.38290  -5.56995 -23.18666 -20.63407   \n",
       "48  2007  51.31211   50.22713   29.72590   0.30388 -15.20221 -26.31681   \n",
       "\n",
       "         _c7       _c8       _c9      _c10      _c11      _c12  \n",
       "0   25.10474   4.18086 -12.28940  -7.92266  -2.35719  14.84515  \n",
       "1    4.26506  -1.73138   0.47459   3.39679   7.83760   0.63716  \n",
       "2  -10.83270  -3.17955  15.79696   8.13097   1.94546 -11.25440  \n",
       "3    1.91220  -0.89241   8.36482  10.13537   0.75602   2.72802  \n",
       "4  -22.38180   3.82114  13.18387   6.79409   0.13491  10.95415  \n",
       "5   11.78804  -1.40178 -14.74664  -3.38668  -2.82051   3.07921  \n",
       "6   -6.90544 -11.30464  -2.99366 -10.19058   0.05695  -9.19643  \n",
       "7   -6.28082   4.84618   4.48798   3.76576   7.18999   3.73851  \n",
       "8   23.70480   7.42180  20.28495  13.66604 -11.26391  -2.02038  \n",
       "9  -30.20624  -0.97975   0.45859  -4.01848   2.19023   2.36963  \n",
       "10  -6.53532 -10.40596  10.92870  -6.08920  -0.29055  -9.33880  \n",
       "11  -4.16057  24.21348 -19.29345  10.72552  -1.46321 -14.02824  \n",
       "12   4.25739   1.08308   0.38853   2.10504  -1.12272   1.93383  \n",
       "13  12.42315  -0.45001   8.16084   9.95128  -1.64830   7.39257  \n",
       "14 -32.22156   5.66247  -8.24809   0.87922   0.61897  -3.45012  \n",
       "15  -9.18882  -8.25995   5.66617  -4.30788   4.82065   4.82386  \n",
       "16 -12.96946  -0.09853  31.63224   6.66526   3.87354  21.70676  \n",
       "17 -11.61197 -11.39358   5.90889  -3.42222  -2.03460  10.24517  \n",
       "18  -5.41814   2.65599   4.65865  -0.61067   2.30098   9.88158  \n",
       "19  30.85162  -5.43540  17.89619   6.78530   6.21067  14.45103  \n",
       "20  -0.22182   3.58241   9.78323   6.94536   8.03950  13.79405  \n",
       "21 -24.96773  -6.22413  23.71276  19.08527  -2.25533  13.73073  \n",
       "22 -28.00448  10.28704  -3.66040   8.89107 -12.46776   6.30600  \n",
       "23  12.02293   2.79499  -5.19954  -1.15734  -0.89640  -5.28798  \n",
       "24  15.79923  -6.40719  10.76245   1.67674  -2.32241   1.04843  \n",
       "25   5.18737  -1.48800  20.57714  14.35089   0.25984   8.74644  \n",
       "26  -2.38415  -9.79773   7.68515   8.98227   2.68005   6.18659  \n",
       "27  -4.53690  10.82041   1.01368   9.63295  -2.85982   4.40291  \n",
       "28 -14.46917  -6.53421  -0.88133  -8.36621   3.46150  10.54291  \n",
       "29 -18.02036 -12.83135  15.25203   9.59564   2.54203   9.39495  \n",
       "30   2.33955  -1.93629  -4.94544  -7.54109  -4.73046   8.73309  \n",
       "31  -0.73466  16.12112  -3.28734   2.59602  -2.85071  -5.35670  \n",
       "32  31.82252   4.94981  -1.09665   6.81644  -9.92324  -8.24287  \n",
       "33  34.50193  12.73817  -4.42280   6.33745  -1.26373  -0.89250  \n",
       "34  16.22440   0.46425  13.05626   0.98613  -3.98354   6.79822  \n",
       "35   1.52172  -3.22226  -0.17274   2.55715  -5.78026  -3.88686  \n",
       "36  -8.21497  -4.60284   4.49600  -3.68910   0.81210   1.48959  \n",
       "37   4.97771  -2.52504  10.33283   0.00178  -5.16821  -6.38352  \n",
       "38   3.96682  -6.23173  -2.66770  -4.92328   0.83518   8.82748  \n",
       "39   4.86706   5.66116   4.60750   8.84098   0.28798  -0.58842  \n",
       "40 -11.54369   1.16675   2.57744   2.77692  -6.71109   1.33080  \n",
       "41  -4.02568 -11.75326  17.21337  -5.00630  -2.53273   3.44127  \n",
       "42 -21.72659  -6.19977  14.45299   5.02854  -1.37727  14.85490  \n",
       "43  -8.41986  -1.03153 -24.52371  -4.98285  -4.36728  -0.10840  \n",
       "44 -15.46518   1.89385 -15.29057  -1.60859   0.00168  -8.62262  \n",
       "45  -5.93480  -6.77425  -3.26310 -12.59730  -4.34072   2.09024  \n",
       "46   7.43033   2.25570   8.93655   3.68914   1.55900   0.40857  \n",
       "47  -9.75664 -11.86098   3.84577   4.53709   1.54454  -3.26180  \n",
       "48   9.64328  -2.93011  15.12701  10.01870   0.74951   3.41399  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(False, .001, seed = 222).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f873a49f-362d-4054-a8f9-b5644fe98655",
   "metadata": {},
   "source": [
    "- (b) Convertir cette base de données au format svmlib dans une variable df_ml.\n",
    "- (c) Afficher un extrait de cette base de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e1476-0122-4801-92ce-27d8d6c2b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Conversion de la base de données au format svmlib\n",
    "rdd_ml = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "df_ml = spark.createDataFrame(rdd_ml, ['label', 'features'])\n",
    "\n",
    "df_ml.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5875b63e-36f1-47e3-a1fa-c3563036f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
    ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 5) \n",
    "(HappyNous executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, Le fichier spécifié est introuvable\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755494db-625b-4a68-a7d9-2754036c9fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "+-----+--------------------+\n",
    "|label|            features|\n",
    "+-----+--------------------+\n",
    "| 2005|[42.15927,-36.860...|\n",
    "| 1997|[40.53094,-23.737...|\n",
    "| 2000|[50.76908,29.8401...|\n",
    "| 2003|[46.3173,13.03044...|\n",
    "| 2009|[49.18434,51.1999...|\n",
    "| 2009|[47.48739,43.6001...|\n",
    "| 2009|[49.73491,106.182...|\n",
    "| 2007|[42.26736,19.4688...|\n",
    "| 2007|[45.13814,35.5714...|\n",
    "| 2007|[44.85672,25.3150...|\n",
    "| 2007|[45.14394,28.8183...|\n",
    "| 2004|[48.58166,39.6431...|\n",
    "| 2009|[41.53981,12.295,...|\n",
    "| 2009|[46.54569,66.5667...|\n",
    "| 2009|[43.76116,-11.945...|\n",
    "| 2009|[43.667,-20.98101...|\n",
    "| 2009|[40.00607,69.8926...|\n",
    "| 1991|[31.1901,-98.5275...|\n",
    "| 1991|[32.60431,-68.393...|\n",
    "| 1991|[28.10519,-105.28...|\n",
    "+-----+--------------------+\n",
    "only showing top 20 rows \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b68741d-5457-4b71-9e3c-022db8ddf9e0",
   "metadata": {},
   "source": [
    "Pour la suite, il est important de garder un échantillon test qui ne sera jamais utilisé par l'algorithme. Cet échantillon sera utilisé pour mesurer les performances du modèle une fois entraîné.\n",
    "\n",
    "- (d) Séparer df_ml en un ensemble train et un ensemble test comprenant respectivement 80% et 20% des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb493c2-22a2-483c-bf1e-70f567c4722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Décomposition des données en deux ensembles d'entraînement et de test\n",
    "# Par défaut l'échantillon est aléatoirement réparti\n",
    "train, test = df_ml.randomSplit([0.8, 0.2], seed=222)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4b6249-4b60-4d27-aab6-cbc157007081",
   "metadata": {},
   "source": [
    "Dans un premier temps, nous allons instancier un modèle de régression linéaire.\n",
    "\n",
    "- (e) Importer la fonction LinearRegression depuis le package pyspark.ml.regression.\n",
    "- (f) Créer un estimateur lr permettant d'effectuer une régression linéaire entre le label 'label'et les features 'features'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0f4a39-64b2-43ee-b0ba-59f4ffa29b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de LinearRegression du package pyspark.ml.regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Création d'un estimateur LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol = 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c07b226-116c-4a32-9837-42881b348ffc",
   "metadata": {},
   "source": [
    "## 2. Création d'une grille de paramètres\n",
    "L'objectif est de trouver le jeu de paramètres qui rend l'algorithme le plus robuste possible. Pour cela, nous allons donner à l'algorithme une grille de paramètres à tester.\n",
    "\n",
    "La fonction ParamGridBuilder() du package pyspark.ml.tuning est dédiée au tuning des paramètres.\n",
    "Elle s'utilise de la façon suivante :\n",
    "\n",
    "Appeler la fonction ParamGridBuilder() sans argument.\n",
    "Ajouter le ou les paramètres en utilisant la méthode addGrid() appliquée au constructeur.\n",
    "Construire la grille par le biais de la méthode build().\n",
    "Exemple :\n",
    "\n",
    "- Création d'une grille contenant les valeurs 0 et 1\n",
    "-  pour les paramètres regParam et elasticNetParam\n",
    "param_grid = ParamGridBuilder().\\\n",
    "   addGrid(lr.regParam, [0, 1]).\\\n",
    "   addGrid(lr.elasticNetParam, [0, 1]).\\\n",
    "   build()   \n",
    "   \n",
    "- (a) Créer une grille de paramètres, appelée param_grid, contenant les valeurs 0, 0.5 et 1 pour les paramètres regParam et elasticNetParam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3417557-c2ae-4352-9463-fb65969267ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de ParamGridBuilder du package pyspark.ml.tuning\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "# Création d'une grille de paramètres\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(lr.regParam, [0, 0.5, 1]).\\\n",
    "    addGrid(lr.elasticNetParam, [0, 0.5, 1]).\\\n",
    "    build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530a8849-dd0b-4700-8806-f1d2756c5c3e",
   "metadata": {},
   "source": [
    "3. Choix d'une métrique d'évaluation\n",
    "\n",
    "Pour l'évaluation des modèles d'apprentissage automatique, un évaluateur est défini pour optimiser les paramètres. Il est alors possible de minimiser une métrique pour choisir les paramètres optimaux.\n",
    "\n",
    "Pour chaque algorithme, il existe un évaluateur au sein du package pyspark.ml.evaluation. Par exemple, pour une régression il existe le RegressionEvaluator permettant d'utiliser la métrique R² ou RMSE.\n",
    "\n",
    "Cet évaluateur est défini en spécifiant les noms des colonnes de label et de prédiction ainsi que la métrique à utiliser.\n",
    "\n",
    "Exemple :\n",
    "\n",
    "ev = RegressionEvaluator(predictionCol='prediction',\n",
    "                                labelCol='label',\n",
    "                                metricName='rmse')   \n",
    "                                \n",
    "- (a) Importer la fonction RegressionEvaluator du package pyspark.ml.evaluation.\n",
    "- (b) Créer un évaluateur evaluator prenant en compte la métrique d'évaluation R² r2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f6ddd5-19f7-4b44-ab35-bddf7f56152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de RegressionEvaluator du package pyspark.ml.evaluation\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Création d'un évaluateur ayant pour métrique d'évaluation r2\n",
    "ev = RegressionEvaluator(predictionCol='prediction',\n",
    "                                labelCol='label',\n",
    "                                metricName='r2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917274b9-c944-41bf-a277-7377bb820e77",
   "metadata": {},
   "source": [
    "## 4. Réglage des paramètres par validation croisée\n",
    "\n",
    "Il est possible de faire varier les paramètres dans cette grille de façon à obtenir les meilleurs paramètres possibles suivant plusieurs méthodes une fois la grille de paramètres construite. La méthode la plus utilisée est la validation croisée.\n",
    "\n",
    "La validation croisée à k échantillons (k-fold cross validation en anglais) est une méthode très efficace pour améliorer de façon importante la robustesse d'un modèle. Son principe de fonctionnement est le suivant :\n",
    "\n",
    "Division de l'échantillon original en k échantillons.\n",
    "- Sélection d'un des k échantillons (en anglais : hold-out) comme ensemble de validation et les (k-1) autres échantillons constitueront l'ensemble d'apprentissage.\n",
    "- Estimation de l'erreur en calculant un test, une mesure ou un score de performance du modèle sur l'échantillon de test.\n",
    "- Répétition de l'opération en sélectionnant un autre échantillon de validation parmi les (k-1) échantillons qui n'ont pas encore été utilisés pour la validation du modèle.   \n",
    "\n",
    "L'opération se répète ainsi k fois pour qu'en fin de compte chaque sous-échantillon ait été utilisé exactement une fois comme ensemble de validation. Enfin, la moyenne des k scores de performance est calculée pour estimer l'erreur de prédiction.    \n",
    "\n",
    "Le package pyspark.ml.tuning fournit une fonction CrossValidator qui se construit en prenant les arguments suivants:\n",
    "\n",
    "- estimator : l'estimateur à prendre en compte (régression linéaire, Random Forest, etc.).\n",
    "- estimatorParamMaps : la grille de paramètres.\n",
    "- evaluator : la métrique d'évaluation.\n",
    "- numFolds : le nombre d'échantillons à créer à partir du jeu de données. Il s'agit de k.   \n",
    "\n",
    "- (a) Importer la fonction CrossValidator du package pyspark.ml.tuning.\n",
    "- (b) Créer un objet CrossValidator à 3 folds nommé cv prenant comme argument l'estimateur lr, la grille de paramètres et l'évaluateur déjà construits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda27f18-1bef-43db-9249-965aa275f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de CrossValidator du package pyspark.ml.tuning\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "# Création d'un objet CrossValidator à 3 folds\n",
    "cv = CrossValidator(estimator = lr,\n",
    "                    estimatorParamMaps = param_grid,\n",
    "                    evaluator=ev,\n",
    "                    numFolds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dba7043-c3db-4353-a6a6-a6f955f7c41e",
   "metadata": {},
   "source": [
    "## 5. Application du modèle\n",
    "Entrainons maintenant le modèle à l'aide de la méthode fit.\n",
    "\n",
    "     - Il faut instancier de nouveau les variables lr, ev et cv.   \n",
    "     \n",
    "- (a) Dans une variable cv_model, appliquer le crossValidator à la base de données train, à l'aide de la méthode fit.\n",
    "      - Cette cellule peut mettre du temps à s'exécuter (~2 minutes).\n",
    "- (b) Bonus: Mesurer le temps pris par l'ordinateur pour appliquer le modèle (cela devrait prendre quelques minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa2e0c4-7c84-4726-b428-567ab7e4e1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de la bibliothèque time et calcul du temps au début de l'exécution (t0)\n",
    "from time import time\n",
    "t0 = time()\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol = 'label')\n",
    "ev = RegressionEvaluator(predictionCol='prediction', labelCol='label', metricName='r2')\n",
    "cv = CrossValidator(estimator = lr, estimatorParamMaps = param_grid, evaluator = ev, numFolds = 3)\n",
    "\n",
    "cv_model = cv.fit(train)\n",
    "\n",
    "tt = time() - t0\n",
    "print(\"Réalisé en {} secondes\".format(round(tt,3)))\n",
    "# -> réalmisé en 19.4 secondes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be51b506-d058-477c-9504-c7ccdb8f5742",
   "metadata": {},
   "source": [
    "- (c) Utiliser ce modèle pour prédire l'année de sortie des chansons de l'échantillon train dans un DataFrame pred_train.\n",
    "- (d) Utiliser ce modèle pour prédire l'année de sortie des chansons de l'échantillon test dans un DataFrame pred_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3d90ac-b749-4705-ab00-242e6c992fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des prédictions des données d'entraînement\n",
    "pred_train = cv_model.transform(train)\n",
    "\n",
    "# Calcul des prédictions des données de test\n",
    "pred_test  = cv_model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2360428-9f78-41f4-894e-9e9f16075548",
   "metadata": {},
   "source": [
    "On peut maintenant obtenir les scores pour les échantillons train et test. L'échantillon test est important car c'est le seul échantillon n'ayant jamais servi d'entraînement dans la construction du modèle.\n",
    "\n",
    "Pour obtenir le score, il faut appliquer deux méthodes successives :\n",
    "\n",
    "setMetricName pour spécifier quelle métrique appliquer.\n",
    "evaluate pour spécifier la base de données dont on veut le score.\n",
    "Exemple :\n",
    "\n",
    "     - ev.setMetricName('r2').evaluate(pred_train)\n",
    "     -  Pour obtenir le R2 sur la base d'entraînement   \n",
    "     \n",
    "- (e) Calculer le RMSE obtenu sur l'échantillon test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a6fd2-81c5-4b71-b65c-bcb19a700edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" -> 10.1666"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15fbfc2-7ae7-4e2e-88d9-508f44a92de9",
   "metadata": {},
   "source": [
    "## 6. Exploitation des résultats \n",
    "\n",
    "L'ensemble des informations exploitables est disponible dans l'argument bestModel.\n",
    "\n",
    "- (a) Afficher les coefficients obtenus par le modèle optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8651d98-c4de-4210-af03-111b912533e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des coefficients du modèle\n",
    "cv_model.bestModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf3ffe6-3b61-44b7-ade6-2ea502e0241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" -> DenseVector([0.7407, -0.0556, -0.0751, 0.1355, 0.0125, -0.2059, -0.0581, -0.0478, -0.1084, 0.1024, -0.4342, 0.0382]) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6f4bea-971d-4fc0-b811-fa19a729a93a",
   "metadata": {},
   "source": [
    "   Les paramètres de la grille obtenus pour le meilleur modèle ne sont pas accessibles directement, mais sont stockés au sein de l'objet java :\n",
    "\n",
    "cv_model.bestModel._java_obj.getRegParam()\n",
    "cv_model.bestModel._java_obj.getElasticNetParam()\n",
    "\n",
    " Cette information est utile pour vérifier que notre grille est bien adaptée au modèle : il faut éviter que le paramètre choisi soit sur un bord de notre intervalle. Il faut ici passer par l'objet java parce que cette option n'est pas encore disponible directement dans PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a94ddc-ec95-4cd2-9b08-5085277387e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
